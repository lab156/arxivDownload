\documentclass{article}
%\usepackage{apacite}
\usepackage{hyperref}

\title{Text-Mining the arXiv with LLMs}
\author{Luis Berlioz\\
    Universidad Nacional Aut√≥noma de Honduras}


\begin{document}
\begin{abstract}
    \input{abstract.tex}
\end{abstract}
\section{Introduction}
%In previous works \cite{Deyan1, glossary, scss}, 
%This work relies on the methodology in \cite{Deyan1, glossary, scss}, to generate the training sets that are used in this 

This work describes the process of finetuning large language models (LLMs) to generate a complete glossary of mathematics from the arXiv website.  It starts with the finetuning of pretrained LLMs for two different tasks. The first is the classification task, in which a sequence classifier is trained on examples of mathematical definitions and non-definitions. This binary classifier is run on the paragraphs from the mathematics articles stored in the arXiv website. The classification task is followed by the NER task (also known as chunking) in which the terms being defined (definienda) are identified. This task is performed using a token classifier and it is run on all the definitions from the previous step. The resulting dataset of definitions and terms can be browsed here: \url{https://efedequis.xyz/argot/}

In the classification task, a binary sequence classifier is trained on a dataset that contains labeled paragraphs. The labels are set according to whether each paragraph is a definition or not. This dataset is obtained using the methodology in \cite{Deyan1}, in which the \LaTeX{} source of the articles is parsed in search of the \verb|\begin{definition}...\end{definition}| macro. This is done using the latexml....
The negative examples are generated by negative sampling... and their amount is set in order to create a balanced dataset.

Similarly for the NER task, 

Several common LLMs can be finetuned to classify short texts. This process is generally called 

The methodology used to produce a training set of labeled definitions appears in \cite{Deyan1, glossary}. It consists of parsing the \LaTeX{} source of the mathematical articles in search of 

To 


\section{Training}
both classification task and NER
Scatter plot of training sessions.

\section{Inference}
Just describe the parallelization and time spent

\section{Conclusions and Future Work}

\bibliographystyle{apalike}
\bibliography{article}
\end{document}
