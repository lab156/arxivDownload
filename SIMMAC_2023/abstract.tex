We present a small update to an ongoing project to automatically create a glossary of all mathematical terms by applying text-mining techniques to the arXiv website. Using an established methodology that obtains training examples from the \LaTeX{} source of mathematical articles in the arXiv, we obtain enough data to finetune a large language model (LLM). The resulting LLMs are able to identify definitions and the term being defined in them.  This article describes the results obtained during the training and inference process, and compares them to previous benchmarks with neural and bag-of-words methods. This includes  significant improvements obtained by using transformer based models compared to LSTM networks. Lastly, the resulting dataset is described. 

