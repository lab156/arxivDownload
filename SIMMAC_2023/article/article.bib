@InProceedings{glossary,
   author =  "Berlioz, Luis",
   title =  "Creating a Database of Definitions From Large Mathematical
   Corpora",
   booktitle =  "Joint Proceedings of the FMM, LML, OpenMath Workshops,
   Doctoral Program and Work in Progress at the Conference on Intelligent
   Computer Mathematics 2019 (CICM 2019)",
   year =  "2019",
   location =  "Prague, Czech Republic",
   url =
   "[2]http://cl-informatik.uibk.ac.at/cek/cicm-wip-tentative/WiP2.pdf"
   }

@article{Deyan1,
  author    = {Deyan Ginev and
               Bruce R. Miller},
  title     = {Scientific Statement Classification over arXiv.org},
  journal   = {CoRR},
  volume    = {abs/1908.10993},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.10993},
  archivePrefix = {arXiv},
  eprint    = {1908.10993},
  timestamp = {Wed, 04 Sep 2019 15:47:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-10993.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kang-etal-2020-document,
    title = "Document-Level Definition Detection in Scholarly Documents: Existing Models, Error Analyses, and Future Directions",
    author = "Kang, Dongyeop  and
      Head, Andrew  and
      Sidhu, Risham  and
      Lo, Kyle  and
      Weld, Daniel  and
      Hearst, Marti A.",
    booktitle = "Proceedings of the First Workshop on Scholarly Document Processing",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.sdp-1.22",
    doi = "10.18653/v1/2020.sdp-1.22",
    pages = "196--206",
    abstract = "The task of definition detection is important for scholarly papers, because papers often make use of technical terminology that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate enough to use in realworld applications. In this paper, we first perform in-depth error analysis of the current best performing definition detection system and discover major causes of errors. Based on this analysis, we develop a new definition detection system, HEDDEx, that utilizes syntactic features, transformer encoders, and heuristic filters, and evaluate it on a standard sentence-level benchmark. Because current benchmarks evaluate randomly sampled sentences, we propose an alternative evaluation that assesses every sentence within a document. This allows for evaluating recall in addition to precision. HEDDEx outperforms the leading system on both the sentence-level and the document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively. We note that performance on the high-recall document-level task is much lower than in the standard evaluation approach, due to the necessity of incorporation of document structure as features. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications.",
}


@inproceedings{scss,
  author       = {Luis Berlioz},
  editor       = {Temur Kutsia},
  title        = {ArGoT: {A} Glossary of Terms extracted from the arXiv},
  booktitle    = {Proceedings of the 9th International Symposium on Symbolic Computation
                  in Software Science, {SCSS} 2021, Hagenberg, Austria, September 8-10,
                  2021},
  series       = {{EPTCS}},
  volume       = {342},
  pages        = {14--21},
  year         = {2021},
  url          = {https://doi.org/10.4204/EPTCS.342.2},
  doi          = {10.4204/EPTCS.342.2},
  timestamp    = {Mon, 29 Nov 2021 16:32:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-02801.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{psc,
author="Buitrago, Paola A.
and Nystrom, Nicholas A.",
editor="Nesmachnow, Sergio
and Castro, Harold
and Tchernykh, Andrei",
title="Neocortex and Bridges-2: A High Performance AI+HPC Ecosystem for Science, Discovery, and Societal Good",
booktitle="High Performance Computing",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="205--219",
abstract="Artificial intelligence (AI) is transforming research through analysis of massive datasets and accelerating simulations by factors of upÂ to a billion. Such acceleration eclipses the speedups that were made possible though improvements in CPU process and design and other kinds of algorithmic advances. It sets the stage for a new era of discovery in which previously intractable challenges will become surmountable, with applications in fields such as discovering the causes of cancer and rare diseases, developing effective, affordable drugs, improving food sustainability, developing detailed understanding of environmental factors to support protection of biodiversity, and developing alternative energy sources as a step toward reversing climate change. To succeed, the research community requires a high-performance computational ecosystem that seamlessly and efficiently brings together scalable AI, general-purpose computing, and large-scale data management. The authors, at the Pittsburgh Supercomputing Center (PSC), launched a second-generation computational ecosystem to enable AI-enabled research, bringing together carefully designed systems and groundbreaking technologies to provide at no cost a uniquely capable platform to the research community. It consists of two major systems: Neocortex and Bridges-2. Neocortex embodies a revolutionary processor architecture to vastly shorten the time required for deep learning training, foster greater integration of artificial deep learning with scientific workflows, and accelerate graph analytics. Bridges-2 integrates additional scalable AI, high-performance computing (HPC), and high-performance parallel file systems for simulation, data pre- and post-processing, visualization, and Big Data as a Service. Neocortex and Bridges-2 are integrated to form a tightly coupled and highly flexible ecosystem for AI- and data-driven research.",
isbn="978-3-030-68035-0"
}


@inproceedings{HFtransformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

