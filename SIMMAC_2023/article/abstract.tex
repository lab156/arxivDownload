We present a small update to an ongoing project that aims at automatically generating a glossary of all mathematical terms by applying text-mining techniques on the arXiv website. Using an established methodology that obtains training examples from the \LaTeX{} source of mathematical articles in the arXiv, we obtain enough data to finetune a large language model (LLM). 
The methodology consists of two independent parts that are fully described in previous work. The first part consists of parsing the \LaTeX{} source of the articles on the arXiv website in search of paragraphs labeled as definitions by the macro \verb|\begin{definition}...\end{definition}|. The second uses the content of freely available websites like Planetmath \url{https://planetmath.org/}, The Stacks project \url{https://stacks.math.columbia.edu/} and Wikipedia. Every article in these websites has a title and many of them contain a \emph{definition} section. Each definition is annotated by searching for the title in the text of the definition. 
The data produced in the first part is used to train a binary text classifier and the second to train a single category NER model. We have successfully trained three types of models: bag-of-words, LSTMs and more recently, Transformer based LLMs. 
The resulting models are able to identify definitions and the terms being defined in them. In this presentation, we describe the results obtained during the training and inference process using LLMs, and compares them to previous benchmarks obtained with neural and bag-of-words methods. This includes significant improvements obtained by using transformer based models compared to LSTM networks. Lastly, the resulting dataset is described, and a rudimentary search engine (https://efedequis.xyz/argot/) is showcased. 

