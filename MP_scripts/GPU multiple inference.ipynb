{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU data parallelism\n",
    "* Worker = is a whole computer with CPU and all (node)\n",
    "* device or accelerator is a GPU or TPU\n",
    "* XLA_GPU = Accelerated Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "import os, sys\n",
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "import queue\n",
    "import glob\n",
    "\n",
    "currentdir = os.path.dirname(\n",
    "    os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "#parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, currentdir)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gpu_wit as gw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f9d8abb100d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0myear_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'math11'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m set1 = set([os.path.split(n)[-1].split('.')[0] \n\u001b[0;32m----> 5\u001b[0;31m             for n in glob.glob(os.path.join(promath_path, year_dir) + '/*.tar.gz')])\n\u001b[0m\u001b[1;32m      6\u001b[0m set2 = set([os.path.split(n)[-1].split('.')[0] \n\u001b[1;32m      7\u001b[0m             for n in glob.glob(os.path.join(inference_path, year_dir) + '/*.xml.gz')])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "promath_path = '/media/hd1/promath/'\n",
    "inference_path = '/media/hd1/glossary/NN.v1/'\n",
    "year_dir = 'math11'\n",
    "set1 = set([os.path.split(n)[-1].split('.')[0] \n",
    "            for n in glob.glob(os.path.join(promath_path, year_dir) + '/*.tar.gz')])\n",
    "set2 = set([os.path.split(n)[-1].split('.')[0] \n",
    "            for n in glob.glob(os.path.join(inference_path, year_dir) + '/*.xml.gz')])\n",
    "set2.pop()\n",
    "set1.difference(set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hola': 1, 'adios': 4, 3: 'macizo', 'culo': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {'hola': 1, 'adios': 2, 3:\"macizo\"}\n",
    "cfg.update({'culo': 4, 'adios': 4})\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dir_diff(promath_path, inference_path, year_dir):\n",
    "    '''\n",
    "promath_path ex. '/media/hd1/promath/' \n",
    "                 expected file format .tar.gz\n",
    "inference_path ex. '/media/hd1/glossary/NN.v1/' \n",
    "                 expected formar .xml.gz\n",
    "year_dir: either a string \"math91\" or a list of strings [\"math01\", \"math02\", ]\n",
    "    '''\n",
    "    if isinstance(year_dir, str):\n",
    "        year_dir = [year_dir, ]\n",
    "        \n",
    "    # promath and inference path lists\n",
    "    pp_lst = []\n",
    "    pi_lst = []\n",
    "    for year in year_dir:\n",
    "        pp_lst += [os.path.join(year, os.path.split(n)[-1].split('.')[0])\n",
    "                for n in glob.glob(os.path.join(promath_path, year) + '/*.tar.gz')]\n",
    "        pi_lst += [os.path.join(year, os.path.split(n)[-1].split('.')[0])\n",
    "                for n in glob.glob(os.path.join(inference_path, year) + '/*.xml.gz')]\n",
    "        # output looks like ['math11/1106_003', '']\n",
    "        \n",
    "    pp_set = set(pp_lst)\n",
    "    pi_set = set(pi_lst)\n",
    "    return pp_set.difference(pi_set)\n",
    "    \n",
    "def check_if_done(promath_path, mined_path):\n",
    "    '''\n",
    "    Inputs:\n",
    "        promath_path: path to data file ex. $PROJECT/promath/math11/1101_001.tar.gz\n",
    "        mined_path:   dir of already mined files. ex. $PROJECT/with_mp/\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "dir_diff(promath_path, inference_path, ['math' + repr(y) for y in range(91, 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Condition(<locked _thread.lock object at 0x7f0881d76930>, 0)>\n",
      "yes cvalue  2\n"
     ]
    }
   ],
   "source": [
    "#c.release()\n",
    "#c.acquire()\n",
    "with c._cond:\n",
    "    print(c._cond)\n",
    "    if not c._value:\n",
    "        print('not cvalue ', c._value)\n",
    "    else:\n",
    "        print('yes cvalue ', c._value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c._value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "q = queue.Queue()\n",
    "for i in range(3):\n",
    "    q.put(i)\n",
    "print(q.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        Condition\n",
       "\u001b[0;31mString form:\u001b[0m <Condition(<unlocked _thread.lock object at 0x7f0881639510>, 0)>\n",
       "\u001b[0;31mFile:\u001b[0m        /usr/lib/python3.8/threading.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Class that implements a condition variable.\n",
       "\n",
       "A condition variable allows one or more threads to wait until they are\n",
       "notified by another thread.\n",
       "\n",
       "If the lock argument is given and not None, it must be a Lock or RLock\n",
       "object, and it is used as the underlying lock. Otherwise, a new RLock object\n",
       "is created and used as the underlying lock.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q.qsize()\n",
    "q.not_empty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker PID: 139678259599104\n",
      "Worker PID: 139677982770944\n",
      "Worker PID: 139677974378240\n",
      "Worker PID: 139677965985536\n"
     ]
    }
   ],
   "source": [
    "# initialize the worker process\n",
    "def init_worker():\n",
    "    # get the pid for the current worker process\n",
    "    pid = os.getpid()\n",
    "    print(f'Worker PID: {pid}', flush=True)\n",
    "\n",
    "def init_thread_worker():\n",
    "    # get the pid for the current worker process\n",
    "    #pid = os.getpid()\n",
    "    pid = threading.get_ident()\n",
    "    print(f'Worker PID: {pid}', flush=True)\n",
    "    \n",
    "with mp.pool.ThreadPool(4,initializer=init_thread_worker):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPU, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "if gpus:\n",
    "  # Create 2 virtual GPUs with 1GB memory each\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
    "         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# List devices\n",
    "# If this cell is run the next cell won't work properly\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "print(gpus)\n",
    "#print(gpus)\n",
    "#tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "#with tf.device('/device:GPU:1'):\n",
    "with tf.device('/gpu:1'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat multi on /gpu:0\n",
      "mat multi on /gpu:1\n",
      "mat multi on /gpu:0\n",
      "mat multi on /gpu:1\n",
      "[None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "gw.paral_threadpool_exec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.pool.ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_mul(gpu):\n",
    "    with tf.device(gpu):\n",
    "        a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "        b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "mat_mul('/gpu:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not end\n",
    "gpu_lst = [f\"/gpu:{k}\" for k in range(2)]\n",
    "with mp.Pool(2) as pool:\n",
    "    print(pool.map(mat_mul, gpu_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya salio de aqui.\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d651237b16be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmp_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-d651237b16be>\u001b[0m in \u001b[0;36mmp_wrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ya salio de aqui.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmp_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def mp_wrap():\n",
    "    gpu_lst = [f\"/gpu:{k}\" for k in range(2)]\n",
    "    with mp.Pool(2) as pool:\n",
    "        result = pool.apply_async(mat_mul, ('/gpu:0',)) \n",
    "        print('ya salio de aqui.')\n",
    "        print(result.get(timeout=10))\n",
    "\n",
    "mp_wrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:GPU:0', device_type='GPU'),\n",
       " LogicalDevice(name='/device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[26]], dtype=int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):       # Run nodes with GPU 0\n",
    "#with tf.device(gpu.name):       # Run nodes with GPU 0\n",
    "    m1 = tf.constant([[3, 5]])\n",
    "    m2 = tf.constant([[2],[4]])\n",
    "    product = tf.matmul(m1, m2)\n",
    "product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.distribute_lib._CurrentDistributionContext at 0x7f15f833b7f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This works for parallel training\n",
    "# I couldn't find an easy working example of inference on multiple GPUs.\n",
    "mirror = tf.distribute.MirroredStrategy()\n",
    "mirror.scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 1522951353799500426,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 17831017549081373243\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 7217096090645788710\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 1073741824\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 13993195329145291770\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way of listing GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.361111111111112"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec = 19*60 + 41\n",
    "# approx 20 mins/ 12 files\n",
    "20/12/60*229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--foo', nargs=2)\n",
    "parser.add_argument('--bar', nargs=\"+\")\n",
    "args = parser.parse_args('--foo a b'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.bar == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "hola macizo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-126df747826b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/hd12'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hola macizo'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: hola macizo"
     ]
    }
   ],
   "source": [
    "assert os.path.isdir('/media/hd12'), 'hola macizo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
