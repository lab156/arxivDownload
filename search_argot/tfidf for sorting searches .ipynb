{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1) Join more phrases (target 300K)\n",
    "\n",
    "2) Create the tf-idf transformation.\n",
    "\n",
    "3) For each article and each term within, annotate each entry with its corresponding tf-idf score.\n",
    "\n",
    "\n",
    "1) To compute the `tf-idf` consider a corpus $D$, with documents $d$ and terms $t$:\n",
    "  $$tf(t,d) = \\frac{f_{t,d}}{\\sum_{\\tau\\in V} f_{\\tau, d}}$$\n",
    "\n",
    "The vocabulary is $V$ and $f_{t,d}$ the count of the term $t$ in $d$. Then compute\n",
    "$$idf(t,D) = \\ln\\left( \\frac{N}{|\\{ d\\in D:\\ t\\in d \\}|}\\right)$$\n",
    "here $N=|D|$ (number of documents in the corpus). Finally the $tfidf(t,d,D) = tf(t,d)\\cdot idf(t,D)$.\n",
    "\n",
    "In the sklearn module, (with `norm = None`)\n",
    "$$tfidf(t, d,D) = f_{t, d}\\cdot(1+\\ln((N+1)/(df(t)+1))$$\n",
    "Where $df$ is the number of documents where $t$ appears.\n",
    "\n",
    "With `norm = \"l1\"` each row is normalized, that is, each term is divided by the sum of the row.\n",
    "\n",
    "## Workflow\n",
    "1) Get the promath data that has the format:\n",
    "\n",
    "     - promath\n",
    "     \n",
    "       - math10\n",
    "       \n",
    "         - 1003_001.tar.gz\n",
    "         - 1003_002.tag.gz\n",
    "         - ...\n",
    "         \n",
    "  The function should take in the file 1003_001.tar.gz, a _list of phrases_ and output a file with the format:\n",
    "```xml\n",
    "<root>\n",
    "  <article name=\"macizo.tx\">\n",
    "      <parag num=1> text </para>\n",
    "      <parag num=2> text </para>\n",
    "  </article>\n",
    "</root>\n",
    "```\n",
    "The text in the para tags is clean, tokenized and joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lxml import etree\n",
    "import gzip\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "\n",
    "import os, sys, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, os.path.join(parentdir, 'embed'))\n",
    "sys.path.insert(0, parentdir)\n",
    "\n",
    "from clean_and_token_text import normalize_text, normalize_phrase, \\\n",
    "                                 token_phrases3, ReadGlossary, join_phrases\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import parsing_xml as px\n",
    "import peep_tar as peep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txmlst = []\n",
    "for t in peep.tar_iter('../tests/few_actual_articles.tar.gz', '.xml'):\n",
    "    txmlst.append(px.DefinitionsXML(t[1]).run_recutext_onall_para(cleaner_fun=normalize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2816/2816 [00:51<00:00, 54.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759240\n"
     ]
    }
   ],
   "source": [
    "phrases_file = glob('/media/hd1/glossary/NN.v1/math*/*')\n",
    "\n",
    "phrases_cnt = Counter()\n",
    "for xml_path in tqdm(phrases_file):\n",
    "    root = etree.parse(xml_path)\n",
    "    phrases_list_temp = [normalize_text(r.text)\\\n",
    "            for r in root.findall('//dfndum') ]\n",
    "    phrases_cnt.update([r for r in phrases_list_temp if len(r.split()) > 1])\n",
    "\n",
    "phrases_lst = [pair[0] for pair in phrases_cnt.most_common()]\n",
    "print(len(phrases_lst))\n",
    "join_fun = lambda s: token_phrases3(s, phrase_lst=phrases_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A phrase with only too few words was given. Phrase: lie",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-7ad916020101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#text = 'let us begin with a rough description of kozlov the general setting and the background of the problem .'\\ntext = 'with a rough .'\\n#text = 'so weird what what what is going on of the let us begin this with background of the problem .'\\n#token_phrases3(text, phrase_lst=(of_lst + ['of the']))\\ntoken_phrases3(text, phrase_lst=ph_dict)\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/Documents/arxivDownload/embed/clean_and_token_text.py\u001b[0m in \u001b[0;36mtoken_phrases3\u001b[0;34m(text, phrase_lst, join_str)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mphrase_default\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mph_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mph_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A phrase with only too few words was given. Phrase: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mtext_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A phrase with only too few words was given. Phrase: lie"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#text = 'let us begin with a rough description of kozlov the general setting and the background of the problem .'\n",
    "text = 'with a rough .'\n",
    "#text = 'so weird what what what is going on of the let us begin this with background of the problem .'\n",
    "#token_phrases3(text, phrase_lst=(of_lst + ['of the']))\n",
    "token_phrases3(text, phrase_lst=phrases_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/2816 [00:00<00:16, 172.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [2816, 2816] files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2816/2816 [00:53<00:00, 52.30it/s] \n",
      "100%|██████████| 2816/2816 [00:55<00:00, 51.11it/s] \n"
     ]
    }
   ],
   "source": [
    "RG = ReadGlossary('/media/hd1/glossary/v3/math*/*.xml.gz', '/media/hd1/glossary/NN.v1/math*/*')\n",
    "ph_dict = RG.first_word_dict(intersect = 'relative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_inline_math_ and',\n",
       " '_inline_math_ _inline_math_',\n",
       " 'recent years',\n",
       " '_inline_math_ of',\n",
       " 'for _inline_math_',\n",
       " '_inline_math_ if',\n",
       " '_inline_math_ also',\n",
       " 'suppose that',\n",
       " 'condition for',\n",
       " '_inline_math_ on',\n",
       " '_inline_math_ a',\n",
       " '_inline_math_ as',\n",
       " '_inline_math_ in',\n",
       " '_inline_math_ up',\n",
       " '_inline_math_ by',\n",
       " '_inline_math_ which',\n",
       " '_inline_math_ that',\n",
       " '_inline_math_ the',\n",
       " '_inline_math_ or',\n",
       " 'family of',\n",
       " 'of the',\n",
       " 'all _inline_math_',\n",
       " 'a _inline_math_',\n",
       " 'well known',\n",
       " 'group of',\n",
       " 'r and',\n",
       " 'a point',\n",
       " 'a linear',\n",
       " ' ',\n",
       " '']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RG.ntc_intersect('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hales jewett number', 5.71612063521971e-07)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in si if (p[0].split())[0] == 'hales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2816/2816 [00:53<00:00, 52.69it/s] \n",
      "100%|██████████| 2816/2816 [00:54<00:00, 51.38it/s] \n"
     ]
    }
   ],
   "source": [
    "vocab_set = RG.ntc_intersect('relative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.34 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(norm='l1')\n",
    "X = vect.fit_transform(corpus) \n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set([t.replace(' ', '_') for t in vocab_set.keys()]))\n",
    "tvect = TfidfVectorizer(sublinear_tf=True, norm='l1', vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the corpus\n",
    "corpus = []\n",
    "article_names = []\n",
    "path_lst = glob('/media/hd1/cleaned_text/joined_math12-34_04-01/math0*/*.xml.gz')\n",
    "path_lst += glob('/media/hd1/cleaned_text/joined_math12-34_04-01/math9*/*.xml.gz')\n",
    "for tar_path in path_lst: \n",
    "    xobj = etree.parse(tar_path)\n",
    "    problem_arts = []\n",
    "    for art in xobj.findall('.//article'):\n",
    "        art_text = ''\n",
    "        article_names.append(art.attrib['name'])\n",
    "        for par in art.findall('.//parag'):\n",
    "            try:\n",
    "                art_text +=  (par.text + \" \")\n",
    "            except TypeError as ee:\n",
    "                #art_text = ''\n",
    "                #print(\"article {} gave the error:\".format(art.attrib['name']), ee)\n",
    "                problem_arts.append(art.attrib['name'])\n",
    "        corpus.append(art_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 48s, sys: 918 ms, total: 2min 49s\n",
      "Wall time: 2min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(85376, 347052)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ttrans = tvect.fit_transform(corpus)\n",
    "ttrans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('good_pair', 0.00859552057326182)\n",
      "('partially_hyperbolic', 0.006482868736963943)\n",
      "('stable_manifolds', 0.006073718429800212)\n",
      "('generalized_eigenspaces', 0.0056295467163887695)\n",
      "('finitely_generated_discrete_group', 0.004597134897897061)\n",
      "('higher_rank', 0.004565492075269953)\n",
      "('weak_hyperbolicity', 0.004222940648390717)\n",
      "('semisimple', 0.004088095118946367)\n",
      "('nondegenerate', 0.003725007543091549)\n",
      "('margulis', 0.003543514391854277)\n",
      "('essential_subset', 0.002550637320075592)\n",
      "('_inline_math_', 0.0022122098236394697)\n",
      "('exponential_map', 0.002209085680647651)\n",
      "('orbit_equivalence', 0.0020292873331886115)\n",
      "('rank_one', 0.0020172744867039765)\n",
      "('lie_groups', 0.001991544206848132)\n",
      "('smooth_action', 0.001831681027789851)\n",
      "('global', 0.0008277823435205402)\n",
      "('non_trivial', 0.0007194400649381925)\n",
      "('line_bundle', 0.0)\n"
     ]
    }
   ],
   "source": [
    "w_lst = ['_inline_math_', \n",
    "         'exponential_map',\n",
    "         'generalized_eigenspaces',\n",
    "         'weak_hyperbolicity',\n",
    "         'partially_hyperbolic',\n",
    "         'finitely_generated_discrete_group',\n",
    "         'global',\n",
    "         'smooth_action',\n",
    "         'nondegenerate',\n",
    "        'good_pair',\n",
    "        'stable_manifolds',\n",
    "        'rank_one',\n",
    "        'essential_subset',\n",
    "        'semisimple',\n",
    "        'orbit_equivalence',\n",
    "        'higher_rank',\n",
    "        'non_trivial',\n",
    "        'lie_groups',\n",
    "        'margulis',\n",
    "        'line_bundle']\n",
    "def tfidf(w):\n",
    "    w_ind = vocab.index(w)\n",
    "    return ttrans[0,w_ind]\n",
    "sort_fun = lambda p: p[1]\n",
    "s_lst = sorted([(w, tfidf(w)) for w in w_lst], key=sort_fun, reverse=True)\n",
    "for p in s_lst:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdelta = dt.datetime.now() - N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdelta.seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10034//60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10034%60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602054"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "167*60 + 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
