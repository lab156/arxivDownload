{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1) create a search table with key=term and value paragragh in raw LaTeXML output\n",
    "\n",
    "2) a term-reference is {term, art_addr, parag_index, p_tag, tfidf}\n",
    "\n",
    "3) serialize the list of term-references\n",
    "\n",
    "4) Test uploading to AWS database, this includes using boto3, dynamodb SDK\n",
    "\n",
    "TODO:\n",
    "- Some `<para>` tags don't have a `<p>` inside so I just used recutext. *ADD* the missing p tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "from marshmallow import Schema, fields, pprint\n",
    "from random import random\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import boto3 as b3\n",
    "import botocore as bcore\n",
    "\n",
    "import json\n",
    "\n",
    "import os, sys, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, os.path.join(parentdir, 'embed'))\n",
    "sys.path.insert(0, parentdir)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from clean_and_token_text import normalize_text, normalize_phrase, join_xml_para_and_write, ReadGlossary\n",
    "import parsing_xml as px\n",
    "import peep_tar as peep\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_path = 'math20/2003_003'\n",
    "argot_path = '/media/hd1/glossary/NN.v1/' + general_path + '.xml.gz'\n",
    "prom_path = '/media/hd1/promath/' + general_path + '.tar.gz'\n",
    "join_path = '/media/hd1/cleaned_text/joined_math12-34_04-01/' + general_path + 'xml.gz'\n",
    "ns = {'latexml': 'http://dlmf.nist.gov/LaTeXML' }\n",
    "\n",
    "argot = etree.parse(argot_path)\n",
    "with tarfile.open(prom_path) as tar_fobj:\n",
    "    art_lst = [k.get_info()['name'] for k in tar_fobj.getmembers()]\n",
    "    members = tar_fobj.getmembers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/2816 [00:00<00:16, 172.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [2816, 2816] files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2816/2816 [00:52<00:00, 53.16it/s] \n",
      "100%|██████████| 2816/2816 [00:54<00:00, 52.13it/s] \n",
      "100%|██████████| 240/240 [1:06:37<00:00, 16.66s/it]\n"
     ]
    }
   ],
   "source": [
    "#glossary_file_lst = glob.glob('/media/hd1/glossary/NN.v1/math9*/*.xml.gz')\n",
    "glossary_file_lst = glob.glob('/media/hd1/glossary/NN.v1/math0*/*.xml.gz')\n",
    "\n",
    "RG = ReadGlossary('/media/hd1/glossary/v3/math*/*.xml.gz', '/media/hd1/glossary/NN.v1/math*/*')\n",
    "vocab = RG.ntc_intersect('relative')\n",
    "\n",
    "corpus = []\n",
    "term_ref_lst = []\n",
    "for glossary_file in tqdm(glossary_file_lst):\n",
    "    # argot_file format: /media/hd1/glossary/NN.v1/math95/9506_001.xml.gz\n",
    "    math_year = glossary_file.split('/')[-2]\n",
    "    subfilename = glossary_file.split('/')[-1].split('.')[0] # should be 9506_001\n",
    "    promath_file = os.path.join('/media/hd1/promath', math_year, subfilename + '.tar.gz')\n",
    "    joined_file = os.path.join('/media/hd1/cleaned_text/joined_math19-35_13-01/',\n",
    "                               math_year, subfilename + '.xml.gz')\n",
    "    \n",
    "    glossary_xml = etree.parse(glossary_file)\n",
    "    joined_xml = etree.parse(joined_file)\n",
    "    promath_tarfobj = tarfile.open(promath_file)\n",
    "    for art in glossary_xml.findall('./article'):\n",
    "        art_name = art.get('name')\n",
    "        # need to use only the basename of the article\n",
    "        #art_basename = os.path.basename(art_name)\n",
    "        joined_name_results = joined_xml.xpath('./article[@name=\"{}\"]'.format(art_name))\n",
    "        # POPULATE THE CORPUS\n",
    "        if len(joined_name_results) > 0:\n",
    "            joined_art_text = ''\n",
    "            for parag in joined_name_results[0].findall('./parag'):\n",
    "                text = '' if parag.text is None else parag.text\n",
    "                joined_art_text += (text + ' ')\n",
    "            corpus.append(joined_art_text)\n",
    "        else:\n",
    "            print(f'joined name search results empty art_name={art_name}')\n",
    "            corpus.append('')\n",
    "        corpus_index = len(corpus) - 1\n",
    "        \n",
    "        # OPEN THE PROMATH FILE\n",
    "        try:\n",
    "            #promath_obj = peep.tar(promath_file, art_name)[1].exml\n",
    "            promath_obj = px.DefinitionsXML(promath_tarfobj.extractfile(art_name))\n",
    "        except AttributeError:\n",
    "            print(f'{art_name} gave attributeError')\n",
    "        # this list is in sync with glossary paragraph index\n",
    "        promath_parag_lst = promath_obj.para_list()\n",
    "        \n",
    "        # LOOP THROUGH THE DFDUMS\n",
    "        for defin in art.findall('./definition'):\n",
    "            p_index = int(defin.get('index'))\n",
    "            for term_raw in defin.findall('./dfndum'):\n",
    "                term = normalize_text(term_raw.text)\n",
    "                if term in vocab:\n",
    "                    term_ref_lst.append((\n",
    "                           term,\n",
    "                            art_name,\n",
    "                            p_index,\n",
    "                            etree.tostring(promath_parag_lst[p_index]).decode('utf-8'),\n",
    "                            corpus_index))\n",
    "    promath_tarfobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_data_path = '/media/hd1/termreferences/termreference_db17-33_22-01/'\n",
    "#with open(pickle_data_path + 'term_ref_lst.pickle', 'rb') as fobj:\n",
    "#    term_ref_lst = pickle.load(fobj)\n",
    "with open(pickle_data_path + 'vocab.pickle', 'rb') as fobj:\n",
    "    vocab = pickle.load(fobj)\n",
    "with open(pickle_data_path + 'corpus.pickle', 'rb') as fobj:\n",
    "    corpus = pickle.load(fobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 16s, sys: 3.79 s, total: 11min 20s\n",
      "Wall time: 11min 20s\n",
      "The shape of the resulting matrix is: (340128, 347045)\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE THE TDIDF MATRIX\n",
    "vocab_ = list(set([t.replace(' ', '_') for t in vocab]))\n",
    "tvect = TfidfVectorizer(sublinear_tf=True, norm='l1', vocabulary=vocab_)\n",
    "                \n",
    "%time ttrans = tvect.fit_transform(corpus)\n",
    "print(f'The shape of the resulting matrix is: {ttrans.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13600245"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tfidf(term_, corpus_index_, vocab_):\n",
    "    ter = term_.replace(' ', '_')\n",
    "    tindex = vocab_.index(ter)\n",
    "    x = ttrans[corpus_index_, tindex]\n",
    "    return int(x * 10_000_000_000)\n",
    "\n",
    "get_tfidf('pure motive', 335180, vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'term_ref_lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-68b6ecab32c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mTR_lst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_ref_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# check the length of p_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m250_000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'term_ref_lst' is not defined"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TermReference(Schema):\n",
    "    truid: int\n",
    "    term : str\n",
    "    addr : str\n",
    "    index : int\n",
    "    p_tag : str\n",
    "    tfidf : int\n",
    "    \n",
    "class TermRefSchema(Schema):\n",
    "    truid = fields.Int()\n",
    "    term =  fields.String()\n",
    "    addr =  fields.String()\n",
    "    index = fields.Int()\n",
    "    p_tag = fields.String()\n",
    "    tfidf = fields.Int()\n",
    "\n",
    "TR_lst=[]\n",
    "for k, tr in enumerate(term_ref_lst[:5000]):\n",
    "    # check the length of p_tag\n",
    "    if len(tr[3]) < 250_000:\n",
    "        TR_lst.append(TermReference(\n",
    "            truid = k,\n",
    "            term= tr[0],\n",
    "               addr = tr[1],\n",
    "               index = tr[2],\n",
    "              p_tag = tr[3],\n",
    "               tfidf = get_tfidf(tr[0], tr[4], vocab_) + tr[2]))\n",
    "# adding tfidf + index has three nice properties\n",
    "#1) together with term it is mostly unique \n",
    "#2) if a term appears twice on the same paragraph, it gets rewritten and merges in the data\n",
    "#3) it sorts in the order in which the term appears on the same article.\n",
    "# Serialize all the data and save to json file\n",
    "trs = TermRefSchema(many=True)\n",
    "tr_ = TermRefSchema()\n",
    "#with open('/home/luis/rm_me/math_json/'+ 'first_5K' + '.json', 'w') as fobj:\n",
    "#    fobj.write(trs.dumps(TR_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[9,6])\n",
    "ax = plt.subplot(111)\n",
    "plt.hist([len(tr.p_tag) for tr in TR_lst],100)\n",
    "#ax.get_yaxis().set_major_formatter(\n",
    "#    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTIVE'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_termreference_table(dynamodb=None, name='TermReference'):\n",
    "    if not dynamodb:\n",
    "        dynamodb = b3.resource('dynamodb', endpoint_url=\"http://localhost:8000\")\n",
    "\n",
    "    table = dynamodb.create_table(\n",
    "        TableName=name,\n",
    "        KeySchema=[\n",
    "            { 'AttributeName': 'term',\n",
    "                'KeyType': 'HASH'   },\n",
    "            { 'AttributeName': 'tfidf',\n",
    "                'KeyType': 'RANGE'   },\n",
    "        ],\n",
    "        AttributeDefinitions=[\n",
    "            { 'AttributeName': 'term',\n",
    "                'AttributeType': 'S' },\n",
    "            { 'AttributeName': 'tfidf',\n",
    "                'AttributeType': 'N' },\n",
    "        ],\n",
    "        ProvisionedThroughput={\n",
    "            'ReadCapacityUnits': 10,\n",
    "            'WriteCapacityUnits': 10 } )\n",
    "    return table\n",
    "\n",
    "movie_t = create_termreference_table(name='TermReference_ss')\n",
    "movie_t.table_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_termreferences(TR_l, dynamodb=None, table=\"TermReference\"):\n",
    "    if not dynamodb:\n",
    "        dynamodb = b3.resource('dynamodb', endpoint_url=\"http://localhost:8000\")\n",
    "\n",
    "    table = dynamodb.Table(table)\n",
    "    for tr in TR_l:\n",
    "        #print(\"Adding termref: \", tr.term, tr.addr)\n",
    "        table.put_item(Item=tr_.dump(tr))\n",
    "\n",
    "load_termreferences(TR_lst, table='TermReference_ss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1e798bf8d51b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mddb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dynamodb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"http://localhost:8000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TermReference_s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m response = table.query(\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mKeyConditionExpression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamodb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconditions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table' is not defined"
     ]
    }
   ],
   "source": [
    "## MAKING QUERIES\n",
    "def get_movie(truid, tfidf, dynamodb=None):\n",
    "    if not dynamodb:\n",
    "        dynamodb = b3.resource('dynamodb', endpoint_url=\"http://localhost:8000\")\n",
    "\n",
    "    table = dynamodb.Table('TermReference2')\n",
    "\n",
    "    try:\n",
    "        response = table.get_item(Key={'truid': truid, \"tfidf\": tfidf})\n",
    "    except bcore.exceptions.ClientError as e:\n",
    "        print(e.response['Error']['Message'])\n",
    "    else:\n",
    "        return response['Item']\n",
    "\n",
    "#res = get_movie(3, 24440)\n",
    "#pprint(res)\n",
    "\n",
    "def query_termreference(term, dynamodb=None, table=\"TermReference\"):\n",
    "    \"\"\"\n",
    "    Finds an exact truid\n",
    "    \"\"\"\n",
    "    if not dynamodb:\n",
    "        dynamodb = boto3.resource('dynamodb', endpoint_url=\"http://localhost:8000\")\n",
    "\n",
    "    table = dynamodb.Table(table)\n",
    "    response = table.query(\n",
    "        KeyConditionExpression=b3.dynamodb.conditions.Key('truid').eq(truid)\n",
    "    )\n",
    "    return response['Items']\n",
    "\n",
    "#res = query_termreference(1005)\n",
    "#pprint(res)\n",
    "ddb = b3.resource('dynamodb', endpoint_url=\"http://localhost:8000\")\n",
    "tab = ddb.Table('TermReference_s')\n",
    "response = table.query(\n",
    "    KeyConditionExpression=b3.dynamodb.conditions.Key('term')\\\n",
    "    .eq('')\n",
    ")\n",
    "[(r['addr'],r['index']) for r in response['Items']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4586"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LIST ALL TABLES\n",
    "dcl = b3.client('dynamodb', endpoint_url='http://localhost:8000')\n",
    "dcl.list_tables()['TableNames']\n",
    "\n",
    "# DELETE A TABLE\n",
    "#ddb = b3.resource('dynamodb', endpoint_url='http://localhost:8000')\n",
    "#table = ddb.Table('TermReference_sss')\n",
    "#table.delete()\n",
    "\n",
    "# SCAN A TABLE\n",
    "ddb = b3.resource('dynamodb', endpoint_url='http://localhost:8000')\n",
    "table = ddb.Table('TermReference_s')\n",
    "table.item_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/aws\", line 19, in <module>\n",
      "    import awscli.clidriver\n",
      "  File \"/usr/lib/python3/dist-packages/awscli/clidriver.py\", line 36, in <module>\n",
      "    from awscli.help import ProviderHelpCommand\n",
      "  File \"/usr/lib/python3/dist-packages/awscli/help.py\", line 23, in <module>\n",
      "    from botocore.docs.bcdoc import docevents\n",
      "ImportError: cannot import name 'docevents' from 'botocore.docs.bcdoc' (/home/luis/.local/lib/python3.8/site-packages/botocore/docs/bcdoc/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "!aws dynamodb list-tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ ARGOT 9505_001/math.9505216/math.9505216.xml -------------- \n",
      "b'<definition index=\"15\">\\n      <stmnt> Given _inline_math_, _inline_math_ is in _inline_math_ if the following is true. _inline_math_ is a disjoint decomposition. _inline_math_ and _inline_math_ are in _inline_math_ where _inline_math_, _inline_math_. There is _inline_math_, an isomorphism between _inline_math_ and _inline_math_. _inline_math_ is the identity. For _inline_math_ the sets _inline_math_ and _inline_math_ are isomorphic. _inline_math_. _inline_math_ is one-to-one and takes only values outside _inline_math_ (which is the same as _inline_math_). _inline_math_. We make _inline_math_ and the ordering on _inline_math_ is the one generated by this. </stmnt>\\n    <dfndum>disjoint decomposition</dfndum><dfndum>isomorphic</dfndum><dfndum>one-to-one</dfndum></definition>\\n  '\n",
      "-------------- PROMath ------------\n",
      " Given _inline_math_, _inline_math_ is in _inline_math_ if the following is true. _inline_math_ is a disjoint decomposition. _inline_math_ and _inline_math_ are in _inline_math_ where _inline_math_, _inline_math_. There is _inline_math_, an isomorphism between _inline_math_ and _inline_math_. _inline_math_ is the identity. For _inline_math_ the sets _inline_math_ and _inline_math_ are isomorphic. _inline_math_. _inline_math_ is one-to-one and takes only values outside _inline_math_ (which is the same as _inline_math_). _inline_math_. We make _inline_math_ and the ordering on _inline_math_ is the one generated by this. \n",
      "---------------- JOINED math.9505216.xml---------------\n",
      "b'<parag index=\"15\">given _inline_math_ _inline_math_ is in _inline_math_ if the following is true _inline_math_ is a disjoint_decomposition _inline_math_ and _inline_math_ are in _inline_math_ where _inline_math_ _inline_math_ there is _inline_math_ an isomorphism between _inline_math_ and _inline_math_ _inline_math_ is the identity for _inline_math_ the sets _inline_math_ and _inline_math_ are isomorphic _inline_math_ _inline_math_ is one_to_one and takes only values outside _inline_math_ which is the same as _inline_math_ _inline_math_ we make _inline_math_ and the ordering on _inline_math_ is the one_generated by this </parag>'\n"
     ]
    }
   ],
   "source": [
    "# check if it is possible to find the same paragraph in promath, argot-glossary, joined\n",
    "def check_sync(parag_index, art_ind_in_argot, general_path_):\n",
    "    argot_path = '/media/hd1/glossary/NN.v1/' + general_path_ + '.xml.gz'\n",
    "    prom_path = '/media/hd1/promath/' + general_path_ + '.tar.gz'\n",
    "    join_path = '/media/hd1/cleaned_text/joined_math12-34_04-01/' +\\\n",
    "                                general_path_ + '.xml.gz'\n",
    "    \n",
    "    # first get the paragraph index from argot\n",
    "    argot_xml = etree.parse(argot_path)\n",
    "    article_ind = argot_xml.getroot()[art_ind_in_argot]\n",
    "    argot_def = article_ind[parag_index]\n",
    "    article_name = article_ind.get('name')\n",
    "    print(f'------------ ARGOT {article_name} -------------- ')\n",
    "    print(etree.tostring(argot_def))\n",
    "    parag_ind = int(argot_def.get('index'))\n",
    "    \n",
    "    #for k,tarobj in enumerate(peep.tar_iter(prom_path, '.xml')):\n",
    "    #    if k == art_ind_in_argot:\n",
    "    \n",
    "    #promath_obj = etree.parse(tarobj[1])\n",
    "    promath_obj = peep.tar(prom_path, article_name)[1].exml\n",
    "    parag = promath_obj.findall('.//latexml:para', namespaces=ns)[parag_ind]\n",
    "    print('-------------- PROMath ------------')\n",
    "    print(px.recutext_xml(parag))\n",
    "    \n",
    "    joined_xml = etree.parse(join_path)\n",
    "    art_basename = article_name.split('/')[-1]\n",
    "    joined_ind = joined_xml.xpath('./article[@name=\"{}\"]'.format(art_basename))[0]\n",
    "    joined_para = joined_ind[parag_ind]\n",
    "    article_name_in_join = joined_ind.get('name')\n",
    "    print(f'---------------- JOINED {article_name_in_join}---------------')\n",
    "    print(etree.tostring(joined_para))\n",
    "            \n",
    "check_sync(1, 5, 'math95/9505_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [01:08<00:00,  8.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary Key=term and Value=(article_address, parag_index)\n",
    "#results = argot.xpath(\".//dfndum[text()='isothermic']\")\n",
    "#results = argot.xpath(\".//dfndum[contains(text(),'quan')]\")\n",
    "results = argot.xpath(\".//dfndum\")\n",
    "term_pair_dict = defaultdict(list)\n",
    "for r in results:\n",
    "    parent = r.getparent()\n",
    "    para_index = int(parent.get('index'))\n",
    "    grand_parent = parent.getparent()\n",
    "    gparent_name = grand_parent.get('name')\n",
    "    term_pair_dict[normalize_phrase(r.text)].append((gparent_name, para_index))\n",
    "    \n",
    "term_para_dict = defaultdict(list)\n",
    "with tarfile.open(prom_path) as tar_fobj:\n",
    "    for term, pair_lst in tqdm(term_pair_dict.items()):\n",
    "        for pair in pair_lst:\n",
    "            xml_fobj = tar_fobj.extractfile(pair[0])\n",
    "            art = etree.parse(xml_fobj)\n",
    "            para_lst = art.findall('.//latexml:para', namespaces=ns)\n",
    "            para_tag = para_lst[pair[1]]\n",
    "            try:\n",
    "                #p_tag = etree.tostring(\n",
    "                #    para_tag.find('.//latexml:p', namespaces=ns)).decode('utf-8').strip()\n",
    "                p_tag = etree.tostring(\n",
    "                         para_tag).decode('utf-8').strip()\n",
    "            except TypeError as e:\n",
    "                p_tag = px.recutext_xml(para_tag)\n",
    "                print(e)\n",
    "            term_para_dict[term].append((*pair, p_tag))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"filename '1501_005/1501.02441/Tiling.xml' not found\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8a09251154b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mget_para\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1501_005/1501.02441/Tiling.xml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-8a09251154b9>\u001b[0m in \u001b[0;36mget_para\u001b[0;34m(art_addr, para, tar_path, run_recutext)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_para\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart_addr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpara\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprom_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_recutext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar_fobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mxml_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtar_fobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_fobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpara_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.//latexml:para'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mextractfile\u001b[0;34m(self, member)\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2095\u001b[0;31m             \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2096\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m             \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mgetmember\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getmember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filename %r not found\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"filename '1501_005/1501.02441/Tiling.xml' not found\""
     ]
    }
   ],
   "source": [
    "def get_para(art_addr, para, tar_path=prom_path, run_recutext=True):\n",
    "    # Get a paragraph from an article compressed in a tar file\n",
    "    with tarfile.open(tar_path) as tar_fobj:\n",
    "        xml_fobj = tar_fobj.extractfile(art_addr)\n",
    "        art = etree.parse(xml_fobj)\n",
    "        para_lst = art.findall('.//latexml:para', namespaces=ns)\n",
    "        para_tag = para_lst[para]\n",
    "        #p_tag = etree.tostring(\n",
    "        #    para_tag.find('latexml:p', namespaces=ns)).decode('utf-8')\n",
    "        if run_recutext:\n",
    "            return px.recutext_xml(para_tag)\n",
    "        else:\n",
    "            return etree.tostring(p_tag).decode('utf-8')\n",
    "get_para(\"1501_005/1501.02441/Tiling.xml\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<dfndum>Locally covariant quantum field</dfndum>'\n",
      "article 1501_005/1501.02682/1501.02682.xml 131\n",
      " Locally covariant quantum field theory _citation_ describes QFT on a category of globally hyperbolic spacetimes _inline_math_. Fixing a spacetime dimension _inline_math_, objects of _inline_math_ are quadruples _inline_math_ where _inline_math_ is a smooth paracompact orientable nonempty _inline_math_-manifold with finitely many connected components, _inline_math_ is a smooth time-orientable metric of signature _inline_math_ on _inline_math_, _inline_math_ and _inline_math_ are choices of orientation and %****␣1501.02682.tex␣Line␣600␣**** time-orientation respectively, 1 The orientation (resp., time-orientation) is conveniently represented as a choice of one of the connected components of the nowhere-zero smooth _inline_math_-forms (resp., _inline_math_-timelike _inline_math_-forms) on _inline_math_. so that the spacetime _inline_math_ is globally hyperbolic. That is, _inline_math_ has no closed causal curves and the intersections _inline_math_ of the causal future of _inline_math_ with the causal past of _inline_math_ is compact (including the possibility of being empty) for any pair of points _inline_math_. A morphism between two objects _inline_math_ and _inline_math_ of _inline_math_ is any smooth embedding _inline_math_ that is isometric, preserves the (time)orientation (i.e., _inline_math_, _inline_math_, _inline_math_) and has a causally convex image. If the image contains a Cauchy surface of _inline_math_, _inline_math_ will be described as a Cauchy morphism. \n",
      "----------------------------\n",
      "b'<dfndum>quantifies the density</dfndum>'\n",
      "article 1501_005/1501.02861/ordinal-embedding-v10-post-BEJ.xml 167\n",
      " Define _inline_math_, which quantifies the density of _inline_math_ in _inline_math_. Because _inline_math_ and _inline_math_ is dense in _inline_math_, we have _inline_math_ as _inline_math_. \n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# search for term containing some text and get the paragraphs (cleaned)\n",
    "results = argot.xpath(\".//dfndum[contains(text(),'quant')]\")\n",
    "with tarfile.open(prom_path) as tar_fobj:\n",
    "    for r in results:\n",
    "        print(etree.tostring(r))\n",
    "        parent = r.getparent()\n",
    "        para_index = int(parent.get('index'))\n",
    "        grand_parent = parent.getparent()\n",
    "        gparent_name = grand_parent.get('name')\n",
    "        print(grand_parent.tag, gparent_name, grand_parent.get('num') )\n",
    "        xml_fobj = tar_fobj.extractfile(gparent_name)\n",
    "        #art = etree.parse(xml_fobj.extractfile())\n",
    "        art = etree.parse(xml_fobj)\n",
    "        para_lst = art.findall('.//latexml:para', namespaces=ns)\n",
    "        print(px.recutext_xml(para_lst[para_index]))\n",
    "        print('----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/luis/rm_me/output/culito_04.pickle', 'rb') as fobj:\n",
    "    lst = pickle.load(fobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 13, 14, 15, 16]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
