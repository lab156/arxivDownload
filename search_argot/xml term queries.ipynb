{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1) create a search table with key=term and value paragragh in raw LaTeXML output\n",
    "\n",
    "2) a term-reference is {term, art_addr, parag_index, p_tag, tfidf}\n",
    "\n",
    "3) serialize the list of term-references\n",
    "\n",
    "TODO:\n",
    "- Some `<para>` tags don't have a `<p>` inside so I just used recutext. *ADD* the missing p tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import pickle\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "from marshmallow import Schema, fields, pprint\n",
    "from random import random\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import os, sys, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, os.path.join(parentdir, 'embed'))\n",
    "sys.path.insert(0, parentdir)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from clean_and_token_text import normalize_text, normalize_phrase, join_xml_para_and_write, ReadGlossary\n",
    "import parsing_xml as px\n",
    "import peep_tar as peep\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_path = 'math20/2003_003'\n",
    "argot_path = '/media/hd1/glossary/NN.v1/' + general_path + '.xml.gz'\n",
    "prom_path = '/media/hd1/promath/' + general_path + '.tar.gz'\n",
    "join_path = '/media/hd1/cleaned_text/joined_math12-34_04-01/' + general_path + 'xml.gz'\n",
    "ns = {'latexml': 'http://dlmf.nist.gov/LaTeXML' }\n",
    "\n",
    "argot = etree.parse(argot_path)\n",
    "with tarfile.open(prom_path) as tar_fobj:\n",
    "    art_lst = [k.get_info()['name'] for k in tar_fobj.getmembers()]\n",
    "    members = tar_fobj.getmembers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2816 [00:00<00:14, 186.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [2816, 2816] files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2816/2816 [01:00<00:00, 46.78it/s] \n",
      "100%|██████████| 2816/2816 [01:00<00:00, 46.19it/s] \n",
      "100%|██████████| 342/342 [1:11:45<00:00, 12.59s/it]\n"
     ]
    }
   ],
   "source": [
    "glossary_file_lst = glob.glob('/media/hd1/glossary/NN.v1/math9*/*.xml.gz')\n",
    "glossary_file_lst += glob.glob('/media/hd1/glossary/NN.v1/math0*/*.xml.gz')\n",
    "\n",
    "#RG = ReadGlossary('/media/hd1/glossary/v3/math*/*.xml.gz', '/media/hd1/glossary/NN.v1/math*/*')\n",
    "#vocab = RG.ntc_intersect('relative')\n",
    "\n",
    "corpus = []\n",
    "term_ref_lst = []\n",
    "for glossary_file in tqdm(glossary_file_lst):\n",
    "    # argot_file format: /media/hd1/glossary/NN.v1/math95/9506_001.xml.gz\n",
    "    math_year = glossary_file.split('/')[-2]\n",
    "    subfilename = glossary_file.split('/')[-1].split('.')[0] # should be 9506_001\n",
    "    promath_file = os.path.join('/media/hd1/promath', math_year, subfilename + '.tar.gz')\n",
    "    joined_file = os.path.join('/media/hd1/cleaned_text/joined_math12-34_04-01/',\n",
    "                               math_year, subfilename + '.xml.gz')\n",
    "    \n",
    "    glossary_xml = etree.parse(glossary_file)\n",
    "    joined_xml = etree.parse(joined_file)\n",
    "    promath_tarfobj = tarfile.open(promath_file)\n",
    "    for art in glossary_xml.findall('./article'):\n",
    "        art_name = art.get('name')\n",
    "        # need to use only the basename of the article\n",
    "        art_basename = os.path.basename(art_name)\n",
    "        joined_name_results = joined_xml.xpath('./article[@name=\"{}\"]'.format(art_basename))\n",
    "        # POPULATE THE CORPUS\n",
    "        if len(joined_name_results) > 0:\n",
    "            joined_art_text = ''\n",
    "            for parag in joined_name_results[0].findall('./parag'):\n",
    "                text = '' if parag.text is None else parag.text\n",
    "                joined_art_text += (text + ' ')\n",
    "            corpus.append(joined_art_text)\n",
    "        else:\n",
    "            print(f'joined name search results empty art_name={art_name}')\n",
    "            corpus.append('')\n",
    "        corpus_index = len(corpus) - 1\n",
    "        \n",
    "        # OPEN THE PROMATH FILE\n",
    "        try:\n",
    "            #promath_obj = peep.tar(promath_file, art_name)[1].exml\n",
    "            promath_obj = px.DefinitionsXML(promath_tarfobj.extractfile(art_name))\n",
    "        except AttributeError:\n",
    "            print(f'{art_name} gave attributeError')\n",
    "        # this list is in sync with glossary paragraph index\n",
    "        promath_parag_lst = promath_obj.para_list()\n",
    "        \n",
    "        # LOOP THROUGH THE DFDUMS\n",
    "        for defin in art.findall('./definition'):\n",
    "            p_index = int(defin.get('index'))\n",
    "            for term_raw in defin.findall('./dfndum'):\n",
    "                term = normalize_text(term_raw.text)\n",
    "                if term in vocab:\n",
    "                    term_ref_lst.append((\n",
    "                           term,\n",
    "                            art_name,\n",
    "                            p_index,\n",
    "                            etree.tostring(promath_parag_lst[p_index]).decode('utf-8'),\n",
    "                            corpus_index))\n",
    "    promath_tarfobj.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 1.03 s, total: 2min 48s\n",
      "Wall time: 2min 48s\n",
      "The shape of the resulting matrix is: (83840, 347047)\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE THE TDIDF MATRIX\n",
    "vocab_ = list(set([t.replace(' ', '_') for t in vocab]))\n",
    "tvect = TfidfVectorizer(sublinear_tf=True, norm='l1', vocabulary=vocab_)\n",
    "                \n",
    "%time ttrans = tvect.fit_transform(corpus)\n",
    "print(f'The shape of the resulting matrix is: {ttrans.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00614318996893933"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tfidf(term_, corpus_index_, vocab_):\n",
    "    ter = term_.replace(' ', '_')\n",
    "    tindex = vocab_.index(ter)\n",
    "    return ttrans[corpus_index_, tindex]\n",
    "\n",
    "get_tfidf('_inline_math_', 6112, vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TermReference(Schema):\n",
    "    term : str\n",
    "    addr : str\n",
    "    index : int\n",
    "    p_tag : str\n",
    "    tfidf : float = field(default_factory=random)\n",
    "    \n",
    "class TermRefSchema(Schema):\n",
    "    term = fields.String()\n",
    "    addr = fields.String()\n",
    "    index = fields.Int()\n",
    "    p_tag = fields.String()\n",
    "    tfidf = fields.Float()\n",
    "\n",
    "TR_lst=[]\n",
    "for tr in term_ref_lst[500_000:510_000]:\n",
    "    TR_lst.append(TermReference(\n",
    "        term= tr[0],\n",
    "           addr = tr[1],\n",
    "           index = tr[2],\n",
    "          p_tag = tr[3],\n",
    "           tfidf = get_tfidf(tr[0], tr[4], vocab_)))\n",
    "\n",
    "# Serialize all the data and save to json file\n",
    "trs = TermRefSchema(many=True)\n",
    "with open('/home/luis/rm_me/math_json/'+ 'half_10K' + '.json', 'w') as fobj:\n",
    "    fobj.write(trs.dumps(TR_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ ARGOT 9505_001/math.9505216/math.9505216.xml -------------- \n",
      "b'<definition index=\"15\">\\n      <stmnt> Given _inline_math_, _inline_math_ is in _inline_math_ if the following is true. _inline_math_ is a disjoint decomposition. _inline_math_ and _inline_math_ are in _inline_math_ where _inline_math_, _inline_math_. There is _inline_math_, an isomorphism between _inline_math_ and _inline_math_. _inline_math_ is the identity. For _inline_math_ the sets _inline_math_ and _inline_math_ are isomorphic. _inline_math_. _inline_math_ is one-to-one and takes only values outside _inline_math_ (which is the same as _inline_math_). _inline_math_. We make _inline_math_ and the ordering on _inline_math_ is the one generated by this. </stmnt>\\n    <dfndum>disjoint decomposition</dfndum><dfndum>isomorphic</dfndum><dfndum>one-to-one</dfndum></definition>\\n  '\n",
      "-------------- PROMath ------------\n",
      " Given _inline_math_, _inline_math_ is in _inline_math_ if the following is true. _inline_math_ is a disjoint decomposition. _inline_math_ and _inline_math_ are in _inline_math_ where _inline_math_, _inline_math_. There is _inline_math_, an isomorphism between _inline_math_ and _inline_math_. _inline_math_ is the identity. For _inline_math_ the sets _inline_math_ and _inline_math_ are isomorphic. _inline_math_. _inline_math_ is one-to-one and takes only values outside _inline_math_ (which is the same as _inline_math_). _inline_math_. We make _inline_math_ and the ordering on _inline_math_ is the one generated by this. \n",
      "---------------- JOINED math.9505216.xml---------------\n",
      "b'<parag index=\"15\">given _inline_math_ _inline_math_ is in _inline_math_ if the following is true _inline_math_ is a disjoint_decomposition _inline_math_ and _inline_math_ are in _inline_math_ where _inline_math_ _inline_math_ there is _inline_math_ an isomorphism between _inline_math_ and _inline_math_ _inline_math_ is the identity for _inline_math_ the sets _inline_math_ and _inline_math_ are isomorphic _inline_math_ _inline_math_ is one_to_one and takes only values outside _inline_math_ which is the same as _inline_math_ _inline_math_ we make _inline_math_ and the ordering on _inline_math_ is the one_generated by this </parag>'\n"
     ]
    }
   ],
   "source": [
    "# check if it is possible to find the same paragraph in promath, argot-glossary, joined\n",
    "def check_sync(parag_index, art_ind_in_argot, general_path_):\n",
    "    argot_path = '/media/hd1/glossary/NN.v1/' + general_path_ + '.xml.gz'\n",
    "    prom_path = '/media/hd1/promath/' + general_path_ + '.tar.gz'\n",
    "    join_path = '/media/hd1/cleaned_text/joined_math12-34_04-01/' +\\\n",
    "                                general_path_ + '.xml.gz'\n",
    "    \n",
    "    # first get the paragraph index from argot\n",
    "    argot_xml = etree.parse(argot_path)\n",
    "    article_ind = argot_xml.getroot()[art_ind_in_argot]\n",
    "    argot_def = article_ind[parag_index]\n",
    "    article_name = article_ind.get('name')\n",
    "    print(f'------------ ARGOT {article_name} -------------- ')\n",
    "    print(etree.tostring(argot_def))\n",
    "    parag_ind = int(argot_def.get('index'))\n",
    "    \n",
    "    #for k,tarobj in enumerate(peep.tar_iter(prom_path, '.xml')):\n",
    "    #    if k == art_ind_in_argot:\n",
    "    \n",
    "    #promath_obj = etree.parse(tarobj[1])\n",
    "    promath_obj = peep.tar(prom_path, article_name)[1].exml\n",
    "    parag = promath_obj.findall('.//latexml:para', namespaces=ns)[parag_ind]\n",
    "    print('-------------- PROMath ------------')\n",
    "    print(px.recutext_xml(parag))\n",
    "    \n",
    "    joined_xml = etree.parse(join_path)\n",
    "    art_basename = article_name.split('/')[-1]\n",
    "    joined_ind = joined_xml.xpath('./article[@name=\"{}\"]'.format(art_basename))[0]\n",
    "    joined_para = joined_ind[parag_ind]\n",
    "    article_name_in_join = joined_ind.get('name')\n",
    "    print(f'---------------- JOINED {article_name_in_join}---------------')\n",
    "    print(etree.tostring(joined_para))\n",
    "            \n",
    "check_sync(1, 5, 'math95/9505_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"filename '1501_005/1501.02441/Tiling.xml' not found\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8a09251154b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mget_para\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1501_005/1501.02441/Tiling.xml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-8a09251154b9>\u001b[0m in \u001b[0;36mget_para\u001b[0;34m(art_addr, para, tar_path, run_recutext)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_para\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart_addr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpara\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprom_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_recutext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar_fobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mxml_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtar_fobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_fobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpara_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.//latexml:para'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mextractfile\u001b[0;34m(self, member)\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2095\u001b[0;31m             \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2096\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m             \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mgetmember\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getmember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filename %r not found\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"filename '1501_005/1501.02441/Tiling.xml' not found\""
     ]
    }
   ],
   "source": [
    "def get_para(art_addr, para, tar_path=prom_path, run_recutext=True):\n",
    "    # Get a paragraph from an article compressed in a tar file\n",
    "    with tarfile.open(tar_path) as tar_fobj:\n",
    "        xml_fobj = tar_fobj.extractfile(art_addr)\n",
    "        art = etree.parse(xml_fobj)\n",
    "        para_lst = art.findall('.//latexml:para', namespaces=ns)\n",
    "        para_tag = para_lst[para]\n",
    "        #p_tag = etree.tostring(\n",
    "        #    para_tag.find('latexml:p', namespaces=ns)).decode('utf-8')\n",
    "        if run_recutext:\n",
    "            return px.recutext_xml(para_tag)\n",
    "        else:\n",
    "            return etree.tostring(p_tag).decode('utf-8')\n",
    "get_para(\"1501_005/1501.02441/Tiling.xml\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [01:08<00:00,  8.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary Key=term and Value=(article_address, parag_index)\n",
    "#results = argot.xpath(\".//dfndum[text()='isothermic']\")\n",
    "#results = argot.xpath(\".//dfndum[contains(text(),'quan')]\")\n",
    "results = argot.xpath(\".//dfndum\")\n",
    "term_pair_dict = defaultdict(list)\n",
    "for r in results:\n",
    "    parent = r.getparent()\n",
    "    para_index = int(parent.get('index'))\n",
    "    grand_parent = parent.getparent()\n",
    "    gparent_name = grand_parent.get('name')\n",
    "    term_pair_dict[normalize_phrase(r.text)].append((gparent_name, para_index))\n",
    "    \n",
    "term_para_dict = defaultdict(list)\n",
    "with tarfile.open(prom_path) as tar_fobj:\n",
    "    for term, pair_lst in tqdm(term_pair_dict.items()):\n",
    "        for pair in pair_lst:\n",
    "            xml_fobj = tar_fobj.extractfile(pair[0])\n",
    "            art = etree.parse(xml_fobj)\n",
    "            para_lst = art.findall('.//latexml:para', namespaces=ns)\n",
    "            para_tag = para_lst[pair[1]]\n",
    "            try:\n",
    "                #p_tag = etree.tostring(\n",
    "                #    para_tag.find('.//latexml:p', namespaces=ns)).decode('utf-8').strip()\n",
    "                p_tag = etree.tostring(\n",
    "                         para_tag).decode('utf-8').strip()\n",
    "            except TypeError as e:\n",
    "                p_tag = px.recutext_xml(para_tag)\n",
    "                print(e)\n",
    "            term_para_dict[term].append((*pair, p_tag))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<dfndum>Locally covariant quantum field</dfndum>'\n",
      "article 1501_005/1501.02682/1501.02682.xml 131\n",
      " Locally covariant quantum field theory _citation_ describes QFT on a category of globally hyperbolic spacetimes _inline_math_. Fixing a spacetime dimension _inline_math_, objects of _inline_math_ are quadruples _inline_math_ where _inline_math_ is a smooth paracompact orientable nonempty _inline_math_-manifold with finitely many connected components, _inline_math_ is a smooth time-orientable metric of signature _inline_math_ on _inline_math_, _inline_math_ and _inline_math_ are choices of orientation and %****␣1501.02682.tex␣Line␣600␣**** time-orientation respectively, 1 The orientation (resp., time-orientation) is conveniently represented as a choice of one of the connected components of the nowhere-zero smooth _inline_math_-forms (resp., _inline_math_-timelike _inline_math_-forms) on _inline_math_. so that the spacetime _inline_math_ is globally hyperbolic. That is, _inline_math_ has no closed causal curves and the intersections _inline_math_ of the causal future of _inline_math_ with the causal past of _inline_math_ is compact (including the possibility of being empty) for any pair of points _inline_math_. A morphism between two objects _inline_math_ and _inline_math_ of _inline_math_ is any smooth embedding _inline_math_ that is isometric, preserves the (time)orientation (i.e., _inline_math_, _inline_math_, _inline_math_) and has a causally convex image. If the image contains a Cauchy surface of _inline_math_, _inline_math_ will be described as a Cauchy morphism. \n",
      "----------------------------\n",
      "b'<dfndum>quantifies the density</dfndum>'\n",
      "article 1501_005/1501.02861/ordinal-embedding-v10-post-BEJ.xml 167\n",
      " Define _inline_math_, which quantifies the density of _inline_math_ in _inline_math_. Because _inline_math_ and _inline_math_ is dense in _inline_math_, we have _inline_math_ as _inline_math_. \n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# search for term containing some text and get the paragraphs (cleaned)\n",
    "results = argot.xpath(\".//dfndum[contains(text(),'quant')]\")\n",
    "with tarfile.open(prom_path) as tar_fobj:\n",
    "    for r in results:\n",
    "        print(etree.tostring(r))\n",
    "        parent = r.getparent()\n",
    "        para_index = int(parent.get('index'))\n",
    "        grand_parent = parent.getparent()\n",
    "        gparent_name = grand_parent.get('name')\n",
    "        print(grand_parent.tag, gparent_name, grand_parent.get('num') )\n",
    "        xml_fobj = tar_fobj.extractfile(gparent_name)\n",
    "        #art = etree.parse(xml_fobj.extractfile())\n",
    "        art = etree.parse(xml_fobj)\n",
    "        para_lst = art.findall('.//latexml:para', namespaces=ns)\n",
    "        print(px.recutext_xml(para_lst[para_index]))\n",
    "        print('----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML ParseError -- 9710_001/math.9710210/oldamstex.xml -- Start tag expected, '<' not found, line 2, column 1 (9710_001.tar.gz, line 2)\n",
      "The file  9710_001/physics.9710002/physics.9710002.xml  is empty.\n",
      "{'good': 25, 'empty': 1, 'bad': 1}\n"
     ]
    }
   ],
   "source": [
    "join_xml_para_and_write('/home/luis/rm_me_parsing/9710_001.tar.gz', '/home/luis/rm_me_parsing/out/', lambda s: s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
