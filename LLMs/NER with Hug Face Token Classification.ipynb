{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925cb2ce-3506-4f7f-9738-22a526901db4",
   "metadata": {},
   "source": [
    "# Token Classification with Hugging Face\n",
    "Based on the tutorial\n",
    "https://huggingface.co/docs/transformers/tasks/token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff65e479-c777-44d4-b49e-6de6faeb5c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 10:50:44.346666: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-07 10:50:44.426299: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-07 10:50:44.525388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-07 10:50:44.594752: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-07 10:50:44.614480: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-07 10:50:44.742519: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-07 10:50:45.879330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/luis/code/arxivDownload/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.optimization_tf because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/code/arxivDownload/venv/lib/python3.10/site-packages/transformers/activations_tf.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/code/arxivDownload/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1586\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0rc1/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/code/arxivDownload/venv/lib/python3.10/site-packages/transformers/optimization_tf.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# This block because Keras loves randomly moving things to different places - this changed somewhere between 2.10 - 2.15\u001b[39;00m\n",
      "File \u001b[0;32m~/code/arxivDownload/venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:38\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n",
      "File \u001b[0;32m~/code/arxivDownload/venv/lib/python3.10/site-packages/transformers/activations_tf.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[0;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset, DatasetDict\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AutoTokenizer,\n\u001b[1;32m      6\u001b[0m                           DataCollatorForTokenClassification,\n\u001b[1;32m      7\u001b[0m                           create_optimizer,\n\u001b[1;32m      8\u001b[0m                           TFAutoModelForTokenClassification,\n\u001b[1;32m      9\u001b[0m                           pipeline,\n\u001b[1;32m     10\u001b[0m                          )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_callbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasMetricCallback\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/code/arxivDownload/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1576\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1576\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1577\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/arxivDownload/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1588\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1589\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1590\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1591\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.optimization_tf because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          DataCollatorForTokenClassification,\n",
    "                          create_optimizer,\n",
    "                          TFAutoModelForTokenClassification,\n",
    "                          pipeline,\n",
    "                         )\n",
    "\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "import evaluate\n",
    "from nltk import word_tokenize\n",
    "from lxml import etree\n",
    "\n",
    "import sys, os\n",
    "currentdir = os.path.abspath(os.path.curdir)\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "sys.path.insert(0,parentdir+'/embed') \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import train_ner as tn\n",
    "import ner\n",
    "import ner.llm_utils as llu\n",
    "import embed.inference_ner as iner\n",
    "import LLMs.mp_infer_HFTrans_ner as mpner\n",
    "\n",
    "from nltk.chunk import conlltags2tree, ChunkScore\n",
    "\n",
    "import toml\n",
    "\n",
    "seqeval = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520ba6b3-54f2-40c6-a534-32b5ea64c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the problematic article is: examples.xml\n",
      "The name of the problematic article is: coding.xml\n",
      "The name of the problematic article is: spaces-pushouts.xml\n",
      "The name of the problematic article is: guide.xml\n",
      "The name of the problematic article is: moduli.xml\n",
      "The name of the problematic article is: more-groupoids.xml\n",
      "The name of the problematic article is: chapters.xml\n",
      "The name of the problematic article is: sets.xml\n",
      "The name of the problematic article is: obsolete.xml\n",
      "The name of the problematic article is: examples-defos.xml\n",
      "The name of the problematic article is: spaces-more-cohomology.xml\n",
      "The name of the problematic article is: bibliography.xml\n",
      "The name of the problematic article is: fdl.xml\n",
      "The name of the problematic article is: limits.xml\n",
      "The name of the problematic article is: conventions.xml\n",
      "The name of the problematic article is: introduction.xml\n",
      "The name of the problematic article is: quot.xml\n",
      "The name of the problematic article is: desirables.xml\n",
      "CPU times: user 3.44 s, sys: 14.2 ms, total: 3.46 s\n",
      "Wall time: 3.45 s\n"
     ]
    }
   ],
   "source": [
    "cfg = tn.gen_cfg()\n",
    "text_lst = tn.get_wiki_pm_stacks_data(cfg)\n",
    "%time sent_tok, trainer_params = tn.gen_sent_tokzer(text_lst, cfg)\n",
    "tokens_lst, ner_tags_lst, title_lst = ner.bio_tag.put_ner_tags(text_lst, sent_tok)\n",
    "def_lst = ner.bio_tag.put_pos_ner_tags(text_lst, sent_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55606240-ff0a-433d-968f-f1da8582c723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'title', 'pos'],\n",
       "        num_rows: 21135\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'title', 'pos'],\n",
       "        num_rows: 2610\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'title', 'pos'],\n",
       "        num_rows: 2349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_lst = [[d[0][1] for d in tree_lst['ner']] for tree_lst in def_lst]\n",
    "data_dict = {\n",
    "    'id': list(range(len(tokens_lst))),\n",
    "    'tokens': tokens_lst,\n",
    "    'ner_tags': ner_tags_lst,\n",
    "    'title': title_lst,\n",
    "    'pos': pos_lst,\n",
    "}\n",
    "ds = Dataset.from_dict(data_dict)\n",
    "temp1_dd = ds.train_test_split(test_size=0.1, shuffle=True)\n",
    "temp2_dd = temp1_dd['train'].train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': temp2_dd['train'],\n",
    "    'test': temp1_dd['test'],\n",
    "    'valid': temp2_dd['test'],\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10eac3e8-31d5-4fac-a676-d4d4a17b7b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 10:26:02.658583: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-23 10:26:02.659482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 10:26:02.659675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 10:26:02.659806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 10:26:03.603872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 10:26:03.604047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 10:26:03.604184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-23 10:26:03.604298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10094 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at /media/hd1/trained_models/ner_model/HFtransformers/ner-2023-08-01_1627/trainer/trans_HF_ner/ner_Aug-01_16-28/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#checkpoint = 'distilbert-base-cased'\n",
    "#checkpoint = 'xlm-roberta-base'\n",
    "checkpoint = 'bert-base-cased'\n",
    "#checkpoint = 'bert-large-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "label_list = ['O', 'B-defndum', 'I-defndum']\n",
    "\n",
    "##checkpoint = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, add_prefix_space=True)\n",
    "model_path = '/media/hd1/trained_models/ner_model/HFtransformers/ner-2023-08-01_1627/trainer/trans_HF_ner/ner_Aug-01_16-28/'\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feaf952e-071d-483f-b9e6-356d7c2f27bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e750e9646f9442490bc528946349f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fbedbe03b24353a3dd44bcba141a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1a5f53ea0d4f09897100494b703660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'],\n",
    "                                truncation=True,\n",
    "                                is_split_into_words=True)\n",
    "    \n",
    "    labels=[]\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "#tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a366cc30-49bc-4535-8ca2-b39f376b91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = [label_list[i] for i in example['ner_tags']]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        'precision': results['overall_precision'],\n",
    "        'recall': results['overall_recall'],\n",
    "        'f1': results['overall_f1'],\n",
    "        'accuracy': results['overall_accuracy'],\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32ecfcf7-b931-48a9-8339-4fe8761bdd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "...     0: \"O\",\n",
    "...     1: \"B-DFNDUM\",\n",
    "...     2: \"I-DFNDUM\",\n",
    "... }\n",
    ">>> label2id = {\n",
    "...     \"O\": 0,\n",
    "...     \"B-DFNDUM\": 1,\n",
    "...     \"I-DFNDUM\": 2,\n",
    "... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff5cc5d-ddb6-44c5-b69a-5525c721f18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_train_epochs = 3\n",
    "num_train_steps = (len(tokenized_ds['train']) // batch_size * num_train_epochs)\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr = 2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ac0c8c9-cb96-4888-9324-dce2cd34c871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "456d2215-cbb6-40da-abe5-fcbf23c21659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/luis/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"valid\"],\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61be65e3-4668-41cc-8790-43351681f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce4432f6-2081-4938-bb62-abf1d1007c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "21154/21154 [==============================] - 2868s 135ms/step - loss: 0.0787 - val_loss: 0.0856 - precision: 0.6460 - recall: 0.6717 - f1: 0.6586 - accuracy: 0.9738\n",
      "Epoch 2/3\n",
      "21154/21154 [==============================] - 2855s 135ms/step - loss: 0.0749 - val_loss: 0.0856 - precision: 0.6460 - recall: 0.6717 - f1: 0.6586 - accuracy: 0.9738\n",
      "Epoch 3/3\n",
      "21154/21154 [==============================] - 2858s 135ms/step - loss: 0.0747 - val_loss: 0.0856 - precision: 0.6460 - recall: 0.6717 - f1: 0.6586 - accuracy: 0.9738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0d0c2b1480>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "callbacks = [metric_callback,]\n",
    "model.fit(x=tf_train_set,\n",
    "          validation_data=tf_validation_set,\n",
    "          epochs=3,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7101efe0-0038-4338-a2d0-7741d47213b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='We define a Banach space as a complete vector normed space.'\n",
      "The pipeline result is:  [{'entity': 'B-DFNDUM', 'score': 0.99795747, 'index': 4, 'word': 'Ban', 'start': 12, 'end': 15}, {'entity': 'I-DFNDUM', 'score': 0.87701976, 'index': 5, 'word': '##ach', 'start': 15, 'end': 18}, {'entity': 'I-DFNDUM', 'score': 0.9935475, 'index': 6, 'word': 'space', 'start': 19, 'end': 24}]\n",
      "[CLS] O\n",
      "We O\n",
      "define O\n",
      "a O\n",
      "Ban B-DFNDUM\n",
      "##ach I-DFNDUM\n",
      "space I-DFNDUM\n",
      "as O\n",
      "a O\n",
      "complete O\n",
      "vector O\n",
      "norm O\n",
      "##ed O\n",
      "space O\n",
      ". O\n",
      "[SEP] O\n",
      "logits=<tf.Tensor: shape=(1, 16, 3), dtype=float32, numpy=\n",
      "array([[[ 3.030289  , -2.0339897 , -2.0566025 ],\n",
      "        [ 7.8431334 , -4.7557993 , -4.332306  ],\n",
      "        [ 7.9283433 , -4.4622803 , -4.4607487 ],\n",
      "        [ 7.735637  , -4.489064  , -3.966631  ],\n",
      "        [-0.56327933,  5.646585  , -4.5539656 ],\n",
      "        [-2.3025973 ,  0.08733091,  2.1395137 ],\n",
      "        [-0.1091496 , -5.2520733 ,  4.9334927 ],\n",
      "        [ 7.901957  , -4.716515  , -4.1931643 ],\n",
      "        [ 7.9019723 , -4.7961826 , -4.1535664 ],\n",
      "        [ 7.08629   , -3.0740774 , -4.472606  ],\n",
      "        [ 6.874388  , -3.749901  , -3.6858654 ],\n",
      "        [ 7.2444654 , -5.137642  , -2.73001   ],\n",
      "        [ 7.477514  , -5.177245  , -3.2656243 ],\n",
      "        [ 7.408746  , -5.2392225 , -3.0462787 ],\n",
      "        [ 7.9188957 , -4.8342133 , -4.2293916 ],\n",
      "        [ 3.42609   , -1.9353966 , -1.8082595 ]]], dtype=float32)>\n",
      "[{'entity': 'DFNDUM', 'score': 0.37021726317470893, 'word': 'Banach space', 'start': 12, 'end': 24}]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"We define a Banach space as a complete vector normed space.\"\"\"\n",
    "#text = ''\n",
    "#j = 19\n",
    "#for t in ds['test'][j]['tokens']:\n",
    "#    text += t + ' '\n",
    "print(f'{text=}')\n",
    "#print(f'The real title is: ', ' '.join([ds['test'][j]['tokens'][k]\n",
    "#                         for k, n in enumerate(ds['test'][j]['ner_tags']) if n != 0]))\n",
    "classifier = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "print('The pipeline result is: ', classifier(text))\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='tf')\n",
    "logits = model(**inputs).logits\n",
    "predicted_ids = tf.math.argmax(logits, axis=-1)\n",
    "predicted_token_class = [model.config.id2label[t] for t in predicted_ids[0].numpy().tolist()]\n",
    "\n",
    "for i in range(len(predicted_token_class)):\n",
    "    print(inputs.tokens()[i], predicted_token_class[i])\n",
    "\n",
    "#tt = tokenizer(ds['test'][j]['tokens'], return_tensors='tf', is_split_into_words=True)\n",
    "tt = tokenizer(text, return_tensors='tf', is_split_into_words=False)\n",
    "logits = model(**tt).logits\n",
    "print(f\"{logits=}\")\n",
    "\n",
    "# Grouping entities\n",
    "predicted_ids = tf.math.argmax(logits, axis=-1)[0]\n",
    "predictions = predicted_ids.numpy().tolist()\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(text, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets['offset_mapping']\n",
    "\n",
    "probs = tf.math.softmax(logits, axis=-1)[0]\n",
    "probs = probs.numpy().tolist()\n",
    "\n",
    "#start, end = inputs.word_to_chars(10)\n",
    "end = 0\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != 'O':\n",
    "        label = label[2:]\n",
    "        start, end = offsets[idx] # 2nd output is the end of word\n",
    "        #idx += 1\n",
    "        \n",
    "        # Grab all tokens labeled with an I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]][2:] == label\n",
    "               ):\n",
    "            all_scores.append(probs[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "            \n",
    "        score = np.mean(all_scores).item()\n",
    "        word = text[start:end]\n",
    "        results.append(\n",
    "            {'entity': label, \n",
    "             'score': score,\n",
    "             'word': word,\n",
    "            'start': start,\n",
    "            'end': end,}\n",
    "        )\n",
    "    idx += 1\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e19a3c27-bae6-4074-a0f5-4382ca640a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#xml_path = '/media/hd1/glossary/inference_class_all/math01/0109_001.xml.gz'\n",
    "xml_path = '/media/hd1/glossary/HFTrans_infer_all/math12/1203_002.xml.gz'\n",
    "model_dir = '/media/hd1/trained_models/ner_model/lstm_ner/ner_Sep-29_15-37/exp_041/'\n",
    "sent_tok = iner.read_sent_tok(os.path.join(model_dir, 'punkt_params.pickle'))\n",
    "root_ret = mpner.prep_raw_data_and_mine(xml_path, sent_tok, tokenizer, {\n",
    "'inference_batch_size': 200, 'max_length': 200}, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b3e8456-4d58-4f91-8dc1-b05cfe7cfac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3), (3, 6)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aws = 4\n",
    "ibs = 3\n",
    "[(s*ibs, (s+1)*ibs) for s in range(aws//ibs + 1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f008a8-1cf8-43be-99ee-3010321f8d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7181be-b55e-4bd9-bf57-45db2080b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<definition index=\"40\">\n",
      "      <stmnt> Let _inline_math_ be a compact orientable manifold and let _inline_math_ be a family of disjoint annuli not lying on the toric components of _inline_math_. Let _inline_math_ be obtained from _inline_math_ by removing _inline_math_ and the toric components of _inline_math_. We define a partially truncated triangulation of _inline_math_ to be a realization of _inline_math_ as a gluing of some _inline_math_&#8217;s along a pairing of the lateral hexagons induced by a simplicial pairing of the faces of the _inline_math_&#8217;s. </stmnt>\n",
      "    <dfndum>partially truncated triangulation</dfndum></definition>\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "elem = root_ret.findall('.//definition')\n",
    "print(etree.tostring(elem[2079],pretty_print=True).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "89271ae7-7009-4aab-b827-ccc4994b75a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]',                      'O'),\n",
      "('We',                      'O'),\n",
      "('say',                      'O'),\n",
      "('a',                      'O'),\n",
      "('forcing',                      'O'),\n",
      "('notion',                      'O'),\n",
      "('_',                      'O'),\n",
      "('in',                      'O'),\n",
      "('##line',                      'O'),\n",
      "('_',                      'O'),\n",
      "('math',                      'O'),\n",
      "('_',                      'O'),\n",
      "('is',                      'O'),\n",
      "('La',                      'O'),\n",
      "('##ver',                      'O'),\n",
      "('-',                      'O'),\n",
      "('like',                      'O'),\n",
      "('if',                      'O'),\n",
      "('there',                      'O'),\n",
      "('is',                      'O'),\n",
      "('a',                      'O'),\n",
      "('_',                      'O'),\n",
      "('in',                      'O'),\n",
      "('##line',                      'O'),\n",
      "('_',                      'O'),\n",
      "('math',                      'O'),\n",
      "('_',                      'O'),\n",
      "('-',                      'O'),\n",
      "('name',                      'O'),\n",
      "('_',                      'O'),\n",
      "('in',                      'O'),\n",
      "('##line',                      'O'),\n",
      "('_',                      'O'),\n",
      "('math',                      'O'),\n",
      "('_',                      'O'),\n",
      "('for',                      'O'),\n",
      "('a',                      'O'),\n",
      "('dominating',                      'O'),\n",
      "('real',                      'O'),\n",
      "('such',                      'O'),\n",
      "('that',                      'O'),\n",
      "('(',                      'O'),\n",
      "('i',                      'O'),\n",
      "(')',                      'O'),\n",
      "('the',                      'O'),\n",
      "('complete',                      'O'),\n",
      "('Bo',                      'O'),\n",
      "('##ole',                      'O'),\n",
      "('##an',                      'O'),\n",
      "('algebra',                      'O'),\n",
      "('generated',                      'O'),\n",
      "('by',                      'O'),\n",
      "('the',                      'O'),\n",
      "('family',                      'O'),\n",
      "('_',                      'O'),\n",
      "('in',                      'O'),\n",
      "('##line',                      'O'),\n",
      "('_',                      'O'),\n",
      "('math',                      'O'),\n",
      "('_',                      'O'),\n",
      "('equals',                      'O'),\n",
      "('r',                      'O'),\n",
      "('.',                      'O'),\n",
      "('o',                      'O'),\n",
      "('.',                      'O'),\n",
      "('[SEP]',                      'O'),\n",
      "------------------------------------\n",
      "('[CLS]',                      'O'),\n",
      "('(',                      'O'),\n",
      "('_',                      'O'),\n",
      "('in',                      'O'),\n",
      "('##line',                      'O'),\n",
      "('_',                      'O'),\n",
      "('math',                      'O'),\n",
      "('_',                      'O'),\n",
      "(')',                      'O'),\n",
      "(',',                      'O'),\n",
      "('and',                      'O'),\n",
      "('(',                      'O'),\n",
      "('ii',                      'O'),\n",
      "(')',                      'O'),\n",
      "('for',                      'O'),\n",
      "('each',                      'O'),\n",
      "('condition',                      'O'),\n",
      "('_',                      'O'),\n",
      "('in',                      'O'),\n",
      "('##line',                      'O'),\n",
      "('_',                      'O'),\n",
      "('math',                      'O'),\n",
      "('_',                      'O'),\n",
      "('there',                      'O'),\n",
      "('exists',                      'O'),\n",
      "('a',                      'O'),\n",
      "('La',                      'B-DFNDUM'),\n",
      "('##ver',                      'I-DFNDUM'),\n",
      "('-',                      'I-DFNDUM'),\n",
      "('tree',                      'I-DFNDUM'),\n",
      "('_',                      'O'),\n",
      "('in',                      'O'),\n",
      "('##line',                      'O'),\n",
      "('_',                      'O'),\n",
      "('math',                      'O'),\n",
      "('_',                      'O'),\n",
      "('so',                      'O'),\n",
      "('that',                      'O'),\n",
      "('_',                      'O'),\n",
      "('display',                      'O'),\n",
      "('_',                      'O'),\n",
      "('math',                      'O'),\n",
      "('_',                      'O'),\n",
      "('We',                      'O'),\n",
      "('express',                      'O'),\n",
      "('this',                      'O'),\n",
      "('by',                      'O'),\n",
      "('saying',                      'O'),\n",
      "('_',                      'O'),\n",
      "('in',                      'O'),\n",
      "('##line',                      'O'),\n",
      "('_',                      'O'),\n",
      "('math',                      'O'),\n",
      "('_',                      'O'),\n",
      "('where',                      'O'),\n",
      "('_',                      'O'),\n",
      "('in',                      'O'),\n",
      "('##line',                      'O'),\n",
      "('_',                      'O'),\n",
      "('math',                      'O'),\n",
      "('_',                      'O'),\n",
      "(')',                      'O'),\n",
      "('.',                      'O'),\n",
      "('[SEP]',                      'O'),\n",
      "('[PAD]',                      'O'),\n",
      "('[PAD]',                      'O'),\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pars = etree.XMLParser(recover=True)                                           \n",
    "xml_tree = etree.parse(xml_path, parser=pars)                                  \n",
    "root = xml_tree.getroot()\n",
    "Defs = root.findall('.//definition')   \n",
    "\n",
    "#sent_lst = iner.str_tok_pos_tags(Defs[12].find('stmnt').text, sent_tok )\n",
    "sent_lst = sent_tok.tokenize(Defs[1061].find('stmnt').text)\n",
    "\n",
    "group_tok = []\n",
    "\n",
    "empty = ''\n",
    "\n",
    "tt = tokenizer(sent_lst, return_tensors='tf', is_split_into_words=False, padding=True, truncation=True)\n",
    "logits = model(**tt)['logits']\n",
    "predicted_ids = tf.math.argmax(logits, axis=-1)\n",
    "predictions = predicted_ids.numpy()\n",
    "for j,p in enumerate(predictions):\n",
    "    label_lst = [model.config.id2label[t] for t in p]\n",
    "    for k,lab in enumerate(label_lst):\n",
    "        print(f\"('{tt.tokens(j)[k]}', {empty:>20} '{lab}'),\")\n",
    "    print('------------------------------------')\n",
    "#join1 = llu.get_words_back(tt.tokens(1))[0]\n",
    "#llu.join_math_tokens(join1)[0]\n",
    "#join1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f5d171-7a18-48f6-abe3-40e9260f907d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m test_lst \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-DFNDUM\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      2\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      3\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-DFNDUM\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI-DFNDUM\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     11\u001b[0m            (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     13\u001b[0m w, p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtest_lst)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mllu\u001b[49m\u001b[38;5;241m.\u001b[39mget_entity(w, p)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llu' is not defined"
     ]
    }
   ],
   "source": [
    "test_lst = [('a', 'B-DFNDUM'),\n",
    "            ('b', 'O'),\n",
    "            ('b', 'B-DFNDUM'),\n",
    "            ('b', 'O'),\n",
    "            ('b', 'O'),\n",
    "            ('b', 'O'),\n",
    "            ('b', 'O'),\n",
    "            ('b', 'B-DFNDUM'),\n",
    "            ('b', 'I-DFNDUM'),\n",
    "            ('b', 'I-DFNDUM'),\n",
    "           ('b', 'O')]\n",
    "            \n",
    "w, p = zip(*test_lst)\n",
    "llu.get_entity(w, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84cff041-ec3a-445d-acde-527829b11de2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  99.5%%\n",
      "    Precision:     94.8%%\n",
      "    Recall:        91.6%%\n",
      "    F-Measure:     93.2%%\n"
     ]
    }
   ],
   "source": [
    "#j=780\n",
    "chunkscore = ChunkScore()\n",
    "\n",
    "spec_toks = list(tokenizer.special_tokens_map.values())\n",
    "spec_toks.remove('[UNK]')\n",
    "#spec_toks.remove('<unk>')\n",
    "\n",
    "\n",
    "dif_len_lst = []\n",
    "for j in range(len(ds['test'])):\n",
    "    tt = tokenizer(ds['test'][j]['tokens'], return_tensors='tf', is_split_into_words=True)\n",
    "    logits = model(**tt).logits\n",
    "\n",
    "    # Grouping entities\n",
    "    predicted_ids = tf.math.argmax(logits, axis=-1)[0]\n",
    "    predictions = predicted_ids.numpy().tolist()\n",
    "    pp = [model.config.id2label[t] for t in predictions]\n",
    "\n",
    "    wl, il = llu.get_words_back(tt.tokens(),\n",
    "                          preds=pp, special_tokens=spec_toks)\n",
    "    try:\n",
    "        wl, il = llu.join_by_example(wl, ds['test'][j]['tokens'], preds=il)\n",
    "    except AssertionError:\n",
    "        print(f'Index {j=} caused the error')\n",
    "\n",
    "    tree_pred = conlltags2tree([(tok, 'Upa', pred) for tok, pred in zip(wl, il)])\n",
    "\n",
    "    jdict = ds['test'][j]\n",
    "    bio_tagged = tn.tf_bio_tagger(jdict['ner_tags'])\n",
    "    tree_gold = conlltags2tree([(jdict['tokens'][i], \n",
    "                                 'Upa', \n",
    "                                 bio_tagged[i])\n",
    "                                for i in range(len(jdict['tokens']))])\n",
    "\n",
    "    chunkscore.score(tree_pred, tree_gold)\n",
    "    \n",
    "    if len(wl) != len(jdict['tokens']):\n",
    "        dif_len_lst.append(j)\n",
    "#for i in range(len(il)):\n",
    "    #print(f\"{wl[i]:<15} {il[i]:<10} {jdict['tokens'][i]:<15} {bio_tagged[i]}\")\n",
    "    #print(f\"{wl[i]:<20} {il[i]:<10}\") \n",
    "print(chunkscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb888d9-7414-4b1f-bb2c-6fd6ef1c80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ad6a3-f2c4-41ee-b21f-001189e35827",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(ds['test'])):\n",
    "    if ['There', 'are', 'two', 'approaches'] == ds['test'][j]['tokens'][:4]:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713da924-719a-4ab4-89df-415074d0b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=142\n",
    "tt = tokenizer(ds['test'][j]['tokens'], return_tensors='tf', is_split_into_words=True)\n",
    "logits = model(**tt).logits\n",
    "\n",
    "# Grouping entities\n",
    "predicted_ids = tf.math.argmax(logits, axis=-1)[0]\n",
    "predictions = predicted_ids.numpy().tolist()\n",
    "pp = [model.config.id2label[t] for t in predictions]\n",
    "\n",
    "wl, il = ner.llm_utils.get_words_back(tt.tokens(),\n",
    "                      preds=pp, special_tokens=tokenizer.special_tokens_map.values())\n",
    "#wl, il = ner.llm_utils.join_math_tokens(wl, il)\n",
    "\n",
    "tree_pred = conlltags2tree([(tok, 'Upa', pred) for tok, pred in zip(wl, il)])\n",
    "\n",
    "jdict = ds['test'][j]\n",
    "bio_tagged = tn.tf_bio_tagger(jdict['ner_tags'])\n",
    "tree_gold = conlltags2tree([(jdict['tokens'][i], \n",
    "                             'Upa', \n",
    "                             bio_tagged[i])\n",
    "                            for i in range(len(jdict['tokens']))])\n",
    "\n",
    "#for i in range(len(il)):\n",
    "#    print(f\"{wl[i]:<15} {il[i]:<10} {jdict['tokens'][i]:<15} {bio_tagged[i]}\")\n",
    "    #print(f\"{wl[i]:<20} {il[i]:<10}\") \n",
    "\n",
    "joined_toks, joined_preds = llu.join_by_example(wl, jdict['tokens'], preds=il)\n",
    "short_gold = ['1', '\\\\in', 'c', 'a', 'be', 'fin']\n",
    "short_pred = ['1', '\\\\', 'in', 'c', 'a', 'b', 'e', 'fin']\n",
    "#joined_preds = join_by_example(short_pred, short_gold)\n",
    "for i in range(len(joined_toks)):\n",
    "    print(f\"{i}  {joined_toks[i]:<15} {joined_preds[i]:<10} {jdict['tokens'][i]:<15} {bio_tagged[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c31d44-1eed-4081-8f92-bd6d7a052927",
   "metadata": {},
   "source": [
    "## Previous Results\n",
    "**bert-base-cased**\n",
    "```text\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  97.7%%\n",
    "    Precision:     74.3%%\n",
    "    Recall:        69.9%%\n",
    "    F-Measure:     72.0%%\n",
    "```\n",
    "\n",
    "```text\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  97.9%%\n",
    "    Precision:     77.9%%\n",
    "    Recall:        72.3%%\n",
    "    F-Measure:     75.0%%\n",
    "```\n",
    "\n",
    "```text\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  98.2%%\n",
    "    Precision:     80.1%%\n",
    "    Recall:        76.7%%\n",
    "    F-Measure:     78.4%%\n",
    "```\n",
    "\n",
    "**bert-large-cased** (Bridges)\n",
    "\n",
    "5 Epochs\n",
    "```text\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  98.3%%\n",
    "    Precision:     82.7%%\n",
    "    Recall:        79.4%%\n",
    "    F-Measure:     81.0%%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7154c2-eb82-4448-a905-d167dbce6292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
