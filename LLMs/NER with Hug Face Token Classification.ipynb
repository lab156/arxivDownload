{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925cb2ce-3506-4f7f-9738-22a526901db4",
   "metadata": {},
   "source": [
    "# Token Classification with Hugging Face\n",
    "Based on the tutorial\n",
    "https://huggingface.co/docs/transformers/tasks/token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff65e479-c777-44d4-b49e-6de6faeb5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          DataCollatorForTokenClassification,\n",
    "                          create_optimizer,\n",
    "                          TFAutoModelForTokenClassification,\n",
    "                          pipeline,\n",
    "                         )\n",
    "\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "import evaluate\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import sys, os\n",
    "currentdir = os.path.abspath(os.path.curdir)\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "sys.path.insert(0,parentdir+'/embed') \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import train_ner as tn\n",
    "import ner\n",
    "import ner.llm_utils as llu\n",
    "\n",
    "from nltk.chunk import conlltags2tree, ChunkScore\n",
    "\n",
    "import toml\n",
    "\n",
    "seqeval = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520ba6b3-54f2-40c6-a534-32b5ea64c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the problematic article is: examples.xml\n",
      "The name of the problematic article is: coding.xml\n",
      "The name of the problematic article is: spaces-pushouts.xml\n",
      "The name of the problematic article is: guide.xml\n",
      "The name of the problematic article is: moduli.xml\n",
      "The name of the problematic article is: more-groupoids.xml\n",
      "The name of the problematic article is: chapters.xml\n",
      "The name of the problematic article is: sets.xml\n",
      "The name of the problematic article is: obsolete.xml\n",
      "The name of the problematic article is: examples-defos.xml\n",
      "The name of the problematic article is: spaces-more-cohomology.xml\n",
      "The name of the problematic article is: bibliography.xml\n",
      "The name of the problematic article is: fdl.xml\n",
      "The name of the problematic article is: limits.xml\n",
      "The name of the problematic article is: conventions.xml\n",
      "The name of the problematic article is: introduction.xml\n",
      "The name of the problematic article is: quot.xml\n",
      "The name of the problematic article is: desirables.xml\n"
     ]
    }
   ],
   "source": [
    "cfg = tn.gen_cfg()\n",
    "text_lst = tn.get_wiki_pm_stacks_data(cfg)\n",
    "sent_tok, trainer_params = tn.gen_sent_tokzer(text_lst, cfg)\n",
    "tokens_lst, ner_tags_lst, title_lst = ner.bio_tag.put_ner_tags(text_lst, sent_tok)\n",
    "def_lst = ner.bio_tag.put_pos_ner_tags(text_lst, sent_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55606240-ff0a-433d-968f-f1da8582c723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'title', 'pos'],\n",
       "        num_rows: 21141\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'title', 'pos'],\n",
       "        num_rows: 2611\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'title', 'pos'],\n",
       "        num_rows: 2350\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_lst = [[d[0][1] for d in tree_lst['ner']] for tree_lst in def_lst]\n",
    "data_dict = {\n",
    "    'id': list(range(len(tokens_lst))),\n",
    "    'tokens': tokens_lst,\n",
    "    'ner_tags': ner_tags_lst,\n",
    "    'title': title_lst,\n",
    "    'pos': pos_lst,\n",
    "}\n",
    "ds = Dataset.from_dict(data_dict)\n",
    "temp1_dd = ds.train_test_split(test_size=0.1, shuffle=True)\n",
    "temp2_dd = temp1_dd['train'].train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': temp2_dd['train'],\n",
    "    'test': temp1_dd['test'],\n",
    "    'valid': temp2_dd['test'],\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10eac3e8-31d5-4fac-a676-d4d4a17b7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-cased'\n",
    "checkpoint = 'bert-base-cased'\n",
    "#checkpoint = 'bert-large-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "label_list = ['O', 'B-defndum', 'I-defndum']\n",
    "\n",
    "##checkpoint = 'gpt2'\n",
    "##tokenizer = AutoTokenizer.from_pretrained(checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff164890-d47a-4cd7-b06b-dad1c28427ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'In',\n",
       " 'an',\n",
       " 'overview',\n",
       " 'presented',\n",
       " 'on',\n",
       " 'its',\n",
       " 'website',\n",
       " 'in',\n",
       " '2008',\n",
       " 'the',\n",
       " 'Su',\n",
       " '##sta',\n",
       " '##ina',\n",
       " '##bility',\n",
       " 'Science',\n",
       " 'Program',\n",
       " 'at',\n",
       " 'Harvard',\n",
       " 'University',\n",
       " 'described',\n",
       " 'the',\n",
       " 'field',\n",
       " 'in',\n",
       " 'the',\n",
       " 'following',\n",
       " 'way',\n",
       " ',',\n",
       " 'stress',\n",
       " '##ing',\n",
       " 'its',\n",
       " 'inter',\n",
       " '##dis',\n",
       " '##ci',\n",
       " '##p',\n",
       " '##lina',\n",
       " '##rity',\n",
       " ':',\n",
       " \"'\",\n",
       " 'Su',\n",
       " '##sta',\n",
       " '##ina',\n",
       " '##bility',\n",
       " 'science',\n",
       " \"'\",\n",
       " 'is',\n",
       " 'problem',\n",
       " '-',\n",
       " 'driven',\n",
       " ',',\n",
       " 'interdisciplinary',\n",
       " 'scholarship',\n",
       " 'that',\n",
       " 'seeks',\n",
       " 'to',\n",
       " 'facilitate',\n",
       " 'the',\n",
       " 'design',\n",
       " ',',\n",
       " 'implementation',\n",
       " ',',\n",
       " 'and',\n",
       " 'evaluation',\n",
       " 'of',\n",
       " 'effective',\n",
       " 'interventions',\n",
       " 'that',\n",
       " 'foster',\n",
       " 'shared',\n",
       " 'prosperity',\n",
       " 'and',\n",
       " 'reduced',\n",
       " 'poverty',\n",
       " 'while',\n",
       " 'protecting',\n",
       " 'the',\n",
       " 'environment',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example = wnut['train'][0]\n",
    "example = ds['train'][10]\n",
    "\n",
    "tok_input = tokenizer(example['tokens'], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tok_input['input_ids'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feaf952e-071d-483f-b9e6-356d7c2f27bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121f1c206efa4561bbd389f6d3f05a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05b068f25fe4bbd86c7caeec89d0fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c429d63644bf4a8c84f5c0b1cd542041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'],\n",
    "                                truncation=True,\n",
    "                                is_split_into_words=True)\n",
    "    \n",
    "    labels=[]\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "#tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a366cc30-49bc-4535-8ca2-b39f376b91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = [label_list[i] for i in example['ner_tags']]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        'precision': results['overall_precision'],\n",
    "        'recall': results['overall_recall'],\n",
    "        'f1': results['overall_f1'],\n",
    "        'accuracy': results['overall_accuracy'],\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ecfcf7-b931-48a9-8339-4fe8761bdd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "...     0: \"O\",\n",
    "...     1: \"B-DFNDUM\",\n",
    "...     2: \"I-DFNDUM\",\n",
    "... }\n",
    ">>> label2id = {\n",
    "...     \"O\": 0,\n",
    "...     \"B-DFNDUM\": 1,\n",
    "...     \"I-DFNDUM\": 2,\n",
    "... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff5cc5d-ddb6-44c5-b69a-5525c721f18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 13:42:57.882085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-27 13:42:57.887266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-27 13:42:57.887412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-27 13:42:57.888157: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-27 13:42:57.888636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-27 13:42:57.888786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-27 13:42:57.888917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-27 13:42:59.099462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-27 13:42:59.099636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-27 13:42:59.099767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-27 13:42:59.099882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10094 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_train_epochs = 3\n",
    "num_train_steps = (len(tokenized_ds['train']) // batch_size * num_train_epochs)\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr = 2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "456d2215-cbb6-40da-abe5-fcbf23c21659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/luis/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"valid\"],\n",
    "    shuffle=False,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61be65e3-4668-41cc-8790-43351681f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce4432f6-2081-4938-bb62-abf1d1007c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5285/5285 [==============================] - 483s 89ms/step - loss: 0.0826 - val_loss: 0.0699 - precision: 0.6699 - recall: 0.6729 - f1: 0.6714 - accuracy: 0.9741\n",
      "Epoch 2/3\n",
      "5285/5285 [==============================] - 467s 88ms/step - loss: 0.0589 - val_loss: 0.0699 - precision: 0.6699 - recall: 0.6729 - f1: 0.6714 - accuracy: 0.9741\n",
      "Epoch 3/3\n",
      "5285/5285 [==============================] - 468s 89ms/step - loss: 0.0588 - val_loss: 0.0699 - precision: 0.6699 - recall: 0.6729 - f1: 0.6714 - accuracy: 0.9741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff958763ca0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "callbacks = [metric_callback,]\n",
    "model.fit(x=tf_train_set,\n",
    "          validation_data=tf_validation_set,\n",
    "          epochs=3,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7101efe0-0038-4338-a2d0-7741d47213b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Formally , a transition system is a pair ( S , - ) where S is a set of states and - is a set of state transitions ( i.e. , a subset of S x S ) . '\n",
      "The real title is:  transition system\n",
      "The pipeline result is:  [{'entity': 'B-DFNDUM', 'score': 0.9698083, 'index': 5, 'word': 'transition', 'start': 13, 'end': 23}, {'entity': 'I-DFNDUM', 'score': 0.9639087, 'index': 6, 'word': 'system', 'start': 24, 'end': 30}]\n",
      "[CLS] O\n",
      "Form O\n",
      "##ally O\n",
      ", O\n",
      "a O\n",
      "transition B-DFNDUM\n",
      "system I-DFNDUM\n",
      "is O\n",
      "a O\n",
      "pair O\n",
      "( O\n",
      "S O\n",
      ", O\n",
      "- O\n",
      ") O\n",
      "where O\n",
      "S O\n",
      "is O\n",
      "a O\n",
      "set O\n",
      "of O\n",
      "states O\n",
      "and O\n",
      "- O\n",
      "is O\n",
      "a O\n",
      "set O\n",
      "of O\n",
      "state O\n",
      "transitions O\n",
      "( O\n",
      "i O\n",
      ". O\n",
      "e O\n",
      ". O\n",
      ", O\n",
      "a O\n",
      "subset O\n",
      "of O\n",
      "S O\n",
      "x O\n",
      "S O\n",
      ") O\n",
      ". O\n",
      "[SEP] O\n",
      "[{'entity': 'DFNDUM', 'score': 0.48646090587135404, 'word': 'transition system', 'start': 13, 'end': 30}]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"We define a Banach space as a complete vector normed space.\"\"\"\n",
    "text = ''\n",
    "j = 19\n",
    "for t in ds['test'][j]['tokens']:\n",
    "    text += t + ' '\n",
    "print(f'{text=}')\n",
    "print(f'The real title is: ', ' '.join([ds['test'][j]['tokens'][k]\n",
    "                                        for k, n in enumerate(ds['test'][j]['ner_tags']) if n != 0]))\n",
    "classifier = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "print('The pipeline result is: ', classifier(text))\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='tf')\n",
    "logits = model(**inputs).logits\n",
    "predicted_ids = tf.math.argmax(logits, axis=-1)\n",
    "predicted_token_class = [model.config.id2label[t] for t in predicted_ids[0].numpy().tolist()]\n",
    "\n",
    "for i in range(len(predicted_token_class)):\n",
    "    print(inputs.tokens()[i], predicted_token_class[i])\n",
    "\n",
    "tt = tokenizer(ds['test'][j]['tokens'], return_tensors='tf', is_split_into_words=True)\n",
    "logits = model(**tt).logits\n",
    "\n",
    "# Grouping entities\n",
    "predicted_ids = tf.math.argmax(logits, axis=-1)[0]\n",
    "predictions = predicted_ids.numpy().tolist()\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(text, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets['offset_mapping']\n",
    "\n",
    "probs = tf.math.softmax(logits, axis=-1)[0]\n",
    "probs = probs.numpy().tolist()\n",
    "\n",
    "#start, end = inputs.word_to_chars(10)\n",
    "end = 0\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != 'O':\n",
    "        label = label[2:]\n",
    "        start, end = offsets[idx] # 2nd output is the end of word\n",
    "        #idx += 1\n",
    "        \n",
    "        # Grab all tokens labeled with an I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]][2:] == label\n",
    "               ):\n",
    "            all_scores.append(probs[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "            \n",
    "        score = np.mean(all_scores).item()\n",
    "        word = text[start:end]\n",
    "        results.append(\n",
    "            {'entity': label, \n",
    "             'score': score,\n",
    "             'word': word,\n",
    "            'start': start,\n",
    "            'end': end,}\n",
    "        )\n",
    "    idx += 1\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84cff041-ec3a-445d-acde-527829b11de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  97.5%%\n",
      "    Precision:     68.5%%\n",
      "    Recall:        67.7%%\n",
      "    F-Measure:     68.1%%\n"
     ]
    }
   ],
   "source": [
    "#j=780\n",
    "chunkscore = ChunkScore()\n",
    "\n",
    "spec_toks = list(tokenizer.special_tokens_map.values())\n",
    "spec_toks.remove('[UNK]')\n",
    "\n",
    "\n",
    "dif_len_lst = []\n",
    "for j in range(len(ds['test'])):\n",
    "    tt = tokenizer(ds['test'][j]['tokens'], return_tensors='tf', is_split_into_words=True)\n",
    "    logits = model(**tt).logits\n",
    "\n",
    "    # Grouping entities\n",
    "    predicted_ids = tf.math.argmax(logits, axis=-1)[0]\n",
    "    predictions = predicted_ids.numpy().tolist()\n",
    "    pp = [model.config.id2label[t] for t in predictions]\n",
    "\n",
    "    wl, il = llu.get_words_back(tt.tokens(),\n",
    "                          preds=pp, special_tokens=spec_toks)\n",
    "    try:\n",
    "        wl, il = llu.join_by_example(wl, ds['test'][j]['tokens'], preds=il)\n",
    "    except AssertionError:\n",
    "        print(f'Index {j=} caused the error')\n",
    "\n",
    "    tree_pred = conlltags2tree([(tok, 'Upa', pred) for tok, pred in zip(wl, il)])\n",
    "\n",
    "    jdict = ds['test'][j]\n",
    "    bio_tagged = tn.tf_bio_tagger(jdict['ner_tags'])\n",
    "    tree_gold = conlltags2tree([(jdict['tokens'][i], \n",
    "                                 'Upa', \n",
    "                                 bio_tagged[i])\n",
    "                                for i in range(len(jdict['tokens']))])\n",
    "\n",
    "    chunkscore.score(tree_pred, tree_gold)\n",
    "    \n",
    "    if len(wl) != len(jdict['tokens']):\n",
    "        dif_len_lst.append(j)\n",
    "#for i in range(len(il)):\n",
    "    #print(f\"{wl[i]:<15} {il[i]:<10} {jdict['tokens'][i]:<15} {bio_tagged[i]}\")\n",
    "    #print(f\"{wl[i]:<20} {il[i]:<10}\") \n",
    "print(chunkscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d02ad6a3-f2c4-41ee-b21f-001189e35827",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(ds['test'])):\n",
    "    if ['There', 'are', 'two', 'approaches'] == ds['test'][j]['tokens'][:4]:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "713da924-719a-4ab4-89df-415074d0b9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  We              O          We              O\n",
      "1  say             O          say             O\n",
      "2  that            O          that            O\n",
      "3  the             O          the             O\n",
      "4  tensor          O          tensor          O\n",
      "5  _inline_math_   O          _inline_math_   O\n",
      "6  is              O          is              O\n",
      "7  a               O          a               O\n",
      "8  Codazzi         B-DFNDUM   Codazzi         B-DFNDUM\n",
      "9  Tensor          I-DFNDUM   Tensor          I-DFNDUM\n",
      "10  if              O          if              O\n",
      "11  _inline_math_   O          _inline_math_   O\n",
      "12  .               O          .               O\n"
     ]
    }
   ],
   "source": [
    "j=142\n",
    "tt = tokenizer(ds['test'][j]['tokens'], return_tensors='tf', is_split_into_words=True)\n",
    "logits = model(**tt).logits\n",
    "\n",
    "# Grouping entities\n",
    "predicted_ids = tf.math.argmax(logits, axis=-1)[0]\n",
    "predictions = predicted_ids.numpy().tolist()\n",
    "pp = [model.config.id2label[t] for t in predictions]\n",
    "\n",
    "wl, il = ner.llm_utils.get_words_back(tt.tokens(),\n",
    "                      preds=pp, special_tokens=tokenizer.special_tokens_map.values())\n",
    "#wl, il = ner.llm_utils.join_math_tokens(wl, il)\n",
    "\n",
    "tree_pred = conlltags2tree([(tok, 'Upa', pred) for tok, pred in zip(wl, il)])\n",
    "\n",
    "jdict = ds['test'][j]\n",
    "bio_tagged = tn.tf_bio_tagger(jdict['ner_tags'])\n",
    "tree_gold = conlltags2tree([(jdict['tokens'][i], \n",
    "                             'Upa', \n",
    "                             bio_tagged[i])\n",
    "                            for i in range(len(jdict['tokens']))])\n",
    "\n",
    "#for i in range(len(il)):\n",
    "#    print(f\"{wl[i]:<15} {il[i]:<10} {jdict['tokens'][i]:<15} {bio_tagged[i]}\")\n",
    "    #print(f\"{wl[i]:<20} {il[i]:<10}\") \n",
    "\n",
    "joined_toks, joined_preds = llu.join_by_example(wl, jdict['tokens'], preds=il)\n",
    "short_gold = ['1', '\\\\in', 'c', 'a', 'be', 'fin']\n",
    "short_pred = ['1', '\\\\', 'in', 'c', 'a', 'b', 'e', 'fin']\n",
    "#joined_preds = join_by_example(short_pred, short_gold)\n",
    "for i in range(len(joined_toks)):\n",
    "    print(f\"{i}  {joined_toks[i]:<15} {joined_preds[i]:<10} {jdict['tokens'][i]:<15} {bio_tagged[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb6b592b-c543-492e-b39a-92385ed3545f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\\\in'.startswith('\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c31d44-1eed-4081-8f92-bd6d7a052927",
   "metadata": {},
   "source": [
    "## Previous Results\n",
    "bert-base-cased\n",
    "```text\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  97.7%%\n",
    "    Precision:     74.3%%\n",
    "    Recall:        69.9%%\n",
    "    F-Measure:     72.0%%\n",
    "```\n",
    "\n",
    "```text\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  97.9%%\n",
    "    Precision:     77.9%%\n",
    "    Recall:        72.3%%\n",
    "    F-Measure:     75.0%%\n",
    "```\n",
    "\n",
    "tf_train_set=\n",
    "\n",
    "<_PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(8, None), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(8, None), dtype=tf.int6\n",
    "4, name=None), 'attention_mask': TensorSpec(shape=(8, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(8, None), dtype=tf.int64, name=None))>                            \n",
    "\n",
    "tf_validation_set=\n",
    "\n",
    "<_PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(None, None), dt\n",
    "ype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7154c2-eb82-4448-a905-d167dbce6292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
