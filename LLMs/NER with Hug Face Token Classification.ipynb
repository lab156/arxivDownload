{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925cb2ce-3506-4f7f-9738-22a526901db4",
   "metadata": {},
   "source": [
    "# Token Classification with Hugging Face\n",
    "Based on the tutorial\n",
    "https://huggingface.co/docs/transformers/tasks/token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff65e479-c777-44d4-b49e-6de6faeb5c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          DataCollatorForTokenClassification,\n",
    "                          create_optimizer,\n",
    "                          TFAutoModelForTokenClassification,\n",
    "                          pipeline,\n",
    "                         )\n",
    "\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "import evaluate\n",
    "from nltk import word_tokenize\n",
    "from lxml import etree\n",
    "\n",
    "import sys, os\n",
    "currentdir = os.path.abspath(os.path.curdir)\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "sys.path.insert(0,parentdir+'/embed') \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import train_ner as tn\n",
    "import ner\n",
    "import ner.llm_utils as llu\n",
    "import embed.inference_ner as iner\n",
    "\n",
    "from nltk.chunk import conlltags2tree, ChunkScore\n",
    "\n",
    "import toml\n",
    "\n",
    "seqeval = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520ba6b3-54f2-40c6-a534-32b5ea64c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the problematic article is: examples.xml\n",
      "The name of the problematic article is: coding.xml\n",
      "The name of the problematic article is: spaces-pushouts.xml\n",
      "The name of the problematic article is: guide.xml\n",
      "The name of the problematic article is: moduli.xml\n",
      "The name of the problematic article is: more-groupoids.xml\n",
      "The name of the problematic article is: chapters.xml\n",
      "The name of the problematic article is: sets.xml\n",
      "The name of the problematic article is: obsolete.xml\n",
      "The name of the problematic article is: examples-defos.xml\n",
      "The name of the problematic article is: spaces-more-cohomology.xml\n",
      "The name of the problematic article is: bibliography.xml\n",
      "The name of the problematic article is: fdl.xml\n",
      "The name of the problematic article is: limits.xml\n",
      "The name of the problematic article is: conventions.xml\n",
      "The name of the problematic article is: introduction.xml\n",
      "The name of the problematic article is: quot.xml\n",
      "The name of the problematic article is: desirables.xml\n"
     ]
    }
   ],
   "source": [
    "cfg = tn.gen_cfg()\n",
    "text_lst = tn.get_wiki_pm_stacks_data(cfg)\n",
    "sent_tok, trainer_params = tn.gen_sent_tokzer(text_lst, cfg)\n",
    "tokens_lst, ner_tags_lst, title_lst = ner.bio_tag.put_ner_tags(text_lst, sent_tok)\n",
    "def_lst = ner.bio_tag.put_pos_ner_tags(text_lst, sent_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55606240-ff0a-433d-968f-f1da8582c723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'title', 'pos'],\n",
       "        num_rows: 21133\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'title', 'pos'],\n",
       "        num_rows: 2610\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'title', 'pos'],\n",
       "        num_rows: 2349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_lst = [[d[0][1] for d in tree_lst['ner']] for tree_lst in def_lst]\n",
    "data_dict = {\n",
    "    'id': list(range(len(tokens_lst))),\n",
    "    'tokens': tokens_lst,\n",
    "    'ner_tags': ner_tags_lst,\n",
    "    'title': title_lst,\n",
    "    'pos': pos_lst,\n",
    "}\n",
    "ds = Dataset.from_dict(data_dict)\n",
    "temp1_dd = ds.train_test_split(test_size=0.1, shuffle=True)\n",
    "temp2_dd = temp1_dd['train'].train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': temp2_dd['train'],\n",
    "    'test': temp1_dd['test'],\n",
    "    'valid': temp2_dd['test'],\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10eac3e8-31d5-4fac-a676-d4d4a17b7b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 11:06:01.955462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-18 11:06:01.960107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-18 11:06:01.960251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-18 11:06:01.961439: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-18 11:06:01.962093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-18 11:06:01.962261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-18 11:06:01.962391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-18 11:06:03.541842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-18 11:06:03.542034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-18 11:06:03.542204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-18 11:06:03.544357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10094 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at /media/hd1/trained_models/ner_model/HFtransformers/ner_Jul-29_13-01/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#checkpoint = 'distilbert-base-cased'\n",
    "#checkpoint = 'xlm-roberta-base'\n",
    "checkpoint = 'bert-base-cased'\n",
    "#checkpoint = 'bert-large-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "label_list = ['O', 'B-defndum', 'I-defndum']\n",
    "\n",
    "##checkpoint = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, add_prefix_space=True)\n",
    "model = TFAutoModelForTokenClassification.from_pretrained('/media/hd1/trained_models/ner_model/HFtransformers/ner_Jul-29_13-01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feaf952e-071d-483f-b9e6-356d7c2f27bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576c888fe9ce4478b466ae91651854f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f43963dd534f418a3e425c08d8c8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54bf4f28e5642a6ac7440cd0faae42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'],\n",
    "                                truncation=True,\n",
    "                                is_split_into_words=True)\n",
    "    \n",
    "    labels=[]\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "#tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a366cc30-49bc-4535-8ca2-b39f376b91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = [label_list[i] for i in example['ner_tags']]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        'precision': results['overall_precision'],\n",
    "        'recall': results['overall_recall'],\n",
    "        'f1': results['overall_f1'],\n",
    "        'accuracy': results['overall_accuracy'],\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ecfcf7-b931-48a9-8339-4fe8761bdd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "...     0: \"O\",\n",
    "...     1: \"B-DFNDUM\",\n",
    "...     2: \"I-DFNDUM\",\n",
    "... }\n",
    ">>> label2id = {\n",
    "...     \"O\": 0,\n",
    "...     \"B-DFNDUM\": 1,\n",
    "...     \"I-DFNDUM\": 2,\n",
    "... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff5cc5d-ddb6-44c5-b69a-5525c721f18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 11:11:03.300327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 11:11:03.301498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 11:11:03.303595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 11:11:03.304092: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 11:11:03.304717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 11:11:03.306746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 11:11:03.306879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 11:11:04.258110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 11:11:04.258297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 11:11:04.258437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 11:11:04.258555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10094 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_train_epochs = 3\n",
    "num_train_steps = (len(tokenized_ds['train']) // batch_size * num_train_epochs)\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr = 2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac0c8c9-cb96-4888-9324-dce2cd34c871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456d2215-cbb6-40da-abe5-fcbf23c21659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/luis/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"valid\"],\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61be65e3-4668-41cc-8790-43351681f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce4432f6-2081-4938-bb62-abf1d1007c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "21154/21154 [==============================] - 2868s 135ms/step - loss: 0.0787 - val_loss: 0.0856 - precision: 0.6460 - recall: 0.6717 - f1: 0.6586 - accuracy: 0.9738\n",
      "Epoch 2/3\n",
      "21154/21154 [==============================] - 2855s 135ms/step - loss: 0.0749 - val_loss: 0.0856 - precision: 0.6460 - recall: 0.6717 - f1: 0.6586 - accuracy: 0.9738\n",
      "Epoch 3/3\n",
      "21154/21154 [==============================] - 2858s 135ms/step - loss: 0.0747 - val_loss: 0.0856 - precision: 0.6460 - recall: 0.6717 - f1: 0.6586 - accuracy: 0.9738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0d0c2b1480>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "callbacks = [metric_callback,]\n",
    "model.fit(x=tf_train_set,\n",
    "          validation_data=tf_validation_set,\n",
    "          epochs=3,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7101efe0-0038-4338-a2d0-7741d47213b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='We define a Banach space as a complete vector normed space.'\n",
      "The pipeline result is:  [{'entity': 'B-DFNDUM', 'score': 0.97570115, 'index': 4, 'word': 'Ban', 'start': 12, 'end': 15}, {'entity': 'B-DFNDUM', 'score': 0.6237467, 'index': 5, 'word': '##ach', 'start': 15, 'end': 18}, {'entity': 'I-DFNDUM', 'score': 0.9263092, 'index': 6, 'word': 'space', 'start': 19, 'end': 24}]\n",
      "[CLS] O\n",
      "We O\n",
      "define O\n",
      "a O\n",
      "Ban B-DFNDUM\n",
      "##ach B-DFNDUM\n",
      "space I-DFNDUM\n",
      "as O\n",
      "a O\n",
      "complete O\n",
      "vector O\n",
      "norm O\n",
      "##ed O\n",
      "space O\n",
      ". O\n",
      "[SEP] O\n",
      "logits=<tf.Tensor: shape=(1, 16, 3), dtype=float32, numpy=\n",
      "array([[[ 6.6380153 , -2.3411176 , -4.1314754 ],\n",
      "        [ 7.800408  , -3.4548326 , -4.5448046 ],\n",
      "        [ 7.7304935 , -3.3997264 , -4.616928  ],\n",
      "        [ 7.6429815 , -4.2596283 , -3.432564  ],\n",
      "        [ 0.5696521 ,  4.267405  , -4.7201014 ],\n",
      "        [-0.5182259 ,  1.0240383 ,  0.08067327],\n",
      "        [ 1.1010076 , -4.3981404 ,  3.636421  ],\n",
      "        [ 7.8216286 , -4.117579  , -3.958748  ],\n",
      "        [ 7.9814715 , -4.2781763 , -3.8653607 ],\n",
      "        [ 5.373061  , -0.53150016, -4.7824554 ],\n",
      "        [ 3.4241815 , -1.8004862 , -1.9854198 ],\n",
      "        [ 3.9452474 , -3.1356828 , -0.78057235],\n",
      "        [ 4.2922378 , -3.1994333 , -1.0443592 ],\n",
      "        [ 4.7119217 , -4.029986  , -0.7725887 ],\n",
      "        [ 7.8659587 , -3.737812  , -4.357636  ],\n",
      "        [ 3.734326  , -1.3125079 , -2.4995537 ]]], dtype=float32)>\n",
      "[{'entity': 'DFNDUM', 'score': 0.5332497017613301, 'word': 'Banach space', 'start': 12, 'end': 24}]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"We define a Banach space as a complete vector normed space.\"\"\"\n",
    "#text = ''\n",
    "#j = 19\n",
    "#for t in ds['test'][j]['tokens']:\n",
    "#    text += t + ' '\n",
    "print(f'{text=}')\n",
    "#print(f'The real title is: ', ' '.join([ds['test'][j]['tokens'][k]\n",
    "#                         for k, n in enumerate(ds['test'][j]['ner_tags']) if n != 0]))\n",
    "classifier = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "print('The pipeline result is: ', classifier(text))\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='tf')\n",
    "logits = model(**inputs).logits\n",
    "predicted_ids = tf.math.argmax(logits, axis=-1)\n",
    "predicted_token_class = [model.config.id2label[t] for t in predicted_ids[0].numpy().tolist()]\n",
    "\n",
    "for i in range(len(predicted_token_class)):\n",
    "    print(inputs.tokens()[i], predicted_token_class[i])\n",
    "\n",
    "#tt = tokenizer(ds['test'][j]['tokens'], return_tensors='tf', is_split_into_words=True)\n",
    "tt = tokenizer(text, return_tensors='tf', is_split_into_words=False)\n",
    "logits = model(**tt).logits\n",
    "print(f\"{logits=}\")\n",
    "\n",
    "# Grouping entities\n",
    "predicted_ids = tf.math.argmax(logits, axis=-1)[0]\n",
    "predictions = predicted_ids.numpy().tolist()\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(text, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets['offset_mapping']\n",
    "\n",
    "probs = tf.math.softmax(logits, axis=-1)[0]\n",
    "probs = probs.numpy().tolist()\n",
    "\n",
    "#start, end = inputs.word_to_chars(10)\n",
    "end = 0\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != 'O':\n",
    "        label = label[2:]\n",
    "        start, end = offsets[idx] # 2nd output is the end of word\n",
    "        #idx += 1\n",
    "        \n",
    "        # Grab all tokens labeled with an I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]][2:] == label\n",
    "               ):\n",
    "            all_scores.append(probs[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "            \n",
    "        score = np.mean(all_scores).item()\n",
    "        word = text[start:end]\n",
    "        results.append(\n",
    "            {'entity': label, \n",
    "             'score': score,\n",
    "             'word': word,\n",
    "            'start': start,\n",
    "            'end': end,}\n",
    "        )\n",
    "    idx += 1\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89271ae7-7009-4aab-b827-ccc4994b75a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m Defs \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//definition\u001b[39m\u001b[38;5;124m'\u001b[39m)   \n\u001b[1;32m      9\u001b[0m sent_lst \u001b[38;5;241m=\u001b[39m iner\u001b[38;5;241m.\u001b[39mstr_tok_pos_tags(Defs[\u001b[38;5;241m12\u001b[39m]\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstmnt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext, sent_tok )\n\u001b[0;32m---> 11\u001b[0m tt \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2520\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2519\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2520\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2578\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2575\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2581\u001b[0m     )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2587\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "xml_path = '/media/hd1/glossary/inference_class_all/math00/0006_001.xml.gz'\n",
    "model_dir = '/media/hd1/trained_models/ner_model/lstm_ner/ner_Sep-29_15-37/exp_041/'\n",
    "sent_tok = iner.read_sent_tok(os.path.join(model_dir, 'punkt_params.pickle'))\n",
    "pars = etree.XMLParser(recover=True)                                           \n",
    "xml_tree = etree.parse(xml_path, parser=pars)                                  \n",
    "root = xml_tree.getroot()\n",
    "Defs = root.findall('.//definition')   \n",
    "\n",
    "sent_lst = iner.str_tok_pos_tags(Defs[12].find('stmnt').text, sent_tok )\n",
    "\n",
    "tt = tokenizer(sent_lst, return_tensors='tf', is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84cff041-ec3a-445d-acde-527829b11de2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  99.5%%\n",
      "    Precision:     94.8%%\n",
      "    Recall:        91.6%%\n",
      "    F-Measure:     93.2%%\n"
     ]
    }
   ],
   "source": [
    "#j=780\n",
    "chunkscore = ChunkScore()\n",
    "\n",
    "spec_toks = list(tokenizer.special_tokens_map.values())\n",
    "spec_toks.remove('[UNK]')\n",
    "#spec_toks.remove('<unk>')\n",
    "\n",
    "\n",
    "dif_len_lst = []\n",
    "for j in range(len(ds['test'])):\n",
    "    tt = tokenizer(ds['test'][j]['tokens'], return_tensors='tf', is_split_into_words=True)\n",
    "    logits = model(**tt).logits\n",
    "\n",
    "    # Grouping entities\n",
    "    predicted_ids = tf.math.argmax(logits, axis=-1)[0]\n",
    "    predictions = predicted_ids.numpy().tolist()\n",
    "    pp = [model.config.id2label[t] for t in predictions]\n",
    "\n",
    "    wl, il = llu.get_words_back(tt.tokens(),\n",
    "                          preds=pp, special_tokens=spec_toks)\n",
    "    try:\n",
    "        wl, il = llu.join_by_example(wl, ds['test'][j]['tokens'], preds=il)\n",
    "    except AssertionError:\n",
    "        print(f'Index {j=} caused the error')\n",
    "\n",
    "    tree_pred = conlltags2tree([(tok, 'Upa', pred) for tok, pred in zip(wl, il)])\n",
    "\n",
    "    jdict = ds['test'][j]\n",
    "    bio_tagged = tn.tf_bio_tagger(jdict['ner_tags'])\n",
    "    tree_gold = conlltags2tree([(jdict['tokens'][i], \n",
    "                                 'Upa', \n",
    "                                 bio_tagged[i])\n",
    "                                for i in range(len(jdict['tokens']))])\n",
    "\n",
    "    chunkscore.score(tree_pred, tree_gold)\n",
    "    \n",
    "    if len(wl) != len(jdict['tokens']):\n",
    "        dif_len_lst.append(j)\n",
    "#for i in range(len(il)):\n",
    "    #print(f\"{wl[i]:<15} {il[i]:<10} {jdict['tokens'][i]:<15} {bio_tagged[i]}\")\n",
    "    #print(f\"{wl[i]:<20} {il[i]:<10}\") \n",
    "print(chunkscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb888d9-7414-4b1f-bb2c-6fd6ef1c80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ad6a3-f2c4-41ee-b21f-001189e35827",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(ds['test'])):\n",
    "    if ['There', 'are', 'two', 'approaches'] == ds['test'][j]['tokens'][:4]:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713da924-719a-4ab4-89df-415074d0b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=142\n",
    "tt = tokenizer(ds['test'][j]['tokens'], return_tensors='tf', is_split_into_words=True)\n",
    "logits = model(**tt).logits\n",
    "\n",
    "# Grouping entities\n",
    "predicted_ids = tf.math.argmax(logits, axis=-1)[0]\n",
    "predictions = predicted_ids.numpy().tolist()\n",
    "pp = [model.config.id2label[t] for t in predictions]\n",
    "\n",
    "wl, il = ner.llm_utils.get_words_back(tt.tokens(),\n",
    "                      preds=pp, special_tokens=tokenizer.special_tokens_map.values())\n",
    "#wl, il = ner.llm_utils.join_math_tokens(wl, il)\n",
    "\n",
    "tree_pred = conlltags2tree([(tok, 'Upa', pred) for tok, pred in zip(wl, il)])\n",
    "\n",
    "jdict = ds['test'][j]\n",
    "bio_tagged = tn.tf_bio_tagger(jdict['ner_tags'])\n",
    "tree_gold = conlltags2tree([(jdict['tokens'][i], \n",
    "                             'Upa', \n",
    "                             bio_tagged[i])\n",
    "                            for i in range(len(jdict['tokens']))])\n",
    "\n",
    "#for i in range(len(il)):\n",
    "#    print(f\"{wl[i]:<15} {il[i]:<10} {jdict['tokens'][i]:<15} {bio_tagged[i]}\")\n",
    "    #print(f\"{wl[i]:<20} {il[i]:<10}\") \n",
    "\n",
    "joined_toks, joined_preds = llu.join_by_example(wl, jdict['tokens'], preds=il)\n",
    "short_gold = ['1', '\\\\in', 'c', 'a', 'be', 'fin']\n",
    "short_pred = ['1', '\\\\', 'in', 'c', 'a', 'b', 'e', 'fin']\n",
    "#joined_preds = join_by_example(short_pred, short_gold)\n",
    "for i in range(len(joined_toks)):\n",
    "    print(f\"{i}  {joined_toks[i]:<15} {joined_preds[i]:<10} {jdict['tokens'][i]:<15} {bio_tagged[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c31d44-1eed-4081-8f92-bd6d7a052927",
   "metadata": {},
   "source": [
    "## Previous Results\n",
    "**bert-base-cased**\n",
    "```text\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  97.7%%\n",
    "    Precision:     74.3%%\n",
    "    Recall:        69.9%%\n",
    "    F-Measure:     72.0%%\n",
    "```\n",
    "\n",
    "```text\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  97.9%%\n",
    "    Precision:     77.9%%\n",
    "    Recall:        72.3%%\n",
    "    F-Measure:     75.0%%\n",
    "```\n",
    "\n",
    "```text\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  98.2%%\n",
    "    Precision:     80.1%%\n",
    "    Recall:        76.7%%\n",
    "    F-Measure:     78.4%%\n",
    "```\n",
    "\n",
    "**bert-large-cased** (Bridges)\n",
    "\n",
    "5 Epochs\n",
    "```text\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  98.3%%\n",
    "    Precision:     82.7%%\n",
    "    Recall:        79.4%%\n",
    "    F-Measure:     81.0%%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7154c2-eb82-4448-a905-d167dbce6292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
