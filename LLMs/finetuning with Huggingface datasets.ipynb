{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3192d772-2bbb-4597-8a90-c58d1d84feff",
   "metadata": {},
   "source": [
    "# Loading data using Hugging Face datasets methods\n",
    "\n",
    "https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html\n",
    "\n",
    "https://huggingface.co/learn/nlp-course/chapter5/5?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2e1621-79d6-4683-95cd-0ed00aaefa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import load_dataset, Dataset, DatasetDict \n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import toml\n",
    "import json\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                         TFAutoModelForSequenceClassification,\n",
    "                         DataCollatorWithPadding,\n",
    "                         TFPreTrainedModel,)\n",
    "\n",
    "# keras for training\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "currentdir = os.path.abspath(os.path.curdir)\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "sys.path.insert(0,parentdir+'/embed') \n",
    "from classifier_trainer.trainer import stream_arxiv_paragraphs\n",
    "from extract import Definiendum\n",
    "import parsing_xml as px\n",
    "import peep_tar as peep\n",
    "\n",
    "from train_lstm import gen_cfg, find_best_cutoff\n",
    "args = []\n",
    "#xml_lst, cfg = gen_cfg(config_path='../config.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f9979e-8094-4f8a-943d-49f4f2d9e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = toml.load('../config.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8748cd74-b1e6-4493-b5a5-130ede0ee41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = cfg['finetuning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "019554e5-7562-4199-9cc4-641fda1b1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandClf():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def predict(self, lst, **kwargs):                                                 \n",
    "        np.random.seed(seed=42)\n",
    "        return np.zeros(len(lst))                                               \n",
    "        \n",
    "class FarseVectorizer():\n",
    "    def __init__(self):                                                               \n",
    "        pass                                                                          \n",
    "    def transform(self, lst):\n",
    "        return [0.0 for _ in lst]\n",
    "\n",
    "class FarseBio():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def parse(self, lst):\n",
    "        return [0.0 for _ in lst]\n",
    "\n",
    "tarpath = '/media/hd1/promath/math11/1105_005.tar.gz'\n",
    "tar_iter = peep.tar_iter(tarpath, '.xml')\n",
    "fname, tobj = next(tar_iter)\n",
    "parsing_obj = px.DefinitionsXML(tobj)\n",
    "clf = RandClf()\n",
    "vtr = FarseVectorizer()\n",
    "dd = Definiendum(parsing_obj, clf, None, vtr, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fe4aebf-03ec-43f8-a94a-0b87a24272a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 4, 5, 7, 8, 9, 10, 11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.first_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad078a-94ee-46ce-b7f0-ba36ebdddf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_lst = glob.glob('/media/hd1/training_defs/math18/*.xml.gz')\n",
    "#xml_lst = xml_lst[:len(xml_lst)//4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86645a35-45d0-4e46-a1b5-299d23f10383",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = stream_arxiv_paragraphs(xml_lst, samples=cfg['batch_size'])\n",
    "\n",
    "all_data = []\n",
    "all_labels = []\n",
    "all_texts = []\n",
    "for s in stream:\n",
    "    try:\n",
    "        #all_data += list(zip(s[0], s[1]))\n",
    "        all_texts += s[0]\n",
    "        all_labels += s[1]\n",
    "    except IndexError:\n",
    "        logger.warning('Index error in the data stream.')\n",
    "data_dict = {\n",
    "    'text': all_texts,\n",
    "    'label': all_labels\n",
    "}\n",
    "ds = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6ff4c0-10bb-4a92-a31c-cce72b16df99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 139787\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1dd680c-524a-4a23-b8b8-0055046d98f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick check the loading of the model\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "#model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "#sequences = all_texts[:10]\n",
    "#batch = dict(\n",
    "#    tokenizer(sequences, return_tensors='tf',\n",
    "#             padding=True, truncation=True)\n",
    "#)\n",
    "#model.compile(\n",
    "#    optimizer='adam',\n",
    "#    loss='binary_crossentropy'\n",
    "#)\n",
    "#labels = tf.convert_to_tensor(all_labels[:10])\n",
    "#model.train_on_batch(batch, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2801a191-4714-4b86-8e3b-c7d2e5a47fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a6c46d72c4413588e4d677223f5455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 139787\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tok_function(example):\n",
    "    # This function can be used with the Dataset.map() method\n",
    "    return tokenizer(example['text'], truncation=True)\n",
    "\n",
    "tkn_data = ds.map(tok_function, batched=True)\n",
    "tkn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc6b465b-831f-4f1b-91b8-fe089144a15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 11322\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1398\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1258\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shrink the data\n",
    "# and split into train, test, and validation\n",
    "tkn_data = tkn_data.select(range(int(0.1*len(tkn_data))))\n",
    "temp1_dd = tkn_data.train_test_split(test_size=0.1, shuffle=True)\n",
    "temp2_dd = temp1_dd['train'].train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "tkn_data = DatasetDict({\n",
    "    'train': temp2_dd['train'],\n",
    "    'test': temp1_dd['test'],\n",
    "    'valid': temp2_dd['test'],\n",
    "})\n",
    "del temp1_dd\n",
    "del temp2_dd\n",
    "tkn_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc1ff8ac-14f2-40f0-92ee-aff774a55e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# This function does no accept the return_tensors argument.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')\n",
    "\n",
    "# Take care of everyting using `to_tf_dataset()`\n",
    "tf_train_data = tkn_data['train'].to_tf_dataset(\n",
    "       columns=['attention_mask', 'input_ids', 'token_type_ids'],\n",
    "       label_cols=['label'],\n",
    "       shuffle=True,\n",
    "       collate_fn=data_collator,\n",
    "       batch_size=8 )\n",
    "\n",
    "tf_valid_data = tkn_data['valid'].to_tf_dataset(\n",
    "       columns=['attention_mask', 'input_ids', 'token_type_ids'],\n",
    "       label_cols=['label'],\n",
    "       shuffle=True,\n",
    "       collate_fn=data_collator,\n",
    "       batch_size=8 )\n",
    "\n",
    "tf_test_data = tkn_data['test'].to_tf_dataset(\n",
    "       columns=['attention_mask', 'input_ids', 'token_type_ids'],\n",
    "       label_cols=['label'],\n",
    "       shuffle=False,\n",
    "       collate_fn=data_collator,\n",
    "       batch_size=8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14691f9-e86a-4585-b249-4607e0ffc8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the training cell\n"
     ]
    }
   ],
   "source": [
    "%%script echo This is the training cell\n",
    "# Decay the learning rate w/ PolynomialDecay\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "num_train_steps = len(tf_train_data)*num_epochs\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "opt = Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "#reload the model to change the optimizer\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "             loss=loss,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "model.fit( tf_train_data, validation_data=tf_valid_data, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "037934ff-34c8-47fd-966a-21a0acac15b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name_or_path': 'bert-base-uncased',\n",
       " 'architectures': ['BertForSequenceClassification'],\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'classifier_dropout': None,\n",
       " 'gradient_checkpointing': False,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'layer_norm_eps': 1e-12,\n",
       " 'max_position_embeddings': 512,\n",
       " 'model_type': 'bert',\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'pad_token_id': 0,\n",
       " 'position_embedding_type': 'absolute',\n",
       " 'transformers_version': '4.30.2',\n",
       " 'type_vocab_size': 2,\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 30522}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae87c51a-d32f-42fe-b567-bce72a5994d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /media/hd1/TransformersFineTuned/class-2023-06-29_1436/model were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /media/hd1/TransformersFineTuned/class-2023-06-29_1436/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#model.fit( tf_train_data, validation_data=tf_valid_data, epochs=num_epochs)\n",
    "with open(model_path+'/config.json', 'r') as fobj:\n",
    "    HF_cfg = json.loads(fobj.read())\n",
    "    \n",
    "model_path = '/media/hd1/TransformersFineTuned/class-2023-06-29_1436/model'\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_cfg['_name_or_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2116a37-a1c5-4946-8dae-3e5fa2139a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepreds = model.predict(tf_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20185a40-0816-4940-bfec-2e83096a33d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.9831173, -4.1443624],\n",
       "       [-2.4828198,  2.5047653],\n",
       "       [-3.7847457,  3.9654768],\n",
       "       ...,\n",
       "       [-3.8161604,  4.048975 ],\n",
       "       [-1.749708 ,  1.7498296],\n",
       "       [ 1.2877778, -1.6421337]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62ad13c2-6ece-4501-99a3-fea96e920f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tf_test_data)['logits']\n",
    "class_preds = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3eca04df-e2f6-4991-8a2d-a12866797b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for b in tf_test_data.as_numpy_iterator():\n",
    "    targets.extend(list(b[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88151df1-3590-423c-861c-754e0891ecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96       686\n",
      "           1       0.96      0.97      0.97       712\n",
      "\n",
      "    accuracy                           0.96      1398\n",
      "   macro avg       0.96      0.96      0.96      1398\n",
      "weighted avg       0.96      0.96      0.96      1398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric_str = metrics.classification_report((class_preds > 0.5).astype(int), targets)\n",
    "print(metric_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a6cf9c2-29a4-4bc1-992e-d692a7430c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncomment if want to save\n"
     ]
    }
   ],
   "source": [
    "%%script echo uncomment if want to save\n",
    "#model.save_pretrained(save_directory='/media/hd1/TransformersFineTuned/BertHF1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "574cb804-f954-4587-9d3d-abd25bcca52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d40145-e0f5-4271-bd3e-a281403a2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset._info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022f698c-bf83-444a-ba6b-804fa8a3d2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'a': 1, 'b':2}\n",
    "d.update({'c':3})\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204044a5-c377-4fa1-8e97-216c2d46747f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 in d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a5109b-8a42-4756-8574-8d9cdef99d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f43205f5-5f88-4589-86d8-349143856451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] filename filename\n",
      "ipykernel_launcher.py: error: the following arguments are required: filename\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "p.add_argument('filename')           # positional argument\n",
    "p.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03111fad-2268-4daf-907b-d5a6b08cb5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "argparse.Namespace"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = parser.parse_args(\"\")\n",
    "type(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27bd9bee-6887-4886-9d33-aab0fc3d00d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Namespace' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Namespace' object is not iterable"
     ]
    }
   ],
   "source": [
    "dict(P)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
