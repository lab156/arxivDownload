{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3192d772-2bbb-4597-8a90-c58d1d84feff",
   "metadata": {},
   "source": [
    "# Loading data using Hugging Face datasets methods\n",
    "\n",
    "https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html\n",
    "\n",
    "https://huggingface.co/learn/nlp-course/chapter5/5?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2e1621-79d6-4683-95cd-0ed00aaefa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import load_dataset, Dataset, DatasetDict \n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import toml\n",
    "import json\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                         TFAutoModelForSequenceClassification,\n",
    "                          TFBertForSequenceClassification,\n",
    "                         DataCollatorWithPadding,\n",
    "                         TFPreTrainedModel,)\n",
    "\n",
    "# keras for training\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import sklearn.metrics as metrics\n",
    "from lxml import etree\n",
    "\n",
    "currentdir = os.path.abspath(os.path.curdir)\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "sys.path.insert(0,parentdir+'/embed') \n",
    "from classifier_trainer.trainer import stream_arxiv_paragraphs\n",
    "import parsing_xml as px\n",
    "import peep_tar as peep\n",
    "\n",
    "from train_lstm import gen_cfg, find_best_cutoff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from extract import Definiendum\n",
    "args = []\n",
    "#xml_lst, cfg = gen_cfg(config_path='../config.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4aff43-2b9a-4029-b4c6-b5ee6394d51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f9979e-8094-4f8a-943d-49f4f2d9e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = toml.load('../config.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8748cd74-b1e6-4493-b5a5-130ede0ee41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = cfg['finetuning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ad078a-94ee-46ce-b7f0-ba36ebdddf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_lst = glob.glob('/media/hd1/training_defs/math18/*.xml.gz')\n",
    "#xml_lst = xml_lst[:len(xml_lst)//4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86645a35-45d0-4e46-a1b5-299d23f10383",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = stream_arxiv_paragraphs(xml_lst, samples=cfg['batch_size'])\n",
    "\n",
    "all_data = []\n",
    "all_labels = []\n",
    "all_texts = []\n",
    "for s in stream:\n",
    "    try:\n",
    "        #all_data += list(zip(s[0], s[1]))\n",
    "        all_texts += s[0]\n",
    "        all_labels += s[1]\n",
    "    except IndexError:\n",
    "        logger.warning('Index error in the data stream.')\n",
    "data_dict = {\n",
    "    'text': all_texts,\n",
    "    'label': all_labels\n",
    "}\n",
    "ds = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d6ff4c0-10bb-4a92-a31c-cce72b16df99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' If _inline_math_ is given and _inline_math_ is a continuous function on the horizontal line _inline_math_ such that _inline_math_ for all _inline_math_, then _display_math_ ',\n",
       " ' Let _inline_math_ be a relation on _inline_math_. The (complementary) neighborhood _inline_math_ of _inline_math_ w.r.t. _inline_math_ is defined as follows: _display_math_ Moreover, we define the set of neighborhoods w.r.t. _inline_math_ as follows: _inline_math_ _inline_math_ ',\n",
       " ' When _inline_math_ satisfies () but is allowed to be discontinuous, much less is known about the regularity of _inline_math_. Caffarelli showed that _inline_math_ is _inline_math_ up to the boundary for some small dimensional _inline_math_ _citation_. In terms of Sobolev regularity, Wang _citation_ showed that for any _inline_math_, one can find sufficiently large _inline_math_ such that _inline_math_ fails to be in _inline_math_ even in the interior of the domain. Nevertheless, for fixed _inline_math_, De Philippis-Figalli _citation_ was able to show that _inline_math_ is in _inline_math_ in the interior of _inline_math_. This was later improved to an interior _inline_math_-estimate independently by De Philippis-Figalli-Savin _citation_ and Schmidt _citation_. ']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1dd680c-524a-4a23-b8b8-0055046d98f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick check the loading of the model\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "#model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "#sequences = all_texts[:10]\n",
    "#batch = dict(\n",
    "#    tokenizer(sequences, return_tensors='tf',\n",
    "#             padding=True, truncation=True)\n",
    "#)\n",
    "#model.compile(\n",
    "#    optimizer='adam',\n",
    "#    loss='binary_crossentropy'\n",
    "#)\n",
    "#labels = tf.convert_to_tensor(all_labels[:10])\n",
    "#model.train_on_batch(batch, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b552b938-fe78-4f48-bf34-2a9c654bf671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7632, 1010, 2023, 2003, 10990, 102], [101, 2023, 2003, 2893, 11771, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['hi, this is exciting',\n",
    "          'this is getting boring'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2801a191-4714-4b86-8e3b-c7d2e5a47fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a6c46d72c4413588e4d677223f5455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 139787\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tok_function(example):\n",
    "    # This function can be used with the Dataset.map() method\n",
    "    return tokenizer(example['text'], truncation=True)\n",
    "\n",
    "tkn_data = ds.map(tok_function, batched=True)\n",
    "tkn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc6b465b-831f-4f1b-91b8-fe089144a15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 11322\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1398\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1258\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shrink the data\n",
    "# and split into train, test, and validation\n",
    "tkn_data = tkn_data.select(range(int(0.1*len(tkn_data))))\n",
    "temp1_dd = tkn_data.train_test_split(test_size=0.1, shuffle=True)\n",
    "temp2_dd = temp1_dd['train'].train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "tkn_data = DatasetDict({\n",
    "    'train': temp2_dd['train'],\n",
    "    'test': temp1_dd['test'],\n",
    "    'valid': temp2_dd['test'],\n",
    "})\n",
    "del temp1_dd\n",
    "del temp2_dd\n",
    "tkn_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc1ff8ac-14f2-40f0-92ee-aff774a55e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# This function does no accept the return_tensors argument.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')\n",
    "\n",
    "# Take care of everyting using `to_tf_dataset()`\n",
    "tf_train_data = tkn_data['train'].to_tf_dataset(\n",
    "       columns=['attention_mask', 'input_ids', 'token_type_ids'],\n",
    "       label_cols=['label'],\n",
    "       shuffle=True,\n",
    "       collate_fn=data_collator,\n",
    "       batch_size=8 )\n",
    "\n",
    "tf_valid_data = tkn_data['valid'].to_tf_dataset(\n",
    "       columns=['attention_mask', 'input_ids', 'token_type_ids'],\n",
    "       label_cols=['label'],\n",
    "       shuffle=True,\n",
    "       collate_fn=data_collator,\n",
    "       batch_size=8 )\n",
    "\n",
    "tf_test_data = tkn_data['test'].to_tf_dataset(\n",
    "       columns=['attention_mask', 'input_ids', 'token_type_ids'],\n",
    "       label_cols=['label'],\n",
    "       shuffle=False,\n",
    "       collate_fn=data_collator,\n",
    "       batch_size=8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14691f9-e86a-4585-b249-4607e0ffc8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the training cell\n"
     ]
    }
   ],
   "source": [
    "%%script echo This is the training cell\n",
    "# Decay the learning rate w/ PolynomialDecay\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "num_train_steps = len(tf_train_data)*num_epochs\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "opt = Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "#reload the model to change the optimizer\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "             loss=loss,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "model.fit( tf_train_data, validation_data=tf_valid_data, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "037934ff-34c8-47fd-966a-21a0acac15b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name_or_path': 'bert-base-uncased',\n",
       " 'architectures': ['BertForSequenceClassification'],\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'classifier_dropout': None,\n",
       " 'gradient_checkpointing': False,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'layer_norm_eps': 1e-12,\n",
       " 'max_position_embeddings': 512,\n",
       " 'model_type': 'bert',\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'pad_token_id': 0,\n",
       " 'position_embedding_type': 'absolute',\n",
       " 'transformers_version': '4.30.2',\n",
       " 'type_vocab_size': 2,\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 30522}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dcd8cb9-fb8a-4dab-bbe4-897b6b91fcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shrink_data_factor': 1.0,\n",
       " 'checkpoint': 'bert-base-uncased',\n",
       " 'glob_data_source': 'training_defs/math*/*.xml.gz',\n",
       " 'data_stream_batch_size': 5000,\n",
       " 'num_epochs': 3,\n",
       " 'batch_size': 32,\n",
       " 'initial_lr': 5e-06,\n",
       " 'end_lr': 0.0,\n",
       " 'savedir': '/opt/data_dir/finetune/class-2023-06-29_1436/model',\n",
       " 'configpath': '/opt/arxivDownload/config.toml',\n",
       " 'base_dir': '/opt/data_dir',\n",
       " 'local_dir': '/tmp/trainer',\n",
       " 'timestamp': 'Jun-29_14-36',\n",
       " 'save_path_dir': '/opt/data_dir/trained_models/finetuning/HFTransformers_Jun-29_14-36',\n",
       " 'num_train_steps': 471501}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(model_path+'/cfg_dict.json', 'r') as fobj:\n",
    "    cfg = json.loads(fobj.read())\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae87c51a-d32f-42fe-b567-bce72a5994d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 16:31:34.328216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-16 16:31:34.329187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-16 16:31:34.329349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-16 16:31:34.329766: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 16:31:34.330608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-16 16:31:34.330807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-16 16:31:34.330983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-16 16:31:35.313270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-16 16:31:35.313441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-16 16:31:35.313572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-16 16:31:35.313686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10094 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /media/hd1/TransformersFineTuned/class-2023-06-29_1436/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#model.fit( tf_train_data, validation_data=tf_valid_data, epochs=num_epochs)\n",
    "    \n",
    "model_path = '/media/hd1/TransformersFineTuned/class-2023-06-29_1436/'\n",
    "with open(model_path+'/cfg_dict.json', 'r') as fobj:\n",
    "    cfg = json.loads(fobj.read())\n",
    "    \n",
    "#model = TFAutoModelForSequenceClassification.from_pretrained(model_path+'model')\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_path+'model')\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg['checkpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019554e5-7562-4199-9cc4-641fda1b1b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0088522f18a349dfb8c748cd4529c72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# test the the opening of the promath tar.gz, parsing and extracting workflow with HF transformers\n",
    "tarpath = '/media/hd1/promath/math19/1906_002.tar.gz'\n",
    "tar_iter = peep.tar_iter(tarpath, '.xml')\n",
    "fname, tobj = next(tar_iter)\n",
    "parsing_obj = px.DefinitionsXML(tobj)\n",
    "dd = Definiendum(parsing_obj, model, None, None, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf5b5c8-fc02-4b34-a757-6764bc9f19a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.25.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86395a34-95d5-4012-80e2-80682584f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We say a continuous function _inline_math_ is convex on _inline_math_, if _inline_math_, one has (2.16) Equation 2.16 _display_math_ \n",
      "----------------------------------------------\n",
      " _inline_math_ is _inline_math_-strongly convex on _inline_math_ _inline_math_ _inline_math_, if _inline_math_ is _inline_math_-uniformly convex on _inline_math_ _inline_math_ _inline_math_. \n",
      "----------------------------------------------\n",
      " We say a function _inline_math_ on _inline_math_ has _inline_math_-Hölder continuous derivatives _inline_math_ _inline_math_, if _inline_math_, one has \n",
      "----------------------------------------------\n",
      " _inline_math_ is said to have _inline_math_-Lipschitz continuous derivatives on _inline_math_ _inline_math_ _inline_math_ if _inline_math_ has _inline_math_-Hölder continuous derivatives on _inline_math_ _inline_math_ _inline_math_. \n",
      "----------------------------------------------\n",
      " In Definition , we unify the definition of Hölder continuous gradients _inline_math_ and high-order Hölder continuous derivatives _inline_math_. For _inline_math_, _inline_math_ denotes the dual norm of _inline_math_; for _inline_math_, _inline_math_ denotes the operator norm of tensor of _inline_math_-th order _inline_math_ _inline_math_, which is defined by (). \n",
      "----------------------------------------------\n",
      " In Assumptions to , for practical concerns and technical reasons, in the following discussion, we will assume that _inline_math_ and _inline_math_. (_inline_math_ means that in our setting if _inline_math_, then _inline_math_.) \n",
      "----------------------------------------------\n",
      " Set constants _inline_math_ such that _inline_math_, _inline_math_ if _inline_math_, _inline_math_ if _inline_math_, _inline_math_ and _inline_math_. _inline_math_. \n",
      "----------------------------------------------\n",
      " Let _inline_math_ be a constant such that _inline_math_, for the initial point _inline_math_ and _inline_math_ a minimum point of _inline_math_. We define two constants: (6.73) Equation 6.73 _inline_math_ Here _inline_math_ and _inline_math_ are carefully chosen for consideration of best convergence rates (as one will see in the proof of the theorem about the convergence rates). \n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for ele in dd.root.findall('definition'):\n",
    "    print(ele.find('stmnt').text)\n",
    "    print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2116a37-a1c5-4946-8dae-3e5fa2139a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepreds = model.predict(tf_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20185a40-0816-4940-bfec-2e83096a33d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.9831173, -4.1443624],\n",
       "       [-2.4828198,  2.5047653],\n",
       "       [-3.7847457,  3.9654768],\n",
       "       ...,\n",
       "       [-3.8161604,  4.048975 ],\n",
       "       [-1.749708 ,  1.7498296],\n",
       "       [ 1.2877778, -1.6421337]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62ad13c2-6ece-4501-99a3-fea96e920f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tf_test_data)['logits']\n",
    "class_preds = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3eca04df-e2f6-4991-8a2d-a12866797b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for b in tf_test_data.as_numpy_iterator():\n",
    "    targets.extend(list(b[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88151df1-3590-423c-861c-754e0891ecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96       686\n",
      "           1       0.96      0.97      0.97       712\n",
      "\n",
      "    accuracy                           0.96      1398\n",
      "   macro avg       0.96      0.96      0.96      1398\n",
      "weighted avg       0.96      0.96      0.96      1398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric_str = metrics.classification_report((class_preds > 0.5).astype(int), targets)\n",
    "print(metric_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a6cf9c2-29a4-4bc1-992e-d692a7430c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncomment if want to save\n"
     ]
    }
   ],
   "source": [
    "%%script echo uncomment if want to save\n",
    "#model.save_pretrained(save_directory='/media/hd1/TransformersFineTuned/BertHF1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "574cb804-f954-4587-9d3d-abd25bcca52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d40145-e0f5-4271-bd3e-a281403a2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset._info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022f698c-bf83-444a-ba6b-804fa8a3d2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'a': 1, 'b':2}\n",
    "d.update({'c':3})\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204044a5-c377-4fa1-8e97-216c2d46747f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 in d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a5109b-8a42-4756-8574-8d9cdef99d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f43205f5-5f88-4589-86d8-349143856451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] filename filename\n",
      "ipykernel_launcher.py: error: the following arguments are required: filename\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "p.add_argument('filename')           # positional argument\n",
    "p.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03111fad-2268-4daf-907b-d5a6b08cb5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "argparse.Namespace"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = parser.parse_args(\"\")\n",
    "type(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27bd9bee-6887-4886-9d33-aab0fc3d00d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Namespace' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Namespace' object is not iterable"
     ]
    }
   ],
   "source": [
    "dict(P)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
