{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-650da48464a8>:7: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "import ner\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from unwiki import unwiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results for the search for definition (currently just Wikipedia)\n",
    "with open('/media/hd1/wikipedia/wiki_definitions_improved.txt', 'r') as wiki_f:\n",
    "    wiki = wiki_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arithmetic mean  -#-%-  Definition  -#-%-  \\'The \\\\\\'\\\\\\'\\\\\\'arithmetic mean\\\\\\'\\\\\\'\\\\\\' (or \\\\\\'\\\\\\'\\\\\\'mean\\\\\\'\\\\\\'\\\\\\' or \\\\\\'\\\\\\'\\\\\\'average\\\\\\'\\\\\\'\\\\\\'), <math>\\\\\\\\bar{x}</math> (read <math>x</math> \\\\\\'\\\\\\'bar\\\\\\'\\\\\\'), is the mean of the <math>n</math> values <math>x_1,x_2,\\\\\\\\ldots,x_n</math>.<ref name=\"JM\">{{cite book| last = Medhi| first = Jyotiprasad| title = Statistical Methods: An Introductory Text| url = https://books.google.com/?id=bRUwgf_q5RsC| year = 1992| publisher = New Age International| isbn = 9788122404197| pages = 53–58 }}</ref>\\\\n\\\\nThe arithmetic mean  is the most commonly used and readily understood measure of central tendency in a [[data set]]. In statistics, the term average refers to any of the measures of central tendency. The arithmetic mean of a set of observed data is defined as being equal to the sum of the numerical values of each and every observation divided by the total number of observations. Symbolically, if we have a data set consisting of the values <math>a_1, a_2, \\\\\\\\ldots, a_n</math>, then the arithmetic mean <math>A</math> is defined by the formula:\\\\n\\\\n:<math>A=\\\\\\\\frac{1}{n}\\\\\\\\sum_{i=1}^n a_i=\\\\\\\\frac{a_1+a_2+\\\\\\\\cdots+a_n}{n}</math>\\\\n\\\\n(See [[summation]] for an explanation of the [[summation operator]]).\\\\n\\\\nFor example, let us consider the monthly salary of 10 employees of a firm: 2500, 2700, 2400, 2300, 2550, 2650, 2750, 2450, 2600, 2400. The arithmetic mean is\\\\n\\\\n: <math>\\\\\\\\frac{ 2500+ 2700+ 2400+ 2300+ 2550+ 2650+ 2750+ 2450+ 2600+ 2400}{10}=2530.</math>\\\\n\\\\nIf the data set is a [[statistical population]] (i.e., consists of every possible observation and not just a subset of them), then the mean of that population is called the \\\\\\'\\\\\\'\\\\\\'population mean\\\\\\'\\\\\\'\\\\\\'. If the data set is a [[sampling (statistics)|statistical sample]] (a subset of the population), we call the statistic resulting from this calculation a \\\\\\'\\\\\\'\\\\\\'sample mean\\\\\\'\\\\\\'\\\\\\'.\\'\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'j.w', 'eng', 'e.g', 'p.h.d', 'spacewalks', 'al', 'vibrations', 'neighbourhood', \"'is\", 'missions', 'r.a', 'ginebra', 'ton', 'juniper', 'jie', 'i.e', 'dr', 'cf', 'z-1', '2π', 'jr', 'ex', 's^2', 'mixture', 'hk', 'sow', 'u.n', 'etc', 'wings', 'az', 'x+2', 'ca', 'pl', 'u.s'}\n"
     ]
    }
   ],
   "source": [
    "# Get data and train the Sentence tokenizer\n",
    "# Uses a standard algorithm (Kiss-Strunk) for unsupervised sentence boundary detection\n",
    "text = ''\n",
    "for i in range(550):\n",
    "    text += unwiki.loads(eval(wiki[i].split('-#-%-')[2]))\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "print(tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicator function  \n",
      "The indicator function of a subset A of a set X is a function\n",
      "\n",
      "_display_math_\n",
      "\n",
      "defined as\n",
      "\n",
      "_display_math_\n",
      "\n",
      "The Iverson bracket allows the equivalent notation, _inline_math_, to be used instead of _inline_math_.\n",
      "\n",
      "The function _inline_math_ is sometimes denoted _inline_math_, _inline_math_, _inline_math_ or even just _inline_math_. (The Greek letter _inline_math_ appears because it is the initial letter of the Greek word χαρακτήρ, which is the ultimate origin of the word characteristic.)\n",
      "\n",
      "The set of all indicator functions on _inline_math_ can be identified with _inline_math_, the power set of _inline_math_.  Consequently, both sets are sometimes denoted by _inline_math_. This is a special case (_inline_math_) of the notation _inline_math_ for the set of all functions _inline_math_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  \"The indicator function of a subset \\'\\'A\\'\\' of a set \\'\\'X\\'\\' is a function\\\\n\\\\n:<math>\\\\\\\\mathbf{1}_A \\\\\\\\colon X \\\\\\\\to \\\\\\\\{ 0,1 \\\\\\\\} </math>\\\\n\\\\ndefined as\\\\n\\\\n:<math>\\\\\\\\mathbf{1}_A(x) :=\\\\n\\\\\\\\begin{cases}\\\\n1 &\\\\\\\\text{if } x \\\\\\\\in A, \\\\\\\\\\\\\\\\\\\\n0 &\\\\\\\\text{if } x \\\\\\\\notin A.\\\\n\\\\\\\\end{cases}\\\\n</math>\\\\n\\\\nThe [[Iverson bracket]] allows the equivalent notation, <math>[x\\\\\\\\in A]</math>, to be used instead of <math>\\\\\\\\mathbf{1}_A(x)</math>.\\\\n\\\\nThe function <math>\\\\\\\\mathbf{1}_A</math> is sometimes denoted <math>I_A</math>, <math>\\\\\\\\chi_A</math>, \\'\\'K<sub>A</sub>\\'\\' or even just <math>A</math>. (The [[Greek alphabet|Greek letter]] <math>\\\\\\\\chi</math> appears because it is the initial letter of the Greek word χαρακτήρ, which is the ultimate origin of the word \\'\\'characteristic\\'\\'.)\\\\n\\\\nThe set of all indicator functions on <math>X</math> can be identified with <math>\\\\\\\\mathcal{P}(X)</math>, the [[power set]] of <math>X</math>.  Consequently, both sets are sometimes denoted by <math>2^X</math>. This is a special case (<math>Y =\\\\\\\\{0,1\\\\\\\\}=2</math>) of the notation <math>Y^X</math> for the set of all functions <math>f:X\\\\\\\\to Y </math>.\"\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The cleaning up of the wiki markup so that it looks like normal written english\n",
    "title, section, defin = wiki[850].split('-#-%-')\n",
    "dclean = unwiki.loads(eval(defin))\n",
    "print(title)\n",
    "print(dclean)\n",
    "defin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data and POS and NER tags for each definition (LONG TIME)\n",
    "def_lst = []\n",
    "for i in range(len(wiki)):\n",
    "    try:\n",
    "        title, section, defin_raw = wiki[i].split('-#-%-')\n",
    "        defin_all = unwiki.loads(eval(defin_raw))\n",
    "        for d in tokenizer.tokenize(defin_all):\n",
    "            if title.lower().strip() in d.lower():\n",
    "                pos_tokens = pos_tag(word_tokenize(d))\n",
    "                def_ner = ner.bio_tag.bio_tagger(title.strip().split(), pos_tokens)\n",
    "                other_ner = [((d[0],d[1]),d[2]) for d in def_ner]\n",
    "                tmp_dict = {'title': title,\n",
    "                           'section': section,\n",
    "                           'defin': d,\n",
    "                           'ner': other_ner}\n",
    "                def_lst.append(tmp_dict)\n",
    "    except ValueError:\n",
    "        print('parsing error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Arithmetic mean  ',\n",
       " 'section': '  Definition  ',\n",
       " 'defin': 'The arithmetic mean (or mean or average), _inline_math_ (read _inline_math_ bar), is the mean of the _inline_math_ values _inline_math_.',\n",
       " 'ner': [(('The', 'DT'), 'O'),\n",
       "  (('arithmetic', 'JJ'), 'B-DFNDUM'),\n",
       "  (('mean', 'NN'), 'I-DFNDUM'),\n",
       "  (('(', '('), 'O'),\n",
       "  (('or', 'CC'), 'O'),\n",
       "  (('mean', 'VB'), 'O'),\n",
       "  (('or', 'CC'), 'O'),\n",
       "  (('average', 'VB'), 'O'),\n",
       "  ((')', ')'), 'O'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('_inline_math_', 'NNP'), 'O'),\n",
       "  (('(', '('), 'O'),\n",
       "  (('read', 'VB'), 'O'),\n",
       "  (('_inline_math_', 'NNP'), 'O'),\n",
       "  (('bar', 'NN'), 'O'),\n",
       "  ((')', ')'), 'O'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('is', 'VBZ'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('mean', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('_inline_math_', 'NN'), 'O'),\n",
       "  (('values', 'VBZ'), 'O'),\n",
       "  (('_inline_math_', 'NNP'), 'O'),\n",
       "  (('.', '.'), 'O')]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 12602\n",
      "#test samples = 1401\n"
     ]
    }
   ],
   "source": [
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    " \n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)       \n",
    "        \n",
    "random.shuffle(def_lst)\n",
    "training_samples = [d['ner'] for d in def_lst[:int(len(def_lst) * 0.9)]]\n",
    "test_samples = [d['ner'] for d in def_lst[int(len(def_lst) * 0.9):]]\n",
    " \n",
    "print(\"#training samples = %s\" % len(training_samples) )   # training samples = 55809\n",
    "print(\"#test samples = %s\" % len(test_samples))            # test samples = 6201\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 96.7 ms, total: 15.4 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "#train the NER Chunking Classifier (TAKES A LONG TIME)\n",
    "%time chunker = NamedEntityChunker(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  91.3%%\n",
      "    Precision:     31.5%%\n",
      "    Recall:        68.4%%\n",
      "    F-Measure:     43.2%%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the most common metrics on the test dataset\n",
    "unpack = lambda l: [(tok, pos, ner) for ((tok, pos), ner) in l]\n",
    "Tree_lst = [conlltags2tree(unpack(t)) for t in test_samples]\n",
    "print(chunker.evaluate(Tree_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Scores\n",
    "\n",
    "Training with an amount of the dataset and evaluating with the rest\n",
    "* With 80% of the dataset\n",
    "\n",
    "* 60% of the data\n",
    "\n",
    "```ChunkParse score:\n",
    "    IOB Accuracy:  91.0%%\n",
    "    Precision:     30.7%%\n",
    "    Recall:        63.9%%\n",
    "    F-Measure:     41.5%%```\n",
    "    \n",
    "```ChunkParse score:\n",
    "    IOB Accuracy:  90.6%%\n",
    "    Precision:     32.4%%\n",
    "    Recall:        68.7%%\n",
    "    F-Measure:     44.0%%```\n",
    "\n",
    "* 90% of the data\n",
    "\n",
    "```ChunkParse score:\n",
    "    IOB Accuracy:  91.2%%\n",
    "    Precision:     32.0%%\n",
    "    Recall:        68.0%%\n",
    "    F-Measure:     43.5%%```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  define/VBP\n",
      "  a/DT\n",
      "  (DFNDUM Banach/NNP space/NN)\n",
      "  as/IN\n",
      "  a/DT\n",
      "  complete/JJ\n",
      "  vector/NN\n",
      "  space/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# An example of a user fed definition\n",
    "print(chunker.parse(pos_tag(word_tokenize(\"We define a Banach space as a complete vector space.\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_metrics(int_range, chunker_fn, data_set = test_samples, print_output=False):\n",
    "    '''\n",
    "    `int_range` is an integer range\n",
    "    NEEDS A TEST_SAMPLES VARIABLE CREATED WHEN SPLITTING THE \n",
    "    TRAINING AND TESTING DATA\n",
    "    Returns two vectors ready to be used in the \n",
    "    metrics classification function\n",
    "    '''\n",
    "    if isinstance(int_range, int):\n",
    "        int_range = [int_range]\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in int_range:\n",
    "        sample = data_set[i]\n",
    "        sm = [s[0] for s in sample]\n",
    "        y_true_tmp = [s[1] for s in sample]\n",
    "        predicted = [v[2] for v in tree2conlltags(chunker_fn.parse(sm))]\n",
    "        y_true += y_true_tmp\n",
    "        y_pred += predicted\n",
    "        if print_output:\n",
    "            for k,s in enumerate(sm):\n",
    "                print('{:15} {:>10}  {:>10}'.format(s[0], y_true_tmp[k], predicted[k]))\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The                      O           O\n",
      "table                    O    B-DFNDUM\n",
      "below                    O    I-DFNDUM\n",
      "enumerates               O           O\n",
      "the                      O           O\n",
      "order             B-DFNDUM           O\n",
      "of                I-DFNDUM           O\n",
      "magnitude         I-DFNDUM           O\n",
      "of                       O           O\n",
      "some                     O           O\n",
      "numbers                  O           O\n",
      "in                       O           O\n",
      "light                    O           O\n",
      "of                       O           O\n",
      "this                     O           O\n",
      "definition               O           O\n",
      ":                        O           O\n",
      "The                      O           O\n",
      "geometric                O    B-DFNDUM\n",
      "mean                     O    I-DFNDUM\n",
      "of                       O           O\n",
      "_inline_math_            O           O\n",
      "and                      O           O\n",
      "_inline_math_            O           O\n",
      "is                       O           O\n",
      "_inline_math_            O           O\n",
      ",                        O           O\n",
      "meaning                  O           O\n",
      "that                     O           O\n",
      "a                        O           O\n",
      "value                    O           O\n",
      "of                       O           O\n",
      "exactly                  O           O\n",
      "_inline_math_            O           O\n",
      "(                        O           O\n",
      "i.e.                     O           O\n",
      ",                        O           O\n",
      "_inline_math_            O           O\n",
      ")                        O           O\n",
      "represents               O           O\n",
      "a                        O           O\n",
      "geometric                O           O\n",
      "``                       O           O\n",
      "halfway                  O    B-DFNDUM\n",
      "point                    O    I-DFNDUM\n",
      "''                       O           O\n",
      "within                   O           O\n",
      "the                      O           O\n",
      "range                    O           O\n",
      "of                       O           O\n",
      "possible                 O           O\n",
      "values                   O           O\n",
      "of                       O           O\n",
      "_inline_math_            O           O\n",
      ".                        O           O\n"
     ]
    }
   ],
   "source": [
    "OO = prepare_for_metrics(119, chunker, data_set=test_samples, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    B-DFNDUM       0.35      0.74      0.47      1351\n",
      "    I-DFNDUM       0.27      0.80      0.40      1055\n",
      "           O       0.99      0.91      0.95     44583\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     46989\n",
      "   macro avg       0.53      0.82      0.61     46989\n",
      "weighted avg       0.95      0.90      0.92     46989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, predicted = prepare_for_metrics(range(len(test_samples)), chunker)\n",
    "print(metrics.classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START3]', '[START3]'),('[START2]', '[START2]'), ('[START1]', '[START1]')] +\\\n",
    "    list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]'), ('[END3]', '[END3]')]\n",
    "    history = ['[START3]', '[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 3, to accommodate the padding\n",
    "    index += 3\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    prev3word, prev3pos = tokens[index - 3]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    next3word, next3pos = tokens[index + 3]\n",
    "    previob = history[index - 1]\n",
    "    prevpreviob = history[index - 2]\n",
    "    prev3iob = history[index - 3]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    "    \n",
    "    is_math = lambda w:(w == '_inline_math_') or (w == '_display_math_')\n",
    "    ismath = is_math(word)\n",
    "    isprevmath = is_math(prevword)\n",
    "    isprevprevmath = is_math(prevprevword)\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "                'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'next3word': next3word,\n",
    "        'next3pos': next3pos,\n",
    "        \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev3word': prev3word,\n",
    "        'prev3pos': prev3pos,\n",
    "        \n",
    "        'prev-iob': previob,\n",
    "        \n",
    "        'prev-prev-iob': prevpreviob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "        \n",
    "        'ismath': ismath,\n",
    "        'isprevmath': isprevmath,\n",
    "        'isprevprevmath': isprevprevmath,\n",
    "    }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
