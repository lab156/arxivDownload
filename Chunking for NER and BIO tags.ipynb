{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-650da48464a8>:7: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "import ner\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from unwiki import unwiki\n",
    "from ner.chunker import NamedEntityChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results for the search for definition (currently just Wikipedia)\n",
    "with open('/media/hd1/wikipedia/wiki_definitions_improved.txt', 'r') as wiki_f:\n",
    "    wiki = wiki_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mixture', \"'is\", 'u.n', 'pl', 'spacewalks', 'sow', 'ca', 'ton', 'neighbourhood', 'p.h.d', 'jie', 'eng', 'missions', 's^2', 'j.w', 'r.a', 'ex', 'ginebra', 'jr', 'x+2', 'u.s', 'cf', 'az', 'juniper', 'hk', '2π', 'al', 'etc', 'wings', 'z-1', 'e.g', 'vibrations', 'dr', 'i.e'}\n"
     ]
    }
   ],
   "source": [
    "# Get data and train the Sentence tokenizer\n",
    "# Uses a standard algorithm (Kiss-Strunk) for unsupervised sentence boundary detection\n",
    "text = ''\n",
    "for i in range(550):\n",
    "    text += unwiki.loads(eval(wiki[i].split('-#-%-')[2]))\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "print(tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicator function  \n",
      "The indicator function of a subset A of a set X is a function\n",
      "\n",
      "_display_math_\n",
      "\n",
      "defined as\n",
      "\n",
      "_display_math_\n",
      "\n",
      "The Iverson bracket allows the equivalent notation, _inline_math_, to be used instead of _inline_math_.\n",
      "\n",
      "The function _inline_math_ is sometimes denoted _inline_math_, _inline_math_, _inline_math_ or even just _inline_math_. (The Greek letter _inline_math_ appears because it is the initial letter of the Greek word χαρακτήρ, which is the ultimate origin of the word characteristic.)\n",
      "\n",
      "The set of all indicator functions on _inline_math_ can be identified with _inline_math_, the power set of _inline_math_.  Consequently, both sets are sometimes denoted by _inline_math_. This is a special case (_inline_math_) of the notation _inline_math_ for the set of all functions _inline_math_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  \"The indicator function of a subset \\'\\'A\\'\\' of a set \\'\\'X\\'\\' is a function\\\\n\\\\n:<math>\\\\\\\\mathbf{1}_A \\\\\\\\colon X \\\\\\\\to \\\\\\\\{ 0,1 \\\\\\\\} </math>\\\\n\\\\ndefined as\\\\n\\\\n:<math>\\\\\\\\mathbf{1}_A(x) :=\\\\n\\\\\\\\begin{cases}\\\\n1 &\\\\\\\\text{if } x \\\\\\\\in A, \\\\\\\\\\\\\\\\\\\\n0 &\\\\\\\\text{if } x \\\\\\\\notin A.\\\\n\\\\\\\\end{cases}\\\\n</math>\\\\n\\\\nThe [[Iverson bracket]] allows the equivalent notation, <math>[x\\\\\\\\in A]</math>, to be used instead of <math>\\\\\\\\mathbf{1}_A(x)</math>.\\\\n\\\\nThe function <math>\\\\\\\\mathbf{1}_A</math> is sometimes denoted <math>I_A</math>, <math>\\\\\\\\chi_A</math>, \\'\\'K<sub>A</sub>\\'\\' or even just <math>A</math>. (The [[Greek alphabet|Greek letter]] <math>\\\\\\\\chi</math> appears because it is the initial letter of the Greek word χαρακτήρ, which is the ultimate origin of the word \\'\\'characteristic\\'\\'.)\\\\n\\\\nThe set of all indicator functions on <math>X</math> can be identified with <math>\\\\\\\\mathcal{P}(X)</math>, the [[power set]] of <math>X</math>.  Consequently, both sets are sometimes denoted by <math>2^X</math>. This is a special case (<math>Y =\\\\\\\\{0,1\\\\\\\\}=2</math>) of the notation <math>Y^X</math> for the set of all functions <math>f:X\\\\\\\\to Y </math>.\"\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The cleaning up of the wiki markup so that it looks like normal written english\n",
    "title, section, defin = wiki[850].split('-#-%-')\n",
    "dclean = unwiki.loads(eval(defin))\n",
    "print(title)\n",
    "print(dclean)\n",
    "defin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data and POS and NER tags for each definition (LONG TIME)\n",
    "def_lst = []\n",
    "for i in range(len(wiki)):\n",
    "    try:\n",
    "        title, section, defin_raw = wiki[i].split('-#-%-')\n",
    "        defin_all = unwiki.loads(eval(defin_raw))\n",
    "        for d in tokenizer.tokenize(defin_all):\n",
    "            if title.lower().strip() in d.lower():\n",
    "                pos_tokens = pos_tag(word_tokenize(d))\n",
    "                def_ner = ner.bio_tag.bio_tagger(title.strip().split(), pos_tokens)\n",
    "                other_ner = [((d[0],d[1]),d[2]) for d in def_ner]\n",
    "                tmp_dict = {'title': title,\n",
    "                           'section': section,\n",
    "                           'defin': d,\n",
    "                           'ner': other_ner}\n",
    "                def_lst.append(tmp_dict)\n",
    "    except ValueError:\n",
    "        print('parsing error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Arithmetic mean  ',\n",
       " 'section': '  Definition  ',\n",
       " 'defin': 'The arithmetic mean (or mean or average), _inline_math_ (read _inline_math_ bar), is the mean of the _inline_math_ values _inline_math_.',\n",
       " 'ner': [(('The', 'DT'), 'O'),\n",
       "  (('arithmetic', 'JJ'), 'B-DFNDUM'),\n",
       "  (('mean', 'NN'), 'I-DFNDUM'),\n",
       "  (('(', '('), 'O'),\n",
       "  (('or', 'CC'), 'O'),\n",
       "  (('mean', 'VB'), 'O'),\n",
       "  (('or', 'CC'), 'O'),\n",
       "  (('average', 'VB'), 'O'),\n",
       "  ((')', ')'), 'O'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('_inline_math_', 'NNP'), 'O'),\n",
       "  (('(', '('), 'O'),\n",
       "  (('read', 'VB'), 'O'),\n",
       "  (('_inline_math_', 'NNP'), 'O'),\n",
       "  (('bar', 'NN'), 'O'),\n",
       "  ((')', ')'), 'O'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('is', 'VBZ'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('mean', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('_inline_math_', 'NN'), 'O'),\n",
       "  (('values', 'VBZ'), 'O'),\n",
       "  (('_inline_math_', 'NNP'), 'O'),\n",
       "  (('.', '.'), 'O')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 12602\n",
      "#test samples = 1401\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(def_lst)\n",
    "training_samples = [d['ner'] for d in def_lst[:int(len(def_lst) * 0.9)]]\n",
    "test_samples = [d['ner'] for d in def_lst[int(len(def_lst) * 0.9):]]\n",
    " \n",
    "print(\"#training samples = %s\" % len(training_samples) )   # training samples = 55809\n",
    "print(\"#test samples = %s\" % len(test_samples))            # test samples = 6201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 151 ms, total: 15.4 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "#train the NER Chunking Classifier (TAKES A LONG TIME)\n",
    "%time chunker = NamedEntityChunker(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  91.3%%\n",
      "    Precision:     32.1%%\n",
      "    Recall:        68.3%%\n",
      "    F-Measure:     43.7%%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the most common metrics on the test dataset\n",
    "unpack = lambda l: [(tok, pos, ner) for ((tok, pos), ner) in l]\n",
    "Tree_lst = [conlltags2tree(unpack(t)) for t in test_samples]\n",
    "print(chunker.evaluate(Tree_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Scores\n",
    "\n",
    "Training with an amount of the dataset and evaluating with the rest\n",
    "* With 80% of the dataset\n",
    "\n",
    "* 60% of the data\n",
    "\n",
    "```ChunkParse score:\n",
    "    IOB Accuracy:  91.0%%\n",
    "    Precision:     30.7%%\n",
    "    Recall:        63.9%%\n",
    "    F-Measure:     41.5%%```\n",
    "    \n",
    "```ChunkParse score:\n",
    "    IOB Accuracy:  90.6%%\n",
    "    Precision:     32.4%%\n",
    "    Recall:        68.7%%\n",
    "    F-Measure:     44.0%%```\n",
    "\n",
    "* 90% of the data\n",
    "\n",
    "```ChunkParse score:\n",
    "    IOB Accuracy:  91.2%%\n",
    "    Precision:     32.0%%\n",
    "    Recall:        68.0%%\n",
    "    F-Measure:     43.5%%```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  define/VBP\n",
      "  a/DT\n",
      "  (DFNDUM Banach/NNP space/NN)\n",
      "  as/IN\n",
      "  a/DT\n",
      "  complete/JJ\n",
      "  vector/NN\n",
      "  space/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# An example of a user fed definition\n",
    "print(chunker.parse(pos_tag(word_tokenize(\"We define a Banach space as a complete vector space.\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_metrics(int_range, chunker_fn, data_set = test_samples, print_output=False):\n",
    "    '''\n",
    "    `int_range` is an integer range\n",
    "    NEEDS A TEST_SAMPLES VARIABLE CREATED WHEN SPLITTING THE \n",
    "    TRAINING AND TESTING DATA\n",
    "    Returns two vectors ready to be used in the \n",
    "    metrics classification function\n",
    "    '''\n",
    "    if isinstance(int_range, int):\n",
    "        int_range = [int_range]\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in int_range:\n",
    "        sample = data_set[i]\n",
    "        sm = [s[0] for s in sample]\n",
    "        y_true_tmp = [s[1] for s in sample]\n",
    "        predicted = [v[2] for v in tree2conlltags(chunker_fn.parse(sm))]\n",
    "        y_true += y_true_tmp\n",
    "        y_pred += predicted\n",
    "        if print_output:\n",
    "            for k,s in enumerate(sm):\n",
    "                print('{:15} {:>10}  {:>10}'.format(s[0], y_true_tmp[k], predicted[k]))\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herbs                    O    B-DFNDUM\n",
      "came                     O    I-DFNDUM\n",
      "to                       O           O\n",
      "be                       O           O\n",
      "considered               O           O\n",
      "in                       O           O\n",
      "3                        O           O\n",
      "groups                   O           O\n",
      ",                        O           O\n",
      "namely                   O           O\n",
      "pot                      O           O\n",
      "herbs                    O           O\n",
      "(                        O           O\n",
      "e.g                      O           O\n",
      ".                        O           O\n",
      "onions                   O           O\n",
      ")                        O           O\n",
      ",                        O           O\n",
      "sweet                    O           O\n",
      "herbs                    O           O\n",
      "(                        O           O\n",
      "e.g                      O           O\n",
      ".                        O           O\n",
      "thyme                    O           O\n",
      ")                        O           O\n",
      "and                      O           O\n",
      "salad                    O           O\n",
      "herbs                    O           O\n",
      "(                        O           O\n",
      "e.g                      O           O\n",
      ".                        O           O\n",
      "wild                     O           O\n",
      "celery                   O           O\n",
      ")                        O           O\n",
      ".                        O           O\n"
     ]
    }
   ],
   "source": [
    "OO = prepare_for_metrics(119, chunker, data_set=test_samples, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    B-DFNDUM       0.36      0.76      0.49      1336\n",
      "    I-DFNDUM       0.33      0.80      0.47      1096\n",
      "           O       0.99      0.92      0.95     43983\n",
      "\n",
      "    accuracy                           0.91     46415\n",
      "   macro avg       0.56      0.83      0.64     46415\n",
      "weighted avg       0.96      0.91      0.93     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, predicted = prepare_for_metrics(range(len(test_samples)), chunker)\n",
    "print(metrics.classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# USE THE VERSION IN THE NER.CHUNKER MODULE INSTEAD\n",
    "# this is just for reference\n",
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    " \n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)       \n",
    "    \n",
    "    \n",
    "\n",
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START3]', '[START3]'),('[START2]', '[START2]'), ('[START1]', '[START1]')] +\\\n",
    "    list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]'), ('[END3]', '[END3]')]\n",
    "    history = ['[START3]', '[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 3, to accommodate the padding\n",
    "    index += 3\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    prev3word, prev3pos = tokens[index - 3]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    next3word, next3pos = tokens[index + 3]\n",
    "    previob = history[index - 1]\n",
    "    prevpreviob = history[index - 2]\n",
    "    prev3iob = history[index - 3]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    "    \n",
    "    is_math = lambda w:(w == '_inline_math_') or (w == '_display_math_')\n",
    "    ismath = is_math(word)\n",
    "    isprevmath = is_math(prevword)\n",
    "    isprevprevmath = is_math(prevprevword)\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "                'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'next3word': next3word,\n",
    "        'next3pos': next3pos,\n",
    "        \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev3word': prev3word,\n",
    "        'prev3pos': prev3pos,\n",
    "        \n",
    "        'prev-iob': previob,\n",
    "        \n",
    "        'prev-prev-iob': prevpreviob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "        \n",
    "        'ismath': ismath,\n",
    "        'isprevmath': isprevmath,\n",
    "        'isprevprevmath': isprevprevmath,\n",
    "    }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
