{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "from collections.abc import Iterable\n",
    "import random\n",
    "from lxml import etree\n",
    "import gzip\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "# Classifiers\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,\\\n",
    "GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from nltk.chunk import ChunkParserI\n",
    "import unidecode\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import matplotlib \n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from unwiki import unwiki\n",
    "from ner.chunker import NamedEntityChunker\n",
    "import ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opens the latest wikipedia defs\n"
     ]
    }
   ],
   "source": [
    "%%script echo opens the latest wikipedia defs\n",
    "# The results for the search for definition (currently just Wikipedia)\n",
    "with open('/media/hd1/wikipedia/wiki_definitions_improved.txt', 'r') as wiki_f:\n",
    "    wikilines = wiki_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save and create xml zipped file\n"
     ]
    }
   ],
   "source": [
    "%%script echo Save and create xml zipped file\n",
    "root = etree.Element('root')\n",
    "\n",
    "for n, line in enumerate(wikilines):\n",
    "    title, section, defin = line.split('-#-%-')\n",
    "    defin = unwiki.loads(eval(defin)).replace('\\n', ' ')\n",
    "    def_tree = etree.Element('definition')\n",
    "    stmnt = etree.SubElement(def_tree, 'stmnt')\n",
    "    stmnt.text = unidecode.unidecode(defin)\n",
    "    dfndum = etree.SubElement(def_tree, 'dfndum')\n",
    "    dfndum.text = title.strip()\n",
    "    if stmnt.text != '':\n",
    "        root.append(def_tree)\n",
    "    #if n > 4976:\n",
    "    #    break\n",
    "#print(etree.tostring(root, pretty_print=True).decode('utf8')) \n",
    "\n",
    "with gzip.open('/media/hd1/wikipedia/wiki_definitions_improved.xml.gz', 'wb') as wiki_fobj:\n",
    "    wiki_fobj.write(etree.tostring(root, pretty_print=True))\n",
    "    #print(etree.tostring(root, pretty_print=True).decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the problematic article is: examples.xml\n",
      "The name of the problematic article is: coding.xml\n",
      "The name of the problematic article is: spaces-pushouts.xml\n",
      "The name of the problematic article is: guide.xml\n",
      "The name of the problematic article is: moduli.xml\n",
      "The name of the problematic article is: more-groupoids.xml\n",
      "The name of the problematic article is: chapters.xml\n",
      "The name of the problematic article is: sets.xml\n",
      "The name of the problematic article is: obsolete.xml\n",
      "The name of the problematic article is: examples-defos.xml\n",
      "The name of the problematic article is: spaces-more-cohomology.xml\n",
      "The name of the problematic article is: bibliography.xml\n",
      "The name of the problematic article is: fdl.xml\n",
      "The name of the problematic article is: limits.xml\n",
      "The name of the problematic article is: conventions.xml\n",
      "The name of the problematic article is: introduction.xml\n",
      "The name of the problematic article is: quot.xml\n",
      "The name of the problematic article is: desirables.xml\n",
      "Found 5314 in the wiki dataset.\n",
      "Found 95 in the stacks dataset.\n",
      "Found 3574 in the PlanetMath dataset.\n"
     ]
    }
   ],
   "source": [
    "def split_fields(elem):\n",
    "    title = elem.find('.//dfndum').text \n",
    "    section = elem.get('name')\n",
    "    defin = elem.find('.//stmnt').text\n",
    "    return (title, section, defin)\n",
    "\n",
    "wiki = []\n",
    "with gzip.open('/media/hd1/wikipedia/wiki_definitions_improved.xml.gz', 'r') as xml_fobj:\n",
    "    def_xml = etree.parse(xml_fobj)\n",
    "    for art in def_xml.findall('definition'):\n",
    "        data = (art.find('.//dfndum').text, '', art.find('.//stmnt').text)\n",
    "        wiki.append(data)\n",
    "        \n",
    "#random.sample(wiki, 1)\n",
    "        \n",
    "plmath = []\n",
    "with gzip.open('/media/hd1/planetmath/datasets/planetmath_definitions.xml.gz', 'r') as xml_fobj:\n",
    "    def_xml = etree.parse(xml_fobj)\n",
    "    for art in def_xml.findall('article'):\n",
    "        plmath.append(split_fields(art))\n",
    "stacks = []\n",
    "with gzip.open('/media/hd1/stacks-project/datasets/stacks-definitions.xml.gz', 'r') as xml_fobj:\n",
    "    def_xml = etree.parse(xml_fobj)\n",
    "    for art in def_xml.findall('article'):\n",
    "        try:\n",
    "            stacks.append(split_fields(art))\n",
    "        except AttributeError:\n",
    "            print('The name of the problematic article is: {}'.format(art.attrib['name']))\n",
    "\n",
    "# Print results\n",
    "print('Found {} in the wiki dataset.'.format(len(wiki)))\n",
    "print('Found {} in the stacks dataset.'.format(len(stacks)))\n",
    "print('Found {} in the PlanetMath dataset.'.format(len(plmath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jie', 'jr', 'i.o', 's.t', '2p', 'mr', 'cf', 'n.b', 'wings', 'a.k.a', 'etc', 'r.a', 's^2', 'pp', 'al', 'axe', 'mixture', 'hk', 'i.e', 'vol', 'resp', 'az', 'scand', 'x+2', 'spacewalks', 'j.w', 'eng', 'ginebra', 'e.g', 'pl', 'p.h.d', 'foreverâ€”e.g', '62f', 'inc', 'ca', 'j.a', 'vibrations', 'ton', 'brethren', 'og', '15k', 'eqs', 'juniper', 'hyp', 'u.n', 'eq', 'u.s', 'sow', 'z-1', 'dr', 'sgs', 'ex', 'a.e', 'fig', 'missions', 'c.c.c'}\n"
     ]
    }
   ],
   "source": [
    "# Get data and train the Sentence tokenizer\n",
    "# Uses a standard algorithm (Kiss-Strunk) for unsupervised sentence boundary detection\n",
    "text = ''\n",
    "for i in range(550):\n",
    "    #text += unwiki.loads(eval(wiki[i].split('-#-%-')[2]))\n",
    "    text += plmath[i][2]\n",
    "for i in range(50):\n",
    "    text += stacks[i][2]\n",
    "for i in range(550):\n",
    "    try:\n",
    "        text += wiki[i][2]\n",
    "    except TypeError:\n",
    "        print(wiki[i])\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "print(tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anti self-dual\n",
      "15A63-Selfdual.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Let _inline_math_ be a finite-dimensional inner-product space over a field _inline_math_. Let _inline_math_ be an endomorphism, and note that the adjoint endomorphism _inline_math_ is also an endomorphism of _inline_math_. It is therefore possible to add, subtract, and compare _inline_math_ and _inline_math_, and we are able to make the following definitions. An endomorphism _inline_math_ is said to be self-dual (a.k.a. self-adjoint) if _display_math_ By contrast, we say that the endomorphism is anti self-dual if _display_math_  Exactly the same definitions can be made for an endomorphism of a complex vector space with a Hermitian inner product.  All of these definitions have their counterparts in the matrix setting. Let _inline_math_ be the matrix of _inline_math_ relative to an orthogonal basis of _inline_math_. Then _inline_math_ is self-dual if and only if _inline_math_ is a symmetric matrix, and anti self-dual if and only if _inline_math_ is a skew-symmetric matrix.  In the case of a Hermitian inner product we must replace the transpose with the conjugate transpose. Thus _inline_math_ is self dual if and only if _inline_math_ is a Hermitian matrix, i.e. _display_math_ It is anti self-dual if and only if _display_math_ '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The cleaning up of the wiki markup so that it looks like normal written english\n",
    "title, section, defin = plmath[850]\n",
    "print(title)\n",
    "print(section)\n",
    "defin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_lst = ner.bio_tag.put_pos_ner_tags(stacks, tokenizer)\\\n",
    "         + ner.bio_tag.put_pos_ner_tags(plmath, tokenizer)\\\n",
    "         + ner.bio_tag.put_pos_ner_tags(wiki, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 20880\n",
      "#test samples = 5220\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(def_lst)\n",
    "training_samples = [d['ner'] for d in def_lst[:int(len(def_lst) * 0.8)]]\n",
    "test_samples = [d['ner'] for d in def_lst[int(len(def_lst) * 0.8):]]\n",
    " \n",
    "print(\"#training samples = %s\" % len(training_samples) )   # training samples = 55809\n",
    "print(\"#test samples = %s\" % len(test_samples))            # test samples = 6201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 524 ms, total: 1min 5s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "#train the NER Chunking Classifier (TAKES A LONG TIME)\n",
    "#%time chunker = NamedEntityChunker(training_samples)\n",
    "classfier = SklearnClassifier(LinearSVC())\n",
    "%time chunker = NamedEntityChunker(training_samples, classifier_builder=classfier.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.1%%\n",
      "    Precision:     69.1%%\n",
      "    Recall:        11.7%%\n",
      "    F-Measure:     20.0%%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the most common metrics on the test dataset\n",
    "unpack = lambda l: [(tok, pos, ner) for ((tok, pos), ner) in l]\n",
    "Tree_lst = [conlltags2tree(unpack(t)) for t in test_samples]\n",
    "print(chunker.evaluate(Tree_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('snake-arm', 'JJ'), ('robot', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('to', 'TO'), ('be', 'VB'), ('confused', 'VBN'), ('with', 'IN'), ('a', 'DT'), ('snakebot', 'NN'), ('which', 'WDT'), ('mimics', 'VBZ'), ('the', 'DT'), ('biomorphic', 'JJ'), ('motion', 'NN'), ('of', 'IN'), ('a', 'DT'), ('snake', 'NN'), ('in', 'IN'), ('order', 'NN'), ('to', 'TO'), ('slither', 'VB'), ('along', 'IN'), ('the', 'DT'), ('ground', 'NN'), ('.', '.')]\n",
      "(S\n",
      "  In/IN\n",
      "  the/DT\n",
      "  In/IN\n",
      "  1988/CD\n",
      "  ,/,\n",
      "  Gleason/NNP\n",
      "  showed/VBD\n",
      "  that/IN\n",
      "  an/DT\n",
      "  _inline_math_-sided/JJ\n",
      "  regular/JJ\n",
      "  polygon/NN\n",
      "  can/MD\n",
      "  be/VB\n",
      "  constructed/VBN\n",
      "  with/IN\n",
      "  ruler/NN\n",
      "  and/CC\n",
      "  compass/NN\n",
      "  if/IN\n",
      "  _inline_math_/NN\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  product/NN\n",
      "  of/IN\n",
      "  two/CD\n",
      "  Pierpont/NNP\n",
      "  primes/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(Tree_lst[13].leaves())\n",
    "print(chunker.parse(Tree_lst[3].leaves()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Scores\n",
    "\n",
    "Training with an amount of the dataset and evaluating with the rest\n",
    "* With 80% split of the stacks data:\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  86.5%%\n",
    "    Precision:     29.0%%\n",
    "    Recall:        60.0%%\n",
    "    F-Measure:     39.1%%\n",
    "```\n",
    "\n",
    "* With 80% of Wiki + Planetmath data:\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  88.4%%\n",
    "    Precision:     26.5%%\n",
    "    Recall:        66.3%%\n",
    "    F-Measure:     37.9%%\n",
    "```\n",
    "\n",
    "* With 90% of Wiki + Planetmath:\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  88.7%%\n",
    "    Precision:     27.1%%\n",
    "    Recall:        67.7%%\n",
    "    F-Measure:     38.7%%\n",
    "```\n",
    "\n",
    "* With 90% of the dataset Planetmath data alone:\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  86.0%%\n",
    "    Precision:     24.0%%\n",
    "    Recall:        67.3%%\n",
    "    F-Measure:     35.3%%\n",
    "```\n",
    "    \n",
    "* With 80% of the dataset\n",
    "\n",
    "* 60% of the data\n",
    "\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  91.0%%\n",
    "    Precision:     30.7%%\n",
    "    Recall:        63.9%%\n",
    "    F-Measure:     41.5%%\n",
    "```\n",
    "    \n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  90.6%%\n",
    "    Precision:     32.4%%\n",
    "    Recall:        68.7%%\n",
    "    F-Measure:     44.0%%\n",
    "```\n",
    "\n",
    "* 90% of the data\n",
    "\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  91.2%%\n",
    "    Precision:     32.0%%\n",
    "    Recall:        68.0%%\n",
    "    F-Measure:     43.5%%\n",
    "```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  define/VBP\n",
      "  a/DT\n",
      "  (DFNDUM Banach/NNP space/NN)\n",
      "  as/IN\n",
      "  a/DT\n",
      "  complete/JJ\n",
      "  vector/NN\n",
      "  space/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# An example of a user fed definition\n",
    "print(chunker.parse(pos_tag(word_tokenize(\"We define a Banach space as a complete vector space.\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_metrics(int_range, chunker_fn, data_set = test_samples, print_output=False):\n",
    "    '''\n",
    "    `int_range` is an integer range\n",
    "    NEEDS A TEST_SAMPLES VARIABLE CREATED WHEN SPLITTING THE \n",
    "    TRAINING AND TESTING DATA\n",
    "    Returns two vectors ready to be used in the \n",
    "    metrics classification function\n",
    "    '''\n",
    "    if isinstance(int_range, int):\n",
    "        int_range = [int_range]\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in int_range:\n",
    "        sample = data_set[i]\n",
    "        sm = [s[0] for s in sample]\n",
    "        y_true_tmp = [s[1] for s in sample]\n",
    "        predicted = [v[2] for v in tree2conlltags(chunker_fn.parse(sm))]\n",
    "        y_true += y_true_tmp\n",
    "        y_pred += predicted\n",
    "        if print_output:\n",
    "            for k,s in enumerate(sm):\n",
    "                print('{:15} {:>10}  {:>10}'.format(s[0], y_true_tmp[k], predicted[k]))\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group                    O           O\n",
      "The                      O           O\n",
      "Janko             B-DFNDUM    B-DFNDUM\n",
      "groups            I-DFNDUM    I-DFNDUM\n",
      "denoted                  O           O\n",
      "by                       O           O\n",
      "_inline_math_            O           O\n",
      ",                        O           O\n",
      "and                      O           O\n",
      "_inline_math_            O           O\n",
      "are                      O           O\n",
      "four                     O           O\n",
      "of                       O           O\n",
      "the                      O           O\n",
      "26                       O           O\n",
      "sporadic                 O           O\n",
      "groups                   O           O\n",
      ".                        O           O\n"
     ]
    }
   ],
   "source": [
    "OO = prepare_for_metrics(19, chunker, data_set=test_samples, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    B-DFNDUM       0.73      0.64      0.68      5873\n",
      "    I-DFNDUM       0.69      0.65      0.67      4319\n",
      "           O       0.98      0.98      0.98    148880\n",
      "\n",
      "    accuracy                           0.96    159072\n",
      "   macro avg       0.80      0.76      0.78    159072\n",
      "weighted avg       0.96      0.96      0.96    159072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, predicted = prepare_for_metrics(range(len(test_samples)), chunker)\n",
    "print(metrics.classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifiers = [\n",
    "    #MultinomialNB(),\n",
    "    #KNeighborsClassifier(2),\n",
    "    #LinearSVC(),\n",
    "    #SVC(),\n",
    "    NuSVC(nu=0.01),\n",
    "    #SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    #DecisionTreeClassifier(),\n",
    "    #RandomForestClassifier(),\n",
    "    #AdaBoostClassifier(),\n",
    "    #GradientBoostingClassifier(),\n",
    "    #GaussianNB(),\n",
    "    #LinearDiscriminantAnalysis(),\n",
    "    #QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "Tree_lst = [conlltags2tree(unpack(t)) for t in test_samples]\n",
    "for k,clf in enumerate(classifiers):\n",
    "    name = clf.__class__.__name__\n",
    "    #clf.fit(xtrain, train_y)\n",
    "    clfier = SklearnClassifier(clf)\n",
    "    #clfier = clf\n",
    "    Now = dt.now()\n",
    "    chunker = NamedEntityChunker(training_samples, classifier_builder=clfier.train)\n",
    "    training_time = (dt.now() - Now).seconds\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    # Evaluate the most common metrics on the test dataset\n",
    "    unpack = lambda l: [(tok, pos, ner) for ((tok, pos), ner) in l]\n",
    "    Now = dt.now()\n",
    "    score = chunker.evaluate(Tree_lst)\n",
    "    acc = score.accuracy()\n",
    "    f1 = score.f_measure()\n",
    "    Seconds = (dt.now() - Now).seconds\n",
    "    inference_rate = len(Tree_lst)/Seconds\n",
    "    print(\"IOB Accuracy: {:.2%}   \".format(acc))\n",
    "    \n",
    "    #prodictions = prob_prediction\n",
    "    print(\"F1 Measure:   {:.2%}\".format(f1))\n",
    "    print(f\"{Seconds = } secs\")\n",
    "    print(f\"InferRate:   {inference_rate:.2f} sentences/sec\")\n",
    "    \n",
    "    #log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    #log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "==============================\n",
    "LinearSVC\n",
    "****Results****\n",
    "IOB Accuracy: 96.14%   \n",
    "F1 Measure:   65.30%\n",
    "==============================\n",
    "SVC\n",
    "****Results****\n",
    "IOB Accuracy: 95.69%   \n",
    "F1 Measure:   50.93%\n",
    "==============================\n",
    "NuSVC nu=0.01\n",
    "****Results****\n",
    "IOB Accuracy: 96.56%   \n",
    "F1 Measure:   65.72%\n",
    "==============================\n",
    "DecisionTreeClassifier\n",
    "****Results****\n",
    "IOB Accuracy: 95.01%   \n",
    "F1 Measure:   52.80%\n",
    "==============================\n",
    "RandomForestClassifier\n",
    "****Results****\n",
    "IOB Accuracy: 95.23%   \n",
    "F1 Measure:   43.11%\n",
    "==============================\n",
    "AdaBoostClassifier\n",
    "****Results****\n",
    "IOB Accuracy: 90.36%   \n",
    "F1 Measure:   12.86%\n",
    "==============================\n",
    "GradientBoostingClassifier\n",
    "****Results****\n",
    "IOB Accuracy: 94.13%   \n",
    "F1 Measure:   19.96%\n",
    "==============================\n",
    "MultinomialNB\n",
    "****Results****\n",
    "IOB Accuracy: 92.58%   \n",
    "F1 Measure:   46.33%\n",
    "training_time = 25 secs\n",
    "InferRate:   21.66 sentences/sec\n",
    "==============================\n",
    "KNeighborsClassifier (N = 2)\n",
    "****Results****\n",
    "IOB Accuracy: 92.76%   \n",
    "F1 Measure:   45.29%\n",
    "training_time = 25 secs\n",
    "InferRate:   0.13 sentences/sec\n",
    "=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41275"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687.9166666666666"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "41275/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
