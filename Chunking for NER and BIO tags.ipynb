{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "from collections.abc import Iterable\n",
    "import random\n",
    "from lxml import etree\n",
    "import gzip\n",
    "\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "import unidecode\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from unwiki import unwiki\n",
    "from ner.chunker import NamedEntityChunker\n",
    "import ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opens the latest wikipedia defs\n"
     ]
    }
   ],
   "source": [
    "%%script echo opens the latest wikipedia defs\n",
    "# The results for the search for definition (currently just Wikipedia)\n",
    "with open('/media/hd1/wikipedia/wiki_definitions_improved.txt', 'r') as wiki_f:\n",
    "    wikilines = wiki_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save and create xml zipped file\n"
     ]
    }
   ],
   "source": [
    "%%script echo Save and create xml zipped file\n",
    "root = etree.Element('root')\n",
    "\n",
    "for n, line in enumerate(wikilines):\n",
    "    title, section, defin = line.split('-#-%-')\n",
    "    defin = unwiki.loads(eval(defin)).replace('\\n', ' ')\n",
    "    def_tree = etree.Element('definition')\n",
    "    stmnt = etree.SubElement(def_tree, 'stmnt')\n",
    "    stmnt.text = unidecode.unidecode(defin)\n",
    "    dfndum = etree.SubElement(def_tree, 'dfndum')\n",
    "    dfndum.text = title.strip()\n",
    "    if stmnt.text != '':\n",
    "        root.append(def_tree)\n",
    "    #if n > 4976:\n",
    "    #    break\n",
    "#print(etree.tostring(root, pretty_print=True).decode('utf8')) \n",
    "\n",
    "with gzip.open('/media/hd1/wikipedia/wiki_definitions_improved.xml.gz', 'wb') as wiki_fobj:\n",
    "    wiki_fobj.write(etree.tostring(root, pretty_print=True))\n",
    "    #print(etree.tostring(root, pretty_print=True).decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the problematic article is: examples.xml\n",
      "The name of the problematic article is: coding.xml\n",
      "The name of the problematic article is: spaces-pushouts.xml\n",
      "The name of the problematic article is: guide.xml\n",
      "The name of the problematic article is: moduli.xml\n",
      "The name of the problematic article is: more-groupoids.xml\n",
      "The name of the problematic article is: chapters.xml\n",
      "The name of the problematic article is: sets.xml\n",
      "The name of the problematic article is: obsolete.xml\n",
      "The name of the problematic article is: examples-defos.xml\n",
      "The name of the problematic article is: spaces-more-cohomology.xml\n",
      "The name of the problematic article is: bibliography.xml\n",
      "The name of the problematic article is: fdl.xml\n",
      "The name of the problematic article is: limits.xml\n",
      "The name of the problematic article is: conventions.xml\n",
      "The name of the problematic article is: introduction.xml\n",
      "The name of the problematic article is: quot.xml\n",
      "The name of the problematic article is: desirables.xml\n",
      "Found 5314 in the wiki dataset.\n",
      "Found 95 in the stacks dataset.\n",
      "Found 3574 in the PlanetMath dataset.\n"
     ]
    }
   ],
   "source": [
    "def split_fields(elem):\n",
    "    title = elem.find('.//dfndum').text \n",
    "    section = elem.get('name')\n",
    "    defin = elem.find('.//stmnt').text\n",
    "    return (title, section, defin)\n",
    "\n",
    "wiki = []\n",
    "with gzip.open('/media/hd1/wikipedia/wiki_definitions_improved.xml.gz', 'r') as xml_fobj:\n",
    "    def_xml = etree.parse(xml_fobj)\n",
    "    for art in def_xml.findall('definition'):\n",
    "        data = (art.find('.//dfndum').text, '', art.find('.//stmnt').text)\n",
    "        wiki.append(data)\n",
    "        \n",
    "#random.sample(wiki, 1)\n",
    "        \n",
    "plmath = []\n",
    "with gzip.open('/media/hd1/planetmath/datasets/planetmath_definitions.xml.gz', 'r') as xml_fobj:\n",
    "    def_xml = etree.parse(xml_fobj)\n",
    "    for art in def_xml.findall('article'):\n",
    "        plmath.append(split_fields(art))\n",
    "stacks = []\n",
    "with gzip.open('/media/hd1/stacks-project/datasets/stacks-definitions.xml.gz', 'r') as xml_fobj:\n",
    "    def_xml = etree.parse(xml_fobj)\n",
    "    for art in def_xml.findall('article'):\n",
    "        try:\n",
    "            stacks.append(split_fields(art))\n",
    "        except AttributeError:\n",
    "            print('The name of the problematic article is: {}'.format(art.attrib['name']))\n",
    "\n",
    "# Print results\n",
    "print('Found {} in the wiki dataset.'.format(len(wiki)))\n",
    "print('Found {} in the stacks dataset.'.format(len(stacks)))\n",
    "print('Found {} in the PlanetMath dataset.'.format(len(plmath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hyp', 'foreverâ€”e.g', 'eq', 'sow', 'vibrations', 'u.n', 's.t', 'inc', 'jie', 'pl', 'x+2', 'resp', 'a.k.a', 'eng', 'al', 's^2', 'r.a', '15k', 'jr', 'pp', 'fig', '2p', 'z-1', 'i.e', 'scand', 'juniper', 'p.h.d', 'ca', 'n.b', 'brethren', 'missions', 'ton', 'mixture', 'vol', 'j.a', 'hk', 'sgs', 'ginebra', 'mr', 'axe', 'i.o', 'az', '62f', 'spacewalks', 'og', 'j.w', 'c.c.c', 'a.e', 'u.s', 'eqs', 'dr', 'cf', 'e.g', 'etc', 'wings', 'ex'}\n"
     ]
    }
   ],
   "source": [
    "# Get data and train the Sentence tokenizer\n",
    "# Uses a standard algorithm (Kiss-Strunk) for unsupervised sentence boundary detection\n",
    "text = ''\n",
    "for i in range(550):\n",
    "    #text += unwiki.loads(eval(wiki[i].split('-#-%-')[2]))\n",
    "    text += plmath[i][2]\n",
    "for i in range(50):\n",
    "    text += stacks[i][2]\n",
    "for i in range(550):\n",
    "    try:\n",
    "        text += wiki[i][2]\n",
    "    except TypeError:\n",
    "        print(wiki[i])\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "print(tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anti self-dual\n",
      "15A63-Selfdual.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Let _inline_math_ be a finite-dimensional inner-product space over a field _inline_math_. Let _inline_math_ be an endomorphism, and note that the adjoint endomorphism _inline_math_ is also an endomorphism of _inline_math_. It is therefore possible to add, subtract, and compare _inline_math_ and _inline_math_, and we are able to make the following definitions. An endomorphism _inline_math_ is said to be self-dual (a.k.a. self-adjoint) if _display_math_ By contrast, we say that the endomorphism is anti self-dual if _display_math_  Exactly the same definitions can be made for an endomorphism of a complex vector space with a Hermitian inner product.  All of these definitions have their counterparts in the matrix setting. Let _inline_math_ be the matrix of _inline_math_ relative to an orthogonal basis of _inline_math_. Then _inline_math_ is self-dual if and only if _inline_math_ is a symmetric matrix, and anti self-dual if and only if _inline_math_ is a skew-symmetric matrix.  In the case of a Hermitian inner product we must replace the transpose with the conjugate transpose. Thus _inline_math_ is self dual if and only if _inline_math_ is a Hermitian matrix, i.e. _display_math_ It is anti self-dual if and only if _display_math_ '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The cleaning up of the wiki markup so that it looks like normal written english\n",
    "title, section, defin = plmath[850]\n",
    "print(title)\n",
    "print(section)\n",
    "defin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data and POS and NER tags for each definition (LONG TIME)\n",
    "#def put_pos_ner_tags(defl):\n",
    "#    '''\n",
    "#    INPUTS\n",
    "#    ------\n",
    "#    defl: list of tuples with the format (title, section, definition)\n",
    "#    tokenizer: sentence tokenizer, splits paragraphs into sentences and \n",
    "#               identifies abbreviations.\n",
    "#    - Checks if the definiendum is contained in the sentence \n",
    "#    - Finds the POS of each word in each sentence\n",
    "#    '''\n",
    "#    def_lst = []\n",
    "#    for i in range(len(defl)):\n",
    "#        try:\n",
    "#            #title, section, defin_raw = wiki[i].split('-#-%-')\n",
    "#            #defin_all = unwiki.loads(eval(defin_raw))\n",
    "#            title, section, defin_all = defl[i]\n",
    "#            for d in tokenizer.tokenize(defin_all):\n",
    "#                if title.lower().strip() in d.lower():\n",
    "#                    pos_tokens = pos_tag(word_tokenize(d))\n",
    "#                    def_ner = ner.bio_tag.bio_tagger(title.strip().split(), pos_tokens)\n",
    "#                    other_ner = [((d[0],d[1]),d[2]) for d in def_ner]\n",
    "#                    tmp_dict = {'title': title,\n",
    "#                               'section': section,\n",
    "#                               'defin': d,\n",
    "#                               'ner': other_ner}\n",
    "#                    def_lst.append(tmp_dict)\n",
    "#        except ValueError:\n",
    "#            print('parsing error')\n",
    "#    return def_lst\n",
    "def_lst2 = ner.bio_tag.put_pos_ner_tags(stacks, tokenizer)\\\n",
    "         + ner.bio_tag.put_pos_ner_tags(plmath, tokenizer)\\\n",
    "         + ner.bio_tag.put_pos_ner_tags(wiki, tokenizer)\n",
    "\n",
    "#for i in range(len(wiki)):\n",
    "#    try:\n",
    "#        title, section, defin_raw = wiki[i]\n",
    "#        defin_all = defin_raw\n",
    "#        for d in tokenizer.tokenize(defin_all):\n",
    "#            if title.lower().strip() in d.lower():\n",
    "#                pos_tokens = pos_tag(word_tokenize(d))\n",
    "#                def_ner = ner.bio_tag.bio_tagger(title.strip().split(), pos_tokens)\n",
    "#                other_ner = [((d[0],d[1]),d[2]) for d in def_ner]\n",
    "#                tmp_dict = {'title': title,\n",
    "#                           'section': section,\n",
    "#                           'defin': d,\n",
    "#                           'ner': other_ner}\n",
    "#                def_lst.append(tmp_dict)\n",
    "#    except ValueError:\n",
    "#        print('parsing error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Government hacking',\n",
       " '',\n",
       " \"The term hacking is used for actions taken by a group of people named as hackers. Hackers have extensive knowledge in the area of technology, specifically with electronic devices, programs and computer networks. These people differ from crackers because use their knowledge for illegal and unethical purposes. Overall, crackers apply their skills to take advantage of vulnerabilities in software and systems. The hacking action consists of manipulating computer systems or electronic devices in order to remotely control the machine or have access to the data stored there.  Due to the innovation of new technologies, it was necessary to update the cryptographic algorithms. This need has raised the level of complexity of the new techniques created for encrypting the data of the individuals in order to guarantee security in the network. Because of the difficulty of deciphering the data, government agencies have begun to search for other options to conduct criminal investigations. One such option is the so-called government hacking.  As it is characterized by the use of government technology resources to actively obtain information on citizens' devices, some say that government agents could also manipulate device data or insert new. Besides the possibility of manipulating data from individuals, tools developed by the government could be used by criminals.\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = wiki[-12]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Government hacking',\n",
       "  'section': '',\n",
       "  'defin': 'One such option is the so-called government hacking.',\n",
       "  'ner': [(('One', 'CD'), 'O'),\n",
       "   (('such', 'JJ'), 'O'),\n",
       "   (('option', 'NN'), 'O'),\n",
       "   (('is', 'VBZ'), 'O'),\n",
       "   (('the', 'DT'), 'O'),\n",
       "   (('so-called', 'JJ'), 'O'),\n",
       "   (('government', 'NN'), 'B-DFNDUM'),\n",
       "   (('hacking', 'NN'), 'I-DFNDUM'),\n",
       "   (('.', '.'), 'O')]}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.bio_tag.put_pos_ner_tags([w], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola', 'adios']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class tak:\n",
    "    def tr(self, s):\n",
    "        return s.split()\n",
    "\n",
    "tak().tr('hola adios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(def_lst)\n",
    "training_samples = [d['ner'] for d in def_lst[:int(len(def_lst) * 0.8)]]\n",
    "test_samples = [d['ner'] for d in def_lst[int(len(def_lst) * 0.8):]]\n",
    " \n",
    "print(\"#training samples = %s\" % len(training_samples) )   # training samples = 55809\n",
    "print(\"#test samples = %s\" % len(test_samples))            # test samples = 6201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the NER Chunking Classifier (TAKES A LONG TIME)\n",
    "%time chunker = NamedEntityChunker(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the most common metrics on the test dataset\n",
    "unpack = lambda l: [(tok, pos, ner) for ((tok, pos), ner) in l]\n",
    "Tree_lst = [conlltags2tree(unpack(t)) for t in test_samples]\n",
    "print(chunker.evaluate(Tree_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Scores\n",
    "\n",
    "Training with an amount of the dataset and evaluating with the rest\n",
    "* With 80% split of the stacks data:\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  86.5%%\n",
    "    Precision:     29.0%%\n",
    "    Recall:        60.0%%\n",
    "    F-Measure:     39.1%%\n",
    "```\n",
    "\n",
    "* With 80% of Wiki + Planetmath data:\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  88.4%%\n",
    "    Precision:     26.5%%\n",
    "    Recall:        66.3%%\n",
    "    F-Measure:     37.9%%\n",
    "```\n",
    "\n",
    "* With 90% of Wiki + Planetmath:\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  88.7%%\n",
    "    Precision:     27.1%%\n",
    "    Recall:        67.7%%\n",
    "    F-Measure:     38.7%%\n",
    "```\n",
    "\n",
    "* With 90% of the dataset Planetmath data alone:\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  86.0%%\n",
    "    Precision:     24.0%%\n",
    "    Recall:        67.3%%\n",
    "    F-Measure:     35.3%%\n",
    "```\n",
    "    \n",
    "* With 80% of the dataset\n",
    "\n",
    "* 60% of the data\n",
    "\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  91.0%%\n",
    "    Precision:     30.7%%\n",
    "    Recall:        63.9%%\n",
    "    F-Measure:     41.5%%\n",
    "```\n",
    "    \n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  90.6%%\n",
    "    Precision:     32.4%%\n",
    "    Recall:        68.7%%\n",
    "    F-Measure:     44.0%%\n",
    "```\n",
    "\n",
    "* 90% of the data\n",
    "\n",
    "```\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  91.2%%\n",
    "    Precision:     32.0%%\n",
    "    Recall:        68.0%%\n",
    "    F-Measure:     43.5%%\n",
    "```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a user fed definition\n",
    "print(chunker.parse(pos_tag(word_tokenize(\"We define a Banach space as a complete vector space.\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_metrics(int_range, chunker_fn, data_set = test_samples, print_output=False):\n",
    "    '''\n",
    "    `int_range` is an integer range\n",
    "    NEEDS A TEST_SAMPLES VARIABLE CREATED WHEN SPLITTING THE \n",
    "    TRAINING AND TESTING DATA\n",
    "    Returns two vectors ready to be used in the \n",
    "    metrics classification function\n",
    "    '''\n",
    "    if isinstance(int_range, int):\n",
    "        int_range = [int_range]\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in int_range:\n",
    "        sample = data_set[i]\n",
    "        sm = [s[0] for s in sample]\n",
    "        y_true_tmp = [s[1] for s in sample]\n",
    "        predicted = [v[2] for v in tree2conlltags(chunker_fn.parse(sm))]\n",
    "        y_true += y_true_tmp\n",
    "        y_pred += predicted\n",
    "        if print_output:\n",
    "            for k,s in enumerate(sm):\n",
    "                print('{:15} {:>10}  {:>10}'.format(s[0], y_true_tmp[k], predicted[k]))\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OO = prepare_for_metrics(19, chunker, data_set=test_samples, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, predicted = prepare_for_metrics(range(len(test_samples)), chunker)\n",
    "print(metrics.classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
