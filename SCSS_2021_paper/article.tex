\documentclass[submission,copyright,creativecommons]{eptcs}
\providecommand{\event}{SCSS 2021} % Name of the event you are submitting to
%\usepackage{breakurl}             % Not needed if you use pdflatex only.
\usepackage{underscore}           % Only needed if you use pdflatex.
%\usepackage[hyperref]{latex/acl2021}
\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath}
%\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage[utf8x]{inputenc}
\usepackage{minted} 
\usepackage{caption}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{pgfplots}

\newcommand{\argot}{\texttt{ArGoT}\xspace}
\newcommand{\isa}{\texttt{IS-A}\xspace}

\title{\texttt{ArGoT}: A Glossary of Terms extracted from the ArXiv}
\author{Luis Berlioz
    \institute{University of Pittsburgh\\
    Pennsylvania, USA}
\email{lab232@pitt.edu}
}
\def\titlerunning{\texttt{ArGoT}: A Glossary Terms extracted from the ArXiv}
\def\authorrunning{Luis Berlioz}
\begin{document}
\maketitle

\begin{abstract}
    \input{abstract}
\end{abstract}

\section{Introduction and Motivation}
Mathematical writing usually adheres to strict conventions of rigor
and consistent usage of terminology.
New concepts are usually introduced in characteristically worded 
definitions (with patterns like \textit{if
    and only if} or \textit{we say a group is abelian...}). 
This feature can be used to train language models to detect if a term is defined in a text.
%Also, the old 
%terms on which the new ones depend are seldom skipped.  
Using this, we have created \argot (\textbf{ar}Xiv \textbf{G}lossary \textbf{o}f \textbf{T}erms), a silver standard data set of terms defined in
the Mathematical articles of the arXiv website. 
We showcase several interesting applications of this data. The data set includes the articles and  paragraph number in which each term appears. By using article metadata, we show that this can be an effective way of assigning an arXiv mathematical category\footnote{https://arxiv.org/archive/math} to each term.
%We demonstrate that hyperbolic word embedding language models  
%can effectively capture relations of entailment in advanced
Another application is to join the terms with more than one word into a single token.
These phrases usually represent important mathematical concepts with a specific meaning.
We show how standard word embedding models like word2vec
\cite{word2vec} and GloVe \cite{pennington2014glove} capture this by embedding phrases instead of individual words.
Even more, the word-vector can be used to predict which mathematical field the term belongs to, and hypernimity relations.
%And in the future, will provide abundant training examples for NLU and automated reasoning.

All these properties  makes \argot  a data set that will be of
interest to the broader NLP research community by providing abundant
examples for automated reasoning and NLU systems. 
Our main objective is to organize a comprehensive dependency graph of mathematical concepts that can be aligned with existing libraries of formalized mathematics like \texttt{mathlib}\footnote{https://github.com/leanprover-community/mathlib}.
The data is downloadable from \url{https://gl.kwarc.info/lab156/math-argot} and the all the code that went
into producing it is in: \url{https://github.com/lab156/arxivDownload}
%the text with a unique predictability that enables the NLP researcher to perform
%adventurous experimentation and simple debugging.

This data set was created  as part of the Formal Abstracts project.
Our group has benefited from a grant from the Sloan  Foundation
(G-2018-10067) and from the computing resources startup allocation
\#TG-DMS190028 and \#TG-DMS200030 on the Bridges-2
supercomputer at the Pittsburgh Supercomputing Center (PSC).
\begin{table}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        %\centering
%\begin{table}
    \small
\centering
\begin{tabular}{lr}
    \hline \textbf{Term} &  \textbf{Count} \\ \hline
lie algebra & 20524 \\
%suppose & 18043 \\
hilbert space & 16881 \\
function & 14920 \\
banach space & 14461 \\
metric space & 12882 \\
\_inline\_math\_-module & 12731 \\
topological space & 12518 \\
%sequence & 12308 \\
disjoint union & 11436 \\
vector space & 11337 \\
simplicial complex & 10943 \\
%graph & 10811 \\
%map & 10654 \\
%morphism & 10596 \\
\hline

\end{tabular}
\caption{\label{term-cnt-tab} Most common multiword entries in the data base. }
%\end{table}
        \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
%\begin{table}
    \small
\centering
\begin{tabular}{lrrr}
    \hline
    \multicolumn{4}{c}{Classification Task} \\
    \hline
\textbf{Method}  & \textbf{Precision} &  \textbf{Recall} & \textbf{F1}\\ 
\hline
    SGD-SVM & 0.88 & 0.87 & 0.87 \\
    Conv1D & 0.92 & 0.92 & 0.92 \\
    BiLSTM & 0.93 & 0.93 & 0.93 \\
     \hline
    \hline
    \multicolumn{4}{c}{NER Task} \\
    \hline
    ChunkParse & 0.32 & 0.68 & 0.43 \\
    LSTM-CRF & 0.69 & 0.65 & 0.67 \\
    \hline
\end{tabular}
\caption{\label{metric-comp} Training metrics on the classification and NER tasks.}
%\end{table}%% End of second table
\end{minipage}
\end{table}




\section{Description of the Term-Definition Extraction Method}
In \cite{glossary, Deyan1}, the authors describe the 
method used  to obtain the training data for a text classification
model that identifies definitions and the Named Entity Recognition (NER) model that identifies the term being defined. 

The classification task consists of training a binary classifier to determine whether a paragraph is a definition or not. We use the \verb/\begin{definition}...\end{definition}/ in the article's \LaTeX{} source to identify true examples. To gather non-definitions, we randomly sample paragraphs out of the same articles.
The source of the training data is the \LaTeX{} source code of the articles available from the arXiv website. A total of 1,552,268 paragraphs labeled as definitions or non-definitions were produced for training. It was split as follows: 80\% training 10\% testing and 10\% validation. This data was used to train three different and common classification models:
\begin{itemize}
    \item The Stochastic Gradient Descent with Support Vector Machines (SGD-SVM). 
    \item The one-dimensional convolutions (Conv1D) neural network.
    \item And Bidirectional LSTM (BiLSTM). 
\end{itemize}
For the first method, we used the implementation distributed with scikit-learn library \cite{scikit-learn}. The last two were implemented in Tensorflow. Table \ref{metric-comp} shows the most common metrics of performance for each method. 

The definitions are then fed into a NER model to identify the term being defined in them.
The data used to train the NER model comes from the Wikipedia English dump\footnote{\url{https://dumps.wikimedia.org/}} and several mathematical websites like
PlanetMath\footnote{\url{https://planetmath.org/}} and The Stacks
Project\footnote{\url{https://stacks.math.columbia.edu/}}.

We tested two different implementations of the NER system, the first is the \emph{ChunkParse} algorithm available from the NLTK library \cite{bird2009natural}. The second is a time-distributed LSTM (LSTM-CRF) \cite{huang2015bidirectional}. Both architectures use a similar set of features that in addition to the words that form the text, detect if the word is capitalized, its part-of-speech (POS) and parses punctuation e.g. to tell if a period is part of an abbreviation or an end of line.
To compare the two implementations, we used the \texttt{ChunkScore} method in
the NLTK library \cite{bird2009natural}. The results appear in Table \ref{metric-comp}. 

We have compiled two different and independent glossaries by running the 
algorithm through all of the arXiv's mathematical content. 
The first one  is based on neural networks (NN), it uses  LSTM for both the classification and NER tasks. 
In contrast, the second one combines the SGD and ChunkParser method to provide a completely independent approach to the previous model.

It is interesting to compare the results obtained using the two models. For the classification task, we have observed Cohen's kappa ($\kappa$) inter-rater agreement of 93\% between the results produced by the two methods.
This corresponds to a high degree of agreement between the two classifiers \cite{cohenkappa}.

As for the final results, Figure \ref{sizes} compares the two glossaries by counting the number of times a term appears in either glossary, and the number of distinct terms.
The results point to a high consistency of the two systems on a relatively small set of 350,000 terms.

Table~\ref{term-cnt-tab} lists some of the most frequently found terms in the
data set. 
\begin{figure}
    \centering
    \input{data-sizes.pgf}
    \caption{\label{sizes} Comparison of the two glossaries. The bar graph on top counts the total appearances of a term in both the NN and SGD glossaries. The bottom compares the relative sizes of the NN-only, intersection, and SGD-only distinct terms.}
\end{figure}

\subsection{Format and Design of the Data Set}
The \argot data set is distributed in the form of compressed
XML
files that follow the same naming convention the arXiv's bulk download
distribution\footnote{arXiv Bulk Data Access: \url{https://arxiv.org/help/bulk\_data}}.
For instance, Table \ref{glossary-example} shows a sample entry
in the fifth file corresponding to July, 2014. The definition's
statement and terms (definiendum) are specified in the \texttt{stmnt} and
\texttt{dfndum} tags respectively and the paragraph \texttt{index} is
specified as an attribute of the \texttt{definition} tag.

%%% database entry example
\begin{table*}[h]
    \centering
    \begin{minted}[fontsize=\small]{xml}
    <article name="1407_005/1407.2218/1407.2218.xml" num="89">
    <definition index="51">
        <stmnt> Assume _inline_math_. We define the following space-time 
        norm if _inline_math_ is a time interval _display_math_ </stmnt>
        <dfndum>space-time norm</dfndum>
    </definition>
    </article>
\end{minted}
\caption{\label{glossary-example} Example of an entry in the term's data set. The statement of the definition is contained in the $<$stmnt$>$ tag. The terms (definiendum) are listed as $<$dfndum$>$ tags. Each entry contains all the information to recover, article's name and paragraph's position.}
\end{table*}


%\begin{table}
\begin{figure}[h!]
\centering
\begin{minipage}{0.4\textwidth}
    \footnotesize
\begin{tabular}{lc}
\hline
\textbf{Category:} & \textbf{Count}\\
\hline
math.FA& 5922 \\
 math.AP& 2045 \\
 math.PR& 1022 \\
 math.DS& 833 \\
 math.OA& 595 \\
 math.CA& 535 \\
 math.DG& 483 \\
 math-ph& 466 \\
 math.OC& 398 \\
 math.CV& 304 \\
 math.NA& 275 \\
 math.GR& 226 \\
 math.MG& 173 \\
 math.LO& 168 \\
 math.SP& 163 \\
math.NT& 131 \\
\hline
\end{tabular}
\begin{tabular}{lc}
\hline
\textbf{Category:} & \textbf{Count}\\
\hline
 math.GN& 108 \\
 math.RT& 85 \\
 math.SG& 77 \\
 math.GT& 76 \\
 math.CO& 61 \\
 math.ST& 61 \\
 math.KT& 50 \\
 math.GM& 48 \\
 math.AG& 35 \\
 math.RA& 33 \\
 math.HO& 32 \\
 math.CT& 23 \\
 math.AT& 15 \\
 math.QA& 10 \\
 math.AC& 8 \\
         & \\
\hline
\end{tabular}
\captionof{table}{Category profile for the term: \emph{Banach Space}. The codes
    are part of the metadata for each arXiv article.
}\label{tab:categories}
%\end{table}
\end{minipage}\hfill
\begin{minipage}{0.55\textwidth}
%\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{images/barcomp.png}
    \captionof{figure}{\label{bar} Comparison between the term's category
        distribution and baseline distribution. Only categories with
        the highest values for the term are shown.}
\end{minipage}
\end{figure}

%\subsection{Related Work}
%There have been several recent projects related to NLP and information extraction from scholarly papers. In \cite{kang-etal-2020-document}, the 

\section{Augmenting Terms with ArXiV's Metadata}
Each mathematical article in the arXiv is classified in one or more
\emph{categories}  by the author at
the time of submission. Categories include \texttt{math.FA} and  \texttt{math.PR} which stand for Functional Analysis and Probability respectively. The full list is available at \url{https://arxiv.org/archive/math}.
This is part of  the arXiv's metadata and also records information like the list of authors, math subject classification (MSC)~codes, date of submission, etc. 

By counting the categories in which a certain term is used, we get an
idea of the subjects that it belongs to. In Table
\ref{tab:categories}, we see the category profile of a very common
term. Since the number of articles in each category varies
significantly, we also take into account the baseline distribution,
that is, the ratio of articles in each category to the total number of
articles.
Hence, it is possible to give an empirical score of a term's
pertinence to a certain category by comparing its category profile
with the baseline distribution. In order to measure how much of an 
outlier a term is to the baseline distribution, we use the KL-divergence:
$$D_{\text{KL}}(P \Vert Q) = \sum_{x\in X} P(x)\log(P(x)/Q(x)),$$
where $P$ and $Q$ are the probability distributions of the term and the baseline respectively. And, $X$ is the set of all the categories.

%of the math articles in the following ways: First, We remove all \LaTeX{} specific code, leaving only the natural language text. Second, we normalize the text. This includes converting all characters to lowercase, removing accents from letters, removing non-ascii characters, etc. Lastly, we 

The next step is to generate word embeddings. To prepare for this, we modify the text by joining multiword terms in \argot to produce individuals tokens. After normalizing the text, i.e. converting to lowercase and removing punctuation and special characters; the result is a large amount of text that is ready to be consumed by either the word2vec or GloVe algorithms. 
In Figure~\ref{scatter}, we observe a t-SNE (t-distributed stochastic
neighbor embedding) visualization of a word2vec 
model produced this way. In this image, each term is assigned its most frequent category. Notice that even though the \argot data set has no access to the arXiv categories, the vectors in the same category cluster together.
We consider this as a strong indication of alignment between clusters and categories.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{scatter_plot/m80p300opt135.png}
    \caption{\label{scatter} t-SNE visualization of the word vectors
        of selected terms in the data set. The terms are selected to be specific to 
        the four categories in the picture.  Points with a label are selected
    at random.}
\end{figure*}
  



%\section{Distributed Representations and Semantic Evaluation}
%\section{Distributed Representations with the arXiv's Content} 
%We produced word embeddings of the arXiv in two rounds. The first one
%was used in the classification and NER tasks, and the second one in
%the embedding of joined terms. The source was processed in the
%following ways: Substitute all math formulas, citations and
%references with tokens, i.e. (\_inline\_math\_, \_cite\_, \_ref\_).
% Perform usual text normalization and tokenization for word 
%embeddings. This includes removal of caps, numbers, non-ascii
%characters, etc. In the second round,  Join the occurring instances 
%of a term to create a unique token, for instance: \emph{banach space}
%is joined as \emph{banach\textbf{\_}space}.
%We produced both GloVe and word2vec word
%embeddings and noticed no significant difference in performance.

\section{Using Hyperbolic Word Embeddings to Extract Hypernymy Relations}
It is natural to want to organize mathematical concepts into taxonomies of various sorts.
For instance, the SMGloM project \cite{ginev2016smglom} introduced a rich standard for mathematical ontologies. Another approach aims to create a semantic hierarchy of concepts such that for a given term we can enumerate all its hypernyms \cite{wang-etal-2017-short}.

This can be  achieved by counting the co-ocurrence \cite{hearst-1992-automatic} of terms in definitions. This approach has certain drawbacks, for instance, it relies on co-ocurrence examples for each pair of terms, this ends up producing an abundance of disconnected (i.e. not co-ocurring) terms \cite{aly-etal-2019-every}.

Another possibility, involves the use of \emph{hyperbolic word embeddings}, in this setting the hypernimity relation becomes a geometric vector in hyperbolic space. This implies that every two terms in the embedding can be compared by using the hyperbolic metric.
This type of word embeddings is known to outperform euclidean models in the representation of hierarchical structures \cite{facebookembeds}.

We used PoincareGlove \cite{tifrea2018poincare} to create hyperbolic
word embeddings. This algorithm modifies the GloVe \textit{euclidean} objective function to use a hyperbolic metric instead. 
In addition to the same text input as word2vec and GloVe, this model requires a small set of examples in order to interpret the embedding. 
For general purpose English text, WordNet \cite{wordnet} is the standard  choice. 
In WordNet, every entry is assigned an integer level in a hypernymy hierarchy (this is the
\texttt{max\_depth} attribute of the NLTK's WordNet
API\footnote{https://www.nltk.org/howto/wordnet.html}). 

To generate something  analogous to WordNet levels for mathematical content, we opted for the PlanetMath data set. This is due to its relatively small size, broad coverage of mathematical knowledge and independence of the arXiv data. 
 Given two term-definition pairs $(t_1, D_1)$ and $(t_2, D_2)$, we say that term
$t_2$ \emph{depends} on the term $t_1$ if $D_2$ contains $t_1$.
For small sets of term-definition pairs with no interdependence, this
simple criterion is enough to create a directed graph $(V, E)$ where
$V$ is the set of all the terms and $E$ is the set of all the
dependency relations. To assign a level $\lambda (v)$ to every 
vertex $v\in V$, solve the following integer linear program:
\begin{align*}
    \text{min} & \quad \sum_{(v,w) \in E} \lambda(w) - \lambda(v)  \\
    \text{s.t.} & \quad \lambda(w) - \lambda(v) \geq 1  \\
     & \forall (v,w) \in E. 
\end{align*}
This linear model appears in \cite{graphsGasner} as a subtask of a directed graph drawing algorithm. There, it is used to estimate the ideal number of levels to draw a directed graph. 


Table \ref{tab:hypernyny} shows the nearest neighbors of four
different terms.  The neighbors are found using the Euclidean distance. 
The terms are 
sorted in order of the average value of their $y$-coordinates (which in
the upper-half plane model represents the variance of the underlying
Gaussian distribution). This is referred to as the \isa rating. 


\begin{table}
    \small
\centering
\begin{tabular}{ll|ll}
    \hline \textbf{Term} &  \isa &  
    \textbf{Term} &  \isa \\ \hline
    hyperbolic\_metric & -1.11 &  &\\
euclidean\_metric & -0.59  & digraph & -0.51 \\
metrics & -0.58 & undirected\_graph & -0.35 \\
riemannian\_metric & -0.46  &  undirected & -0.20 \\
riemannian & -0.42  & \textbf{directed\_graph} &  0.0\\
riemannian\_manif & -0.40 & graph & 1.24 \\
curvature & -0.27  & & \\
\textbf{metric} & 0.0 & & \\
\hline
banach\_algebra & -1.11  & probability\_distr & -0.24 \\
normed\_space & -0.98 & \textbf{random\_variable} & 0.0 \\
banach\_spaces & -0.38 & expectation & 0.23 \\
banach & -0.29  & distribution & 0.46 \\
closed\_subspace & -0.25 & probability & 0.67 \\
\textbf{banach\_space} & 0.0 & & \\
norm & 0.79 & & \\

\end{tabular}
\caption{\label{tab:hypernyny} 
   Query results sorted by \isa score (terms in upper lines tend to depend semantically on lower lines).  Cosine similar words were sorted by the
    \isa rating of the term in bold font. }
\end{table}

\section{Conclusions and Further Work}
We introduced \argot, an comprehensive glossary of mathematics
automatically collected from the mathematical content on the arXiv website. 
Essentially, it is set of term-definition pairs, where 
each pair can be contextualized in a large semantic network of
mathematical knowledge, i.e., dependency graph. We also showed how this 
network is reflected in the latent space of its vector embeddings. This
has great potential for use in experimentation of natural language
algorithms, by providing a source of logically consistent data. 

This project is an ongoing effort to align mathematical concepts
in natural language with  online repositories of formalized
mathematics like
\texttt{mathlib}\footnote{https://github.com/leanprover-community/mathlib}. 
As described in \cite{kaliszyk2016standard}, this type of alignment is called \emph{automatically found aligment}.

In the near future we plan to further improve on the classification 
and NER tasks by creating a data set using solely the neural version
of the classifier and NER model. Also,  by using state-of-the-art
methods like the masked transformer language model \cite{bert} to
further improve the results. 
We also plan to compile the complete dependency graph in one 
large graph database. 


%This type of domain specific data collection 
%and ontology population is becoming more commonplace as NLP models
%improve in performance. We hope this data set and will be helpful 
%to the researchers a similar project of 

%In order to perceive this property, 
%It is common for mathematical terms to be composed of multiple tokens.
%For instance, \emph{Riemann integral} and
%\emph{integral domain}. This means that detecting the multi-word entitiesis  necessary in order to take full advantage of mathematical text. 



%\nocite{*}
\bibliographystyle{eptcs}
\bibliography{article}
\end{document}
