{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "NER",
     "Tensorflow2.0"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional,\\\n",
    "                      GRU, Dropout, GlobalAveragePooling1D, Conv1D, TimeDistributed\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "import math\n",
    "#import collections.Iterable as Iterable\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, inspect, sys\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "from unwiki import unwiki\n",
    "import ner\n",
    "from embed_utils import open_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ginebra', 'z-1', 'cf', 'fig', 'i.e', 'ph.d', 'gr', 'u.s', 'technician', 'u.n', 'pl', 'p.h.d', 'nationality', 'jie', 'happiness', '4n', 'exhibition', 'j.w', 'hk', 'ἀ', 'c.w', 'q.v', 'j.c', 'r.a', 'vibrations', 'a.k.a', 'mtv', 'n.b', 'e.g', 'spacewalks', 'qom', 'rod', '4π', 'juniper', \"'is\", 'sow', 's.c', 'al', 's^2', 'ye', 'i.i.d', 'eng', 'j.d', 'bx', 'x+2', 's.t', 'jr', 'etc', 'oct', 'rim', 'a.i', 'fins', 'p.o.a', 'az', 'fn', 'ca', 'rye', 'f.h', 'g.i'}\n"
     ]
    }
   ],
   "source": [
    "with open('/media/hd1/wikipedia/wiki_definitions_improved.txt', 'r') as wiki_f:\n",
    "    wiki = wiki_f.readlines()\n",
    "\n",
    "# Get data and train the Sentence tokenizer\n",
    "# Uses a standard algorithm (Kiss-Strunk) for unsupervised sentence boundary detection\n",
    "text = ''\n",
    "for i in range(1550):\n",
    "    text += unwiki.loads(eval(wiki[i].split('-#-%-')[2]))\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "sent_tok = PunktSentenceTokenizer(trainer.get_params())\n",
    "print(sent_tok._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e.g', 'cf', 'i.e', 'l'}\n"
     ]
    }
   ],
   "source": [
    "# Detecting abbreviations from clean text\n",
    "with open('/media/hd1/clean_text/math05', 'r') as text_f:\n",
    "    text = text_f.read()[:100000]\n",
    "\n",
    "# Get data and train the Sentence tokenizer\n",
    "# Uses a standard algorithm (Kiss-Strunk) for unsupervised sentence boundary detection\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text, finalize=True)\n",
    "sent_tok = PunktSentenceTokenizer(trainer.get_params())\n",
    "print(sent_tok._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'eq', 'i.e', 'e.g', 'f.g', 'eqs', 'w.r.t'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* protect _inline_math_ from keras tokenizer, right now it is breaking it up\n",
    "* Search for a minimal stemmer that finds for example zero-sum games in zero-sum game or absolute continuity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "word_tok = Tokenizer(oov_token='<UNK>')\n",
    "clean_str = lambda s: unwiki.loads(eval(s)).replace('\\n', ' ')\n",
    "fields = {'texts': [], 'titles': [], }\n",
    "for w in wiki:\n",
    "    title, section, defin_parag = w.split('-#-%-')\n",
    "    defin_parag = clean_str(defin_parag)\n",
    "    for defin in sent_tok.tokenize(defin_parag):\n",
    "        fields['titles'].append(title.lower().strip())\n",
    "        fields['texts'].append(defin)\n",
    "word_tok.fit_on_texts(fields['titles'] + fields['texts'])\n",
    "\n",
    "rev_word_index = (1 + len(word_tok.word_index))*['***']\n",
    "for word,ind in word_tok.word_index.items():\n",
    "    rev_word_index[ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Despite this, unions were formed and began to acquire political power, eventually resulting in a body of labour law that not only legalized organizing efforts, but codified the relationship between employers and those employees organized into unions.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields['texts'][1683]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59 empty sentences\n"
     ]
    }
   ],
   "source": [
    "fields['labels'] = []\n",
    "fields['tokens'] = word_tok.texts_to_sequences(fields['texts'])\n",
    "empty_sentence_lst = []\n",
    "for N in range(len(fields['texts'])):\n",
    "    title_lst = word_tok.texts_to_sequences([fields['titles'][N].strip()])[0]\n",
    "    tags = ner.bio_tag.bio_tkn_tagger(title_lst, fields['tokens'][N] )\n",
    "    try:\n",
    "        fields['labels'].append(list(zip(*tags))[1])\n",
    "    except IndexError:\n",
    "        fields['labels'].append(['0'])\n",
    "        empty_sentence_lst.append(N)\n",
    "print(f'Found {len(empty_sentence_lst)} empty sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the title of the article is: palestinians in lebanon\n",
      "10505 palestinians     2\n",
      "    9           in     3\n",
      " 7817      lebanon     3\n",
      "  200      include     1\n",
      "    1          the     1\n",
      " 9535  palestinian     1\n",
      " 7219     refugees     1\n",
      "  119          who     1\n",
      "18498         fled     1\n",
      "    8           to     1\n",
      " 7817      lebanon     1\n",
      "  336       during     1\n",
      "    1          the     1\n",
      " 6745         1948     1\n",
      " 6649    palestine     1\n",
      "  574          war     1\n",
      "    6          and     1\n",
      "   57        their     1\n",
      " 2780  descendants     1\n",
      "    1          the     1\n",
      " 9535  palestinian     1\n",
      "22650     militias     1\n",
      "   23        which     1\n",
      "22551      resided     1\n",
      "    9           in     1\n",
      " 7817      lebanon     1\n",
      "    9           in     1\n",
      "    1          the     1\n",
      " 2786        1970s     1\n",
      "    6          and     1\n",
      " 2997        1980s     1\n",
      "    6          and     1\n",
      " 9535  palestinian     1\n",
      " 9425    nationals     1\n",
      "  119          who     1\n",
      "   40         have     1\n",
      " 1918     recently     1\n",
      " 3636        moved     1\n",
      "    8           to     1\n",
      " 7817      lebanon     1\n"
     ]
    }
   ],
   "source": [
    "K = 31765\n",
    "Tex = fields['texts'][K]\n",
    "Tok = fields['tokens'][K]\n",
    "Lab = fields['labels'][K]\n",
    "Tit = fields['titles'][K]\n",
    "print(f'the title of the article is: {Tit}')\n",
    "for ind, t in enumerate(Tok):\n",
    "    print('{0:>5} {1:>12} {2:>5}'.format(t, rev_word_index[t], Lab[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['maxlen'] = max([len(l) for l in fields['tokens']])//12\n",
    "cfg['padding'] = 'pre'\n",
    "train_seq = pad_sequences(fields['tokens'], maxlen=cfg['maxlen'], padding=cfg['padding'])\n",
    "train_lab = pad_sequences(fields['labels'], maxlen=cfg['maxlen'], padding=cfg['padding'])\n",
    "train_seq2 = []\n",
    "train_lab2 = []\n",
    "for ind, t in enumerate(train_lab):\n",
    "    if 2 in t:\n",
    "        train_seq2.append(train_seq[ind])\n",
    "        train_lab2.append(train_lab[ind])\n",
    "train_seq2 = np.array(train_seq2)\n",
    "train_lab2 = np.array(train_lab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerModel(tf.keras.Model):\n",
    "    def __init__(self, hidden_num, vocab_size, label_size, embedding_size):\n",
    "        super(NerModel, self).__init__()\n",
    "        self.hidden_num = hidden_num\n",
    "        self.vocab_size = vocab_size\n",
    "        self.label_size = label_size\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size, embedding_size)\n",
    "        self.biLSTM = Bidirectional(LSTM(hidden_num, return_sequences=True))\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense = Dense(label_size)\n",
    "        \n",
    "        self.transition_params = tf.Variable(tf.random.uniform(shape=(label_size, label_size)))\n",
    "        \n",
    "    def call(self, text, labels=None, training=None):\n",
    "        text_lens = tf.math.reduce_sum(tf.cast(tf.math.not_equal(text, 0), dtype=tf.int32), axis=-1)\n",
    "        inputs = self.embedding(text)\n",
    "        inputs = self.biLSTM(inputs)\n",
    "        inputs = self.dropout(inputs, training)\n",
    "        logits = self.dense(inputs)\n",
    "        \n",
    "        if labels is not None:\n",
    "            label_sequences = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "            log_likelihood, self.transition_params = \\\n",
    "            tfa.text.crf_log_likelihood(logits, label_sequences, text_lens,\n",
    "                                        transition_params=self.transition_params)\n",
    "            return logits, text_lens, log_likelihood\n",
    "        else:\n",
    "            return logits, text_lens\n",
    "        \n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 20, loss 5.8109 , accuracy 0.9755\n",
      "model saved\n",
      "epoch 3, step 40, loss 1.7540 , accuracy 0.9940\n",
      "model saved\n",
      "epoch 4, step 60, loss 1.6287 , accuracy 0.9976\n",
      "model saved\n",
      "epoch 6, step 80, loss 0.6859 , accuracy 0.9990\n",
      "model saved\n",
      "epoch 7, step 100, loss 1.0274 , accuracy 0.9979\n",
      "epoch 9, step 120, loss 0.5475 , accuracy 0.9989\n",
      "epoch 10, step 140, loss 0.6320 , accuracy 0.9987\n",
      "epoch 12, step 160, loss 0.5858 , accuracy 0.9989\n",
      "epoch 13, step 180, loss 0.7105 , accuracy 0.9984\n",
      "epoch 15, step 200, loss 0.5403 , accuracy 0.9988\n",
      "epoch 16, step 220, loss 0.5145 , accuracy 0.9987\n",
      "epoch 18, step 240, loss 0.6273 , accuracy 0.9986\n",
      "epoch 19, step 260, loss 0.0032 , accuracy 1.0000\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# Train NER model\n",
    "cfg['learning_rate'] = 0.1\n",
    "model = NerModel(64, len(word_tok.word_index)+1, 4, 100)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "def train_one_step(text_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, text_lens, log_likelihood = model(text_batch, labels_batch, training=True)\n",
    "        loss = - tf.reduce_mean(log_likelihood)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, logits, text_lens\n",
    "\n",
    "def get_acc_one_step(logits, text_lens, labels_batch):\n",
    "    paths = []\n",
    "    accuracy = 0\n",
    "    for logit, text_len, labels in zip(logits, text_lens, labels_batch):\n",
    "        viterbi_path, _ = tfa.text.viterbi_decode(logit[:text_len], model.transition_params)\n",
    "        paths.append(viterbi_path)\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([viterbi_path],\n",
    "                                                            padding='post'), dtype=tf.int32),\n",
    "            tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([labels[:text_len]],\n",
    "                                                            padding='post'), dtype=tf.int32)\n",
    "        )\n",
    "        accuracy = accuracy + tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        # print(tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))\n",
    "    accuracy = accuracy / len(paths)\n",
    "    return accuracy\n",
    "\n",
    "best_acc = 0\n",
    "step = 0\n",
    "epochs = 20\n",
    "bs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for (text_batch, labels_batch) in \\\n",
    "    [[train_seq2[bs*i:bs*(i+1)], train_lab2[bs*i:bs*(i+1)]]\\\n",
    "     for i in range(math.ceil(len(train_seq2)/bs))]:\n",
    "        step = step + 1\n",
    "        loss, logits, text_lens = train_one_step(text_batch, labels_batch)\n",
    "        if step % 20 == 0:\n",
    "            accuracy = get_acc_one_step(logits, text_lens, labels_batch)\n",
    "            print('epoch %d, step %d, loss %.4f , accuracy %.4f' % (epoch, step, loss, accuracy))\n",
    "            if accuracy > best_acc:\n",
    "                best_acc = accuracy\n",
    "                #ckpt_manager.save()\n",
    "                print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ner_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  4208300   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional multiple                  84480     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  516       \n",
      "=================================================================\n",
      "Total params: 4,293,312\n",
      "Trainable params: 4,293,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_str = 'A banach space is defined as named entity recognition'\n",
    "sample_tok = word_tok.texts_to_sequences([sample_str])\n",
    "sample_pad = pad_sequences(sample_tok, maxlen=cfg['maxlen'], padding=cfg['padding'])\n",
    "pred = [model.predict(text_batch[i])[1] for i in range(len(text_batch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 667122 and dimension of embed: 200\n"
     ]
    }
   ],
   "source": [
    "cf = {'input_dim': len(word_tok.word_index)+1,\n",
    "      'output_dim': 25,\n",
    "     'input_length': max([len(l) for l in fields['tokens']])//12,\n",
    "     'n_tags': 4,\n",
    "     'batch_size': 1000}\n",
    "    \n",
    "# Define the categorical labels\n",
    "train_lab2_cat = np.array([to_categorical(c, num_classes=cf['n_tags']) for c in train_lab2])\n",
    "embed_matrix = np.zeros((cf['input_dim'], 200))\n",
    "coverage_cnt = 0\n",
    "with open_w2v('/media/hd1/embeddings/model14-14_12-08/vectors.bin') as embed_dict:\n",
    "    for word, ind in word_tok.word_index.items():\n",
    "        vect = embed_dict.get(word)\n",
    "        if vect is not None:\n",
    "            vect = vect/np.linalg.norm(vect)\n",
    "            embed_matrix[ind] = vect\n",
    "            coverage_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = {'input_dim': len(word_tok.word_index)+1,\n",
    "      'output_dim': 25,\n",
    "     'input_length': max([len(l) for l in fields['tokens']])//12,\n",
    "     'n_tags': 4,\n",
    "     'batch_size': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_101 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_51 (Embedding)     (None, 42, 25)            1052100   \n",
      "_________________________________________________________________\n",
      "bidirectional_50 (Bidirectio (None, 42, 50)            10200     \n",
      "_________________________________________________________________\n",
      "lstm_101 (LSTM)              (None, 42, 25)            7600      \n",
      "_________________________________________________________________\n",
      "time_distributed_49 (TimeDis (None, 42, 4)             104       \n",
      "=================================================================\n",
      "Total params: 1,070,004\n",
      "Trainable params: 1,070,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DEFINE MODEL WITH biLSTM AND TRAIN FUNCTION    \n",
    "def get_bilstm_lstm_model(cfg_dict):\n",
    "    model = Sequential()\n",
    "    # Add Embedding layer\n",
    "   # model.add(Embedding(cfg_dict['input_dim'], \n",
    "   #                     output_dim=cfg_dict['output_dim'],\n",
    "   #                     input_length=cfg_dict['input_length'],\n",
    "   #                    weights = [embed_matrix],\n",
    "   #                    trainable = False))\n",
    "    model.add(Embedding(cfg_dict['input_dim'], \n",
    "                        output_dim=cfg_dict['output_dim'],\n",
    "                        input_length=cfg_dict['input_length']))\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=cfg_dict['output_dim'],\n",
    "                                 return_sequences=True,\n",
    "                                 dropout=0.2, \n",
    "                                 recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=cfg_dict['output_dim'],\n",
    "                   return_sequences=True, dropout=0.2, recurrent_dropout=0.2,\n",
    "                   recurrent_initializer='glorot_uniform'))\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(cfg_dict['n_tags'], activation=\"relu\")))\n",
    "    #Optimiser \n",
    "    adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(X, y, model, epochs=10):\n",
    "    out = {'accuracy': [], 'loss': [], 'val_accuracy': [], 'val_loss': []}\n",
    "    # fit model for one epoch on this sequence\n",
    "    res = model.fit(X, y, verbose=1, epochs=epochs,\n",
    "                    batch_size=cf['batch_size'],\n",
    "                    validation_split=0.2 )\n",
    "    out['accuracy'].append(res.history['accuracy'])\n",
    "    out['val_accuracy'].append(res.history['val_accuracy'])\n",
    "    out['loss'].append(res.history['loss'])\n",
    "    out['val_loss'].append(res.history['val_loss'])\n",
    "    return out\n",
    "model_bilstm_lstm = get_bilstm_lstm_model(cf)\n",
    "#plot_model(model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "9/9 [==============================] - 2s 231ms/step - loss: 4.7187 - accuracy: 0.4372 - val_loss: 1.1146 - val_accuracy: 0.7263\n",
      "Epoch 2/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 1.0899 - accuracy: 0.6726 - val_loss: 1.0372 - val_accuracy: 0.6554\n",
      "Epoch 3/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.9811 - accuracy: 0.6547 - val_loss: 0.9227 - val_accuracy: 0.6728\n",
      "Epoch 4/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.8891 - accuracy: 0.6846 - val_loss: 0.8544 - val_accuracy: 0.7126\n",
      "Epoch 5/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.8342 - accuracy: 0.7280 - val_loss: 0.8232 - val_accuracy: 0.7525\n",
      "Epoch 6/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.8009 - accuracy: 0.7639 - val_loss: 0.7965 - val_accuracy: 0.7820\n",
      "Epoch 7/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.7697 - accuracy: 0.7940 - val_loss: 0.7654 - val_accuracy: 0.8039\n",
      "Epoch 8/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.7399 - accuracy: 0.8158 - val_loss: 0.7351 - val_accuracy: 0.8228\n",
      "Epoch 9/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.7153 - accuracy: 0.8334 - val_loss: 0.7129 - val_accuracy: 0.8360\n",
      "Epoch 10/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.6910 - accuracy: 0.8463 - val_loss: 0.6887 - val_accuracy: 0.8470\n",
      "Epoch 11/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.6659 - accuracy: 0.8574 - val_loss: 0.6628 - val_accuracy: 0.8565\n",
      "Epoch 12/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.6385 - accuracy: 0.8659 - val_loss: 0.6355 - val_accuracy: 0.8635\n",
      "Epoch 13/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.6073 - accuracy: 0.8729 - val_loss: 0.6109 - val_accuracy: 0.8690\n",
      "Epoch 14/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.5784 - accuracy: 0.8776 - val_loss: 0.5901 - val_accuracy: 0.8735\n",
      "Epoch 15/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.5519 - accuracy: 0.8823 - val_loss: 0.5706 - val_accuracy: 0.8782\n",
      "Epoch 16/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.5264 - accuracy: 0.8874 - val_loss: 0.5466 - val_accuracy: 0.8838\n",
      "Epoch 17/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.4986 - accuracy: 0.8928 - val_loss: 0.5186 - val_accuracy: 0.8891\n",
      "Epoch 18/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.4709 - accuracy: 0.8978 - val_loss: 0.4932 - val_accuracy: 0.8936\n",
      "Epoch 19/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.4460 - accuracy: 0.9024 - val_loss: 0.4703 - val_accuracy: 0.8973\n",
      "Epoch 20/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.4248 - accuracy: 0.9061 - val_loss: 0.4526 - val_accuracy: 0.9003\n",
      "Epoch 21/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.4060 - accuracy: 0.9090 - val_loss: 0.4391 - val_accuracy: 0.9026\n",
      "Epoch 22/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.3904 - accuracy: 0.9113 - val_loss: 0.4318 - val_accuracy: 0.9045\n",
      "Epoch 23/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.3800 - accuracy: 0.9133 - val_loss: 0.4233 - val_accuracy: 0.9065\n",
      "Epoch 24/250\n",
      "9/9 [==============================] - 2s 202ms/step - loss: 0.3679 - accuracy: 0.9152 - val_loss: 0.3997 - val_accuracy: 0.9090\n",
      "Epoch 25/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.3537 - accuracy: 0.9176 - val_loss: 0.3898 - val_accuracy: 0.9108\n",
      "Epoch 26/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.3440 - accuracy: 0.9195 - val_loss: 0.3867 - val_accuracy: 0.9118\n",
      "Epoch 27/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.3366 - accuracy: 0.9202 - val_loss: 0.3906 - val_accuracy: 0.9126\n",
      "Epoch 28/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.3317 - accuracy: 0.9216 - val_loss: 0.3794 - val_accuracy: 0.9142\n",
      "Epoch 29/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.3258 - accuracy: 0.9228 - val_loss: 0.3654 - val_accuracy: 0.9161\n",
      "Epoch 30/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.3509 - accuracy: 0.9254 - val_loss: 0.3953 - val_accuracy: 0.9181\n",
      "Epoch 31/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.3656 - accuracy: 0.9265 - val_loss: 0.3894 - val_accuracy: 0.9187\n",
      "Epoch 32/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.3459 - accuracy: 0.9268 - val_loss: 0.3733 - val_accuracy: 0.9188\n",
      "Epoch 33/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.3207 - accuracy: 0.9273 - val_loss: 0.3596 - val_accuracy: 0.9188\n",
      "Epoch 34/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.3021 - accuracy: 0.9275 - val_loss: 0.3610 - val_accuracy: 0.9189\n",
      "Epoch 35/250\n",
      "9/9 [==============================] - 2s 202ms/step - loss: 0.2961 - accuracy: 0.9275 - val_loss: 0.3595 - val_accuracy: 0.9198\n",
      "Epoch 36/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.2891 - accuracy: 0.9291 - val_loss: 0.3431 - val_accuracy: 0.9218\n",
      "Epoch 37/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.2865 - accuracy: 0.9307 - val_loss: 0.3430 - val_accuracy: 0.9223\n",
      "Epoch 38/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.2843 - accuracy: 0.9311 - val_loss: 0.3524 - val_accuracy: 0.9223\n",
      "Epoch 39/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.2809 - accuracy: 0.9313 - val_loss: 0.3515 - val_accuracy: 0.9226\n",
      "Epoch 40/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.2746 - accuracy: 0.9313 - val_loss: 0.3527 - val_accuracy: 0.9228\n",
      "Epoch 41/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2726 - accuracy: 0.9318 - val_loss: 0.3288 - val_accuracy: 0.9250\n",
      "Epoch 42/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.2881 - accuracy: 0.9353 - val_loss: 0.3415 - val_accuracy: 0.9293\n",
      "Epoch 43/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.3091 - accuracy: 0.9370 - val_loss: 0.3456 - val_accuracy: 0.9296\n",
      "Epoch 44/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.3019 - accuracy: 0.9366 - val_loss: 0.3383 - val_accuracy: 0.9281\n",
      "Epoch 45/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.2835 - accuracy: 0.9357 - val_loss: 0.3357 - val_accuracy: 0.9267\n",
      "Epoch 46/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.2680 - accuracy: 0.9346 - val_loss: 0.3432 - val_accuracy: 0.9252\n",
      "Epoch 47/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.2636 - accuracy: 0.9340 - val_loss: 0.3501 - val_accuracy: 0.9244\n",
      "Epoch 48/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.2584 - accuracy: 0.9334 - val_loss: 0.3488 - val_accuracy: 0.9244\n",
      "Epoch 49/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.2532 - accuracy: 0.9339 - val_loss: 0.3461 - val_accuracy: 0.9249\n",
      "Epoch 50/250\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 0.2469 - accuracy: 0.9343 - val_loss: 0.3402 - val_accuracy: 0.9256\n",
      "Epoch 51/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2435 - accuracy: 0.9349 - val_loss: 0.3379 - val_accuracy: 0.9261\n",
      "Epoch 52/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2414 - accuracy: 0.9353 - val_loss: 0.3395 - val_accuracy: 0.9265\n",
      "Epoch 53/250\n",
      "9/9 [==============================] - 2s 194ms/step - loss: 0.2397 - accuracy: 0.9354 - val_loss: 0.3438 - val_accuracy: 0.9266\n",
      "Epoch 54/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2362 - accuracy: 0.9359 - val_loss: 0.3431 - val_accuracy: 0.9270\n",
      "Epoch 55/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.2345 - accuracy: 0.9362 - val_loss: 0.3451 - val_accuracy: 0.9273\n",
      "Epoch 56/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.2320 - accuracy: 0.9366 - val_loss: 0.3473 - val_accuracy: 0.9276\n",
      "Epoch 57/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2294 - accuracy: 0.9374 - val_loss: 0.3451 - val_accuracy: 0.9282\n",
      "Epoch 58/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2262 - accuracy: 0.9378 - val_loss: 0.3416 - val_accuracy: 0.9288\n",
      "Epoch 59/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.2250 - accuracy: 0.9381 - val_loss: 0.3424 - val_accuracy: 0.9293\n",
      "Epoch 60/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.2235 - accuracy: 0.9382 - val_loss: 0.3485 - val_accuracy: 0.9295\n",
      "Epoch 61/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.2212 - accuracy: 0.9387 - val_loss: 0.3424 - val_accuracy: 0.9299\n",
      "Epoch 62/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2196 - accuracy: 0.9389 - val_loss: 0.3431 - val_accuracy: 0.9301\n",
      "Epoch 63/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.2177 - accuracy: 0.9392 - val_loss: 0.3443 - val_accuracy: 0.9308\n",
      "Epoch 64/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.2153 - accuracy: 0.9394 - val_loss: 0.3462 - val_accuracy: 0.9300\n",
      "Epoch 65/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.2148 - accuracy: 0.9392 - val_loss: 0.3497 - val_accuracy: 0.9297\n",
      "Epoch 66/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.2149 - accuracy: 0.9388 - val_loss: 0.3502 - val_accuracy: 0.9296\n",
      "Epoch 67/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2121 - accuracy: 0.9393 - val_loss: 0.3556 - val_accuracy: 0.9299\n",
      "Epoch 68/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2102 - accuracy: 0.9396 - val_loss: 0.3481 - val_accuracy: 0.9308\n",
      "Epoch 69/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2077 - accuracy: 0.9401 - val_loss: 0.3456 - val_accuracy: 0.9314\n",
      "Epoch 70/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.2075 - accuracy: 0.9406 - val_loss: 0.3520 - val_accuracy: 0.9316\n",
      "Epoch 71/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2055 - accuracy: 0.9409 - val_loss: 0.3578 - val_accuracy: 0.9320\n",
      "Epoch 72/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.2046 - accuracy: 0.9411 - val_loss: 0.3567 - val_accuracy: 0.9323\n",
      "Epoch 73/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2012 - accuracy: 0.9414 - val_loss: 0.3424 - val_accuracy: 0.9329\n",
      "Epoch 74/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1998 - accuracy: 0.9415 - val_loss: 0.3537 - val_accuracy: 0.9327\n",
      "Epoch 75/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1993 - accuracy: 0.9414 - val_loss: 0.3655 - val_accuracy: 0.9325\n",
      "Epoch 76/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1993 - accuracy: 0.9417 - val_loss: 0.3310 - val_accuracy: 0.9335\n",
      "Epoch 77/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.2016 - accuracy: 0.9417 - val_loss: 0.3349 - val_accuracy: 0.9336\n",
      "Epoch 78/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1977 - accuracy: 0.9420 - val_loss: 0.3491 - val_accuracy: 0.9336\n",
      "Epoch 79/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1949 - accuracy: 0.9422 - val_loss: 0.3629 - val_accuracy: 0.9341\n",
      "Epoch 80/250\n",
      "9/9 [==============================] - 2s 202ms/step - loss: 0.1926 - accuracy: 0.9430 - val_loss: 0.3604 - val_accuracy: 0.9355\n",
      "Epoch 81/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1929 - accuracy: 0.9434 - val_loss: 0.3622 - val_accuracy: 0.9357\n",
      "Epoch 82/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1910 - accuracy: 0.9435 - val_loss: 0.3513 - val_accuracy: 0.9358\n",
      "Epoch 83/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1907 - accuracy: 0.9435 - val_loss: 0.3495 - val_accuracy: 0.9355\n",
      "Epoch 84/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1892 - accuracy: 0.9435 - val_loss: 0.3662 - val_accuracy: 0.9352\n",
      "Epoch 85/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1897 - accuracy: 0.9434 - val_loss: 0.3824 - val_accuracy: 0.9348\n",
      "Epoch 86/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1886 - accuracy: 0.9433 - val_loss: 0.3825 - val_accuracy: 0.9349\n",
      "Epoch 87/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1860 - accuracy: 0.9435 - val_loss: 0.3679 - val_accuracy: 0.9353\n",
      "Epoch 88/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1837 - accuracy: 0.9437 - val_loss: 0.3608 - val_accuracy: 0.9356\n",
      "Epoch 89/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1834 - accuracy: 0.9439 - val_loss: 0.3590 - val_accuracy: 0.9356\n",
      "Epoch 90/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1836 - accuracy: 0.9439 - val_loss: 0.3765 - val_accuracy: 0.9355\n",
      "Epoch 91/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1821 - accuracy: 0.9440 - val_loss: 0.3626 - val_accuracy: 0.9358\n",
      "Epoch 92/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1828 - accuracy: 0.9445 - val_loss: 0.3208 - val_accuracy: 0.9374\n",
      "Epoch 93/250\n",
      "9/9 [==============================] - 2s 194ms/step - loss: 0.1873 - accuracy: 0.9449 - val_loss: 0.3212 - val_accuracy: 0.9377\n",
      "Epoch 94/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1811 - accuracy: 0.9448 - val_loss: 0.3568 - val_accuracy: 0.9371\n",
      "Epoch 95/250\n",
      "9/9 [==============================] - 2s 204ms/step - loss: 0.1797 - accuracy: 0.9445 - val_loss: 0.3776 - val_accuracy: 0.9371\n",
      "Epoch 96/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1791 - accuracy: 0.9446 - val_loss: 0.3632 - val_accuracy: 0.9372\n",
      "Epoch 97/250\n",
      "9/9 [==============================] - 2s 202ms/step - loss: 0.1765 - accuracy: 0.9449 - val_loss: 0.3669 - val_accuracy: 0.9373\n",
      "Epoch 98/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1771 - accuracy: 0.9449 - val_loss: 0.3815 - val_accuracy: 0.9373\n",
      "Epoch 99/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1754 - accuracy: 0.9453 - val_loss: 0.3415 - val_accuracy: 0.9379\n",
      "Epoch 100/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1803 - accuracy: 0.9450 - val_loss: 0.3401 - val_accuracy: 0.9377\n",
      "Epoch 101/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1792 - accuracy: 0.9449 - val_loss: 0.3679 - val_accuracy: 0.9373\n",
      "Epoch 102/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1774 - accuracy: 0.9447 - val_loss: 0.3960 - val_accuracy: 0.9371\n",
      "Epoch 103/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1738 - accuracy: 0.9451 - val_loss: 0.3317 - val_accuracy: 0.9382\n",
      "Epoch 104/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1789 - accuracy: 0.9455 - val_loss: 0.3203 - val_accuracy: 0.9384\n",
      "Epoch 105/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1770 - accuracy: 0.9454 - val_loss: 0.3627 - val_accuracy: 0.9377\n",
      "Epoch 106/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1744 - accuracy: 0.9451 - val_loss: 0.4072 - val_accuracy: 0.9374\n",
      "Epoch 107/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1754 - accuracy: 0.9451 - val_loss: 0.4085 - val_accuracy: 0.9378\n",
      "Epoch 108/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1721 - accuracy: 0.9453 - val_loss: 0.3935 - val_accuracy: 0.9383\n",
      "Epoch 109/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1710 - accuracy: 0.9454 - val_loss: 0.3731 - val_accuracy: 0.9384\n",
      "Epoch 110/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1707 - accuracy: 0.9455 - val_loss: 0.3923 - val_accuracy: 0.9382\n",
      "Epoch 111/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1701 - accuracy: 0.9455 - val_loss: 0.3916 - val_accuracy: 0.9383\n",
      "Epoch 112/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1662 - accuracy: 0.9455 - val_loss: 0.3842 - val_accuracy: 0.9386\n",
      "Epoch 113/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1667 - accuracy: 0.9457 - val_loss: 0.3968 - val_accuracy: 0.9385\n",
      "Epoch 114/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1670 - accuracy: 0.9456 - val_loss: 0.4112 - val_accuracy: 0.9383\n",
      "Epoch 115/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1660 - accuracy: 0.9458 - val_loss: 0.4017 - val_accuracy: 0.9383\n",
      "Epoch 116/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1663 - accuracy: 0.9460 - val_loss: 0.4012 - val_accuracy: 0.9384\n",
      "Epoch 117/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1642 - accuracy: 0.9459 - val_loss: 0.3521 - val_accuracy: 0.9396\n",
      "Epoch 118/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1738 - accuracy: 0.9458 - val_loss: 0.3293 - val_accuracy: 0.9404\n",
      "Epoch 119/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1784 - accuracy: 0.9456 - val_loss: 0.3525 - val_accuracy: 0.9400\n",
      "Epoch 120/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1719 - accuracy: 0.9458 - val_loss: 0.3781 - val_accuracy: 0.9389\n",
      "Epoch 121/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1675 - accuracy: 0.9457 - val_loss: 0.3962 - val_accuracy: 0.9382\n",
      "Epoch 122/250\n",
      "9/9 [==============================] - 2s 202ms/step - loss: 0.1629 - accuracy: 0.9457 - val_loss: 0.3966 - val_accuracy: 0.9386\n",
      "Epoch 123/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1634 - accuracy: 0.9459 - val_loss: 0.3944 - val_accuracy: 0.9396\n",
      "Epoch 124/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1617 - accuracy: 0.9460 - val_loss: 0.4038 - val_accuracy: 0.9396\n",
      "Epoch 125/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1619 - accuracy: 0.9458 - val_loss: 0.3783 - val_accuracy: 0.9408\n",
      "Epoch 126/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1608 - accuracy: 0.9450 - val_loss: 0.3902 - val_accuracy: 0.9410\n",
      "Epoch 127/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1592 - accuracy: 0.9456 - val_loss: 0.4100 - val_accuracy: 0.9409\n",
      "Epoch 128/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1602 - accuracy: 0.9463 - val_loss: 0.4134 - val_accuracy: 0.9403\n",
      "Epoch 129/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1574 - accuracy: 0.9464 - val_loss: 0.4192 - val_accuracy: 0.9396\n",
      "Epoch 130/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1584 - accuracy: 0.9463 - val_loss: 0.3899 - val_accuracy: 0.9396\n",
      "Epoch 131/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1642 - accuracy: 0.9466 - val_loss: 0.3552 - val_accuracy: 0.9400\n",
      "Epoch 132/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1666 - accuracy: 0.9467 - val_loss: 0.3649 - val_accuracy: 0.9400\n",
      "Epoch 133/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1654 - accuracy: 0.9468 - val_loss: 0.3636 - val_accuracy: 0.9405\n",
      "Epoch 134/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1597 - accuracy: 0.9468 - val_loss: 0.4045 - val_accuracy: 0.9400\n",
      "Epoch 135/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1563 - accuracy: 0.9466 - val_loss: 0.4279 - val_accuracy: 0.9398\n",
      "Epoch 136/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1570 - accuracy: 0.9464 - val_loss: 0.4188 - val_accuracy: 0.9401\n",
      "Epoch 137/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1549 - accuracy: 0.9466 - val_loss: 0.4073 - val_accuracy: 0.9401\n",
      "Epoch 138/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1525 - accuracy: 0.9468 - val_loss: 0.3984 - val_accuracy: 0.9403\n",
      "Epoch 139/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1539 - accuracy: 0.9467 - val_loss: 0.4076 - val_accuracy: 0.9404\n",
      "Epoch 140/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1524 - accuracy: 0.9466 - val_loss: 0.4161 - val_accuracy: 0.9404\n",
      "Epoch 141/250\n",
      "9/9 [==============================] - 2s 194ms/step - loss: 0.1513 - accuracy: 0.9466 - val_loss: 0.4049 - val_accuracy: 0.9407\n",
      "Epoch 142/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1520 - accuracy: 0.9468 - val_loss: 0.4165 - val_accuracy: 0.9404\n",
      "Epoch 143/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1519 - accuracy: 0.9469 - val_loss: 0.4249 - val_accuracy: 0.9402\n",
      "Epoch 144/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1505 - accuracy: 0.9466 - val_loss: 0.4159 - val_accuracy: 0.9403\n",
      "Epoch 145/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1497 - accuracy: 0.9468 - val_loss: 0.4117 - val_accuracy: 0.9406\n",
      "Epoch 146/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1501 - accuracy: 0.9468 - val_loss: 0.4049 - val_accuracy: 0.9408\n",
      "Epoch 147/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1510 - accuracy: 0.9462 - val_loss: 0.3709 - val_accuracy: 0.9395\n",
      "Epoch 148/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1621 - accuracy: 0.9392 - val_loss: 0.3771 - val_accuracy: 0.9361\n",
      "Epoch 149/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1610 - accuracy: 0.9392 - val_loss: 0.4027 - val_accuracy: 0.9392\n",
      "Epoch 150/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1549 - accuracy: 0.9434 - val_loss: 0.3929 - val_accuracy: 0.9416\n",
      "Epoch 151/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1511 - accuracy: 0.9459 - val_loss: 0.4066 - val_accuracy: 0.9419\n",
      "Epoch 152/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1492 - accuracy: 0.9471 - val_loss: 0.4055 - val_accuracy: 0.9410\n",
      "Epoch 153/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1465 - accuracy: 0.9471 - val_loss: 0.4084 - val_accuracy: 0.9405\n",
      "Epoch 154/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1471 - accuracy: 0.9470 - val_loss: 0.4083 - val_accuracy: 0.9403\n",
      "Epoch 155/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1464 - accuracy: 0.9468 - val_loss: 0.4037 - val_accuracy: 0.9403\n",
      "Epoch 156/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1449 - accuracy: 0.9470 - val_loss: 0.4145 - val_accuracy: 0.9403\n",
      "Epoch 157/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1463 - accuracy: 0.9468 - val_loss: 0.4266 - val_accuracy: 0.9404\n",
      "Epoch 158/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1452 - accuracy: 0.9469 - val_loss: 0.4150 - val_accuracy: 0.9415\n",
      "Epoch 159/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1435 - accuracy: 0.9472 - val_loss: 0.4172 - val_accuracy: 0.9418\n",
      "Epoch 160/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1443 - accuracy: 0.9469 - val_loss: 0.4193 - val_accuracy: 0.9420\n",
      "Epoch 161/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1435 - accuracy: 0.9469 - val_loss: 0.4253 - val_accuracy: 0.9415\n",
      "Epoch 162/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1431 - accuracy: 0.9472 - val_loss: 0.4353 - val_accuracy: 0.9406\n",
      "Epoch 163/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1427 - accuracy: 0.9471 - val_loss: 0.4389 - val_accuracy: 0.9404\n",
      "Epoch 164/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1423 - accuracy: 0.9469 - val_loss: 0.4297 - val_accuracy: 0.9405\n",
      "Epoch 165/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1401 - accuracy: 0.9471 - val_loss: 0.4238 - val_accuracy: 0.9408\n",
      "Epoch 166/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1412 - accuracy: 0.9470 - val_loss: 0.4318 - val_accuracy: 0.9409\n",
      "Epoch 167/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1402 - accuracy: 0.9472 - val_loss: 0.4428 - val_accuracy: 0.9409\n",
      "Epoch 168/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1418 - accuracy: 0.9471 - val_loss: 0.4375 - val_accuracy: 0.9413\n",
      "Epoch 169/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1406 - accuracy: 0.9470 - val_loss: 0.4407 - val_accuracy: 0.9413\n",
      "Epoch 170/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1402 - accuracy: 0.9472 - val_loss: 0.4410 - val_accuracy: 0.9412\n",
      "Epoch 171/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1392 - accuracy: 0.9472 - val_loss: 0.4023 - val_accuracy: 0.9418\n",
      "Epoch 172/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1435 - accuracy: 0.9471 - val_loss: 0.3906 - val_accuracy: 0.9419\n",
      "Epoch 173/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1417 - accuracy: 0.9471 - val_loss: 0.4258 - val_accuracy: 0.9414\n",
      "Epoch 174/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1423 - accuracy: 0.9471 - val_loss: 0.3709 - val_accuracy: 0.9423\n",
      "Epoch 175/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1496 - accuracy: 0.9465 - val_loss: 0.3697 - val_accuracy: 0.9422\n",
      "Epoch 176/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1487 - accuracy: 0.9459 - val_loss: 0.3715 - val_accuracy: 0.9423\n",
      "Epoch 177/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1419 - accuracy: 0.9473 - val_loss: 0.3921 - val_accuracy: 0.9418\n",
      "Epoch 178/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1382 - accuracy: 0.9475 - val_loss: 0.4137 - val_accuracy: 0.9406\n",
      "Epoch 179/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1387 - accuracy: 0.9472 - val_loss: 0.3884 - val_accuracy: 0.9406\n",
      "Epoch 180/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1406 - accuracy: 0.9475 - val_loss: 0.3807 - val_accuracy: 0.9409\n",
      "Epoch 181/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1383 - accuracy: 0.9475 - val_loss: 0.3949 - val_accuracy: 0.9409\n",
      "Epoch 182/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1375 - accuracy: 0.9475 - val_loss: 0.4254 - val_accuracy: 0.9408\n",
      "Epoch 183/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1383 - accuracy: 0.9472 - val_loss: 0.4446 - val_accuracy: 0.9408\n",
      "Epoch 184/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1385 - accuracy: 0.9472 - val_loss: 0.4438 - val_accuracy: 0.9408\n",
      "Epoch 185/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1364 - accuracy: 0.9474 - val_loss: 0.4316 - val_accuracy: 0.9413\n",
      "Epoch 186/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1356 - accuracy: 0.9473 - val_loss: 0.4375 - val_accuracy: 0.9413\n",
      "Epoch 187/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1357 - accuracy: 0.9472 - val_loss: 0.4380 - val_accuracy: 0.9415\n",
      "Epoch 188/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1341 - accuracy: 0.9473 - val_loss: 0.4375 - val_accuracy: 0.9417\n",
      "Epoch 189/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1345 - accuracy: 0.9474 - val_loss: 0.4059 - val_accuracy: 0.9423\n",
      "Epoch 190/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1363 - accuracy: 0.9480 - val_loss: 0.3936 - val_accuracy: 0.9423\n",
      "Epoch 191/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1355 - accuracy: 0.9481 - val_loss: 0.4031 - val_accuracy: 0.9418\n",
      "Epoch 192/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1355 - accuracy: 0.9482 - val_loss: 0.4272 - val_accuracy: 0.9412\n",
      "Epoch 193/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1354 - accuracy: 0.9478 - val_loss: 0.4465 - val_accuracy: 0.9410\n",
      "Epoch 194/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1342 - accuracy: 0.9477 - val_loss: 0.4358 - val_accuracy: 0.9415\n",
      "Epoch 195/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1326 - accuracy: 0.9478 - val_loss: 0.4253 - val_accuracy: 0.9421\n",
      "Epoch 196/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1326 - accuracy: 0.9479 - val_loss: 0.4395 - val_accuracy: 0.9422\n",
      "Epoch 197/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1329 - accuracy: 0.9478 - val_loss: 0.4561 - val_accuracy: 0.9417\n",
      "Epoch 198/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1326 - accuracy: 0.9476 - val_loss: 0.4466 - val_accuracy: 0.9420\n",
      "Epoch 199/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1316 - accuracy: 0.9480 - val_loss: 0.4226 - val_accuracy: 0.9421\n",
      "Epoch 200/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1315 - accuracy: 0.9479 - val_loss: 0.4319 - val_accuracy: 0.9422\n",
      "Epoch 201/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1301 - accuracy: 0.9477 - val_loss: 0.4562 - val_accuracy: 0.9416\n",
      "Epoch 202/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1327 - accuracy: 0.9476 - val_loss: 0.4652 - val_accuracy: 0.9412\n",
      "Epoch 203/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1308 - accuracy: 0.9475 - val_loss: 0.4507 - val_accuracy: 0.9415\n",
      "Epoch 204/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1288 - accuracy: 0.9478 - val_loss: 0.4377 - val_accuracy: 0.9423\n",
      "Epoch 205/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1282 - accuracy: 0.9481 - val_loss: 0.4309 - val_accuracy: 0.9426\n",
      "Epoch 206/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1288 - accuracy: 0.9479 - val_loss: 0.4462 - val_accuracy: 0.9423\n",
      "Epoch 207/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1293 - accuracy: 0.9479 - val_loss: 0.4565 - val_accuracy: 0.9416\n",
      "Epoch 208/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1285 - accuracy: 0.9476 - val_loss: 0.4633 - val_accuracy: 0.9411\n",
      "Epoch 209/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1275 - accuracy: 0.9475 - val_loss: 0.4600 - val_accuracy: 0.9413\n",
      "Epoch 210/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1275 - accuracy: 0.9477 - val_loss: 0.4234 - val_accuracy: 0.9423\n",
      "Epoch 211/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1320 - accuracy: 0.9463 - val_loss: 0.3931 - val_accuracy: 0.9406\n",
      "Epoch 212/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1363 - accuracy: 0.9440 - val_loss: 0.4181 - val_accuracy: 0.9418\n",
      "Epoch 213/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1307 - accuracy: 0.9463 - val_loss: 0.4478 - val_accuracy: 0.9428\n",
      "Epoch 214/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1273 - accuracy: 0.9479 - val_loss: 0.4551 - val_accuracy: 0.9417\n",
      "Epoch 215/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1273 - accuracy: 0.9476 - val_loss: 0.4271 - val_accuracy: 0.9412\n",
      "Epoch 216/250\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.1273 - accuracy: 0.9476 - val_loss: 0.4372 - val_accuracy: 0.9410\n",
      "Epoch 217/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1267 - accuracy: 0.9479 - val_loss: 0.4601 - val_accuracy: 0.9410\n",
      "Epoch 218/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1409 - accuracy: 0.9485 - val_loss: 0.3744 - val_accuracy: 0.9418\n",
      "Epoch 219/250\n",
      "9/9 [==============================] - 2s 202ms/step - loss: 0.1735 - accuracy: 0.9488 - val_loss: 0.3907 - val_accuracy: 0.9417\n",
      "Epoch 220/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1519 - accuracy: 0.9491 - val_loss: 0.4231 - val_accuracy: 0.9418\n",
      "Epoch 221/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1329 - accuracy: 0.9485 - val_loss: 0.4604 - val_accuracy: 0.9419\n",
      "Epoch 222/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1305 - accuracy: 0.9484 - val_loss: 0.4767 - val_accuracy: 0.9418\n",
      "Epoch 223/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1325 - accuracy: 0.9484 - val_loss: 0.4710 - val_accuracy: 0.9416\n",
      "Epoch 224/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1253 - accuracy: 0.9484 - val_loss: 0.4481 - val_accuracy: 0.9418\n",
      "Epoch 225/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1243 - accuracy: 0.9485 - val_loss: 0.4448 - val_accuracy: 0.9418\n",
      "Epoch 226/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1252 - accuracy: 0.9484 - val_loss: 0.4125 - val_accuracy: 0.9423\n",
      "Epoch 227/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1294 - accuracy: 0.9487 - val_loss: 0.4038 - val_accuracy: 0.9408\n",
      "Epoch 228/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1281 - accuracy: 0.9482 - val_loss: 0.4341 - val_accuracy: 0.9382\n",
      "Epoch 229/250\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 0.1268 - accuracy: 0.9470 - val_loss: 0.4508 - val_accuracy: 0.9388\n",
      "Epoch 230/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1241 - accuracy: 0.9476 - val_loss: 0.4600 - val_accuracy: 0.9401\n",
      "Epoch 231/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1235 - accuracy: 0.9482 - val_loss: 0.4559 - val_accuracy: 0.9408\n",
      "Epoch 232/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1228 - accuracy: 0.9488 - val_loss: 0.4555 - val_accuracy: 0.9412\n",
      "Epoch 233/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1213 - accuracy: 0.9485 - val_loss: 0.4614 - val_accuracy: 0.9414\n",
      "Epoch 234/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1211 - accuracy: 0.9488 - val_loss: 0.4665 - val_accuracy: 0.9413\n",
      "Epoch 235/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1215 - accuracy: 0.9483 - val_loss: 0.4612 - val_accuracy: 0.9413\n",
      "Epoch 236/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1209 - accuracy: 0.9487 - val_loss: 0.4195 - val_accuracy: 0.9420\n",
      "Epoch 237/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1239 - accuracy: 0.9492 - val_loss: 0.4067 - val_accuracy: 0.9418\n",
      "Epoch 238/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1251 - accuracy: 0.9486 - val_loss: 0.4363 - val_accuracy: 0.9413\n",
      "Epoch 239/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1224 - accuracy: 0.9484 - val_loss: 0.4654 - val_accuracy: 0.9411\n",
      "Epoch 240/250\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1243 - accuracy: 0.9485 - val_loss: 0.3846 - val_accuracy: 0.9421\n",
      "Epoch 241/250\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.1342 - accuracy: 0.9486 - val_loss: 0.3944 - val_accuracy: 0.9423\n",
      "Epoch 242/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1253 - accuracy: 0.9489 - val_loss: 0.4440 - val_accuracy: 0.9421\n",
      "Epoch 243/250\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.1217 - accuracy: 0.9489 - val_loss: 0.4369 - val_accuracy: 0.9418\n",
      "Epoch 244/250\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1236 - accuracy: 0.9491 - val_loss: 0.4352 - val_accuracy: 0.9415\n",
      "Epoch 245/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1224 - accuracy: 0.9487 - val_loss: 0.4539 - val_accuracy: 0.9415\n",
      "Epoch 246/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1203 - accuracy: 0.9493 - val_loss: 0.4700 - val_accuracy: 0.9416\n",
      "Epoch 247/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1205 - accuracy: 0.9493 - val_loss: 0.4738 - val_accuracy: 0.9419\n",
      "Epoch 248/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1205 - accuracy: 0.9497 - val_loss: 0.4152 - val_accuracy: 0.9424\n",
      "Epoch 249/250\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.1295 - accuracy: 0.9499 - val_loss: 0.3794 - val_accuracy: 0.9425\n",
      "Epoch 250/250\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.1306 - accuracy: 0.9493 - val_loss: 0.4190 - val_accuracy: 0.9422\n"
     ]
    }
   ],
   "source": [
    "train_lab2_cat = np.array([to_categorical(c, num_classes=cf['n_tags']) for c in train_lab2])\n",
    "history = train_model(train_seq2, train_lab2_cat, model_bilstm_lstm, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.33094025, 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.62217945, 0.        , 0.        ]],\n",
       "\n",
       "       [[0.04379983, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.09287409, 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.39219385, 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.24337988, 0.02881631, 0.08813117]],\n",
       "\n",
       "       [[0.02116665, 0.06577511, 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.5775164 , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.21581925, 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.6224925 , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.24337988, 0.02881631, 0.08813117]],\n",
       "\n",
       "       [[0.02116665, 0.06577511, 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.13682748, 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_str = 'banach spaces are defined as complete vector space of some king'\n",
    "sample_tok = word_tok.texts_to_sequences([sample_str])\n",
    "#sample_pad = pad_sequences(sample_tok, maxlen=cf['input_length'], padding=cfg['padding'])\n",
    "sample_pad = train_seq2[N]\n",
    "pred = model_bilstm_lstm.predict(sample_pad)\n",
    "#np.argmax(pred.squeeze(), axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   35,\n",
       "       5756,   65, 8216,  266, 1309,   18,   19, 7531, 2699, 4470,   20,\n",
       "          2, 1613,    3,    2, 9206,   18,   42,    9, 1005,    4, 8590,\n",
       "        100,  501,  200,   16,  166,   20, 2258, 1416,   65], dtype=int32)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq2[N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]], dtype=float32)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = train_seq2[N].reshape(1,42)\n",
    "model_bilstm_lstm.predict(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                   ***: 0 0\n",
      "                 there: 1 1\n",
      "                   are: 1 1\n",
      "                   two: 1 1\n",
      "            variations: 1 1\n",
      "                    of: 1 1\n",
      "                   the: 1 1\n",
      "                  zero: 2 2\n",
      "               divisor: 3 2\n",
      "                 graph: 3 1\n",
      "              commonly: 1 1\n",
      "                  used: 1 1\n"
     ]
    }
   ],
   "source": [
    "def decoder(T, L):\n",
    "    pred = model_bilstm_lstm.predict(T)\n",
    "    P = np.argmax(pred.squeeze(), axis=1)\n",
    "    for ind, t in enumerate(T):\n",
    "        if True: #t != 0:\n",
    "            print(\"{0:>22}: {1:} {2:}\".format(rev_word_index[t], L[ind], P[ind]))\n",
    "#decoder(sample_pad[0], np.argmax(pred.squeeze(), axis=1))\n",
    "N = -52\n",
    "decoder(train_seq2[N], train_lab2[N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2TklEQVR4nO3dd5xU9bn48c8zM1vYXdrCUmTpghTpq6JGQdAES8QSRGOMei0xUWP5eQ2WqDGW3MQUTUwi3otK1BCD0ajX2CjiDbZFFJUiKCCdFdiFZdkyM8/vj++Z3dk+C4uzc3jer9e85sxp8/3OnHnO9/ucMqKqGGOM8a9AsgtgjDHm4LJAb4wxPmeB3hhjfM4CvTHG+JwFemOM8blQsgtQV9euXbVfv37JLoYxxqSUJUuWfKWqeQ1Na3OBvl+/fhQWFia7GMYYk1JEZH1j0yx1Y4wxPmeB3hhjfM4CvTHG+JwFemOM8TkL9MYY43PNBnoRmSUi20Xkk0ami4g8JCJrRGSZiIyNm3axiKz2Hhe3ZsGNMcYkJpEW/ePAlCamnwoM8h5XAn8CEJFc4E7gGOBo4E4R6XwghTXGGNNyzZ5Hr6qLRKRfE7NMBWaru9/xOyLSSUR6AhOB11V1J4CIvI7bYfz1gEvdiMpwlPSQZaNaUySqVIaj7Kmookt2BsGAUBmOsm13OeVVEcoqI1RFokSiSkSVaBT3rEo0qkSibjgShagqsZtiCyACgnjPICLeVDeXqhtyz25dAOnBAAO75TC4e/sGy1wRjrCjtBIRKK+KUhmOMqhbDoGAVM8TjkQp2VfF7vIweyvCpAUDHNGj4fVFosrufVVURaKUVUYIBoRdZZWUVoSJRiEUFMIRJSCQmR5kTO9OiAgl+6rYUVrBlpJyoqqkBQN0yU6nvMq9d8m+KvZWhEFgSI/29Ouazepte9hRWkkkqpRWhBER97mpErujeOwzjH0uUaV6etR7rv85u89XJP5zjltR/Esan1z3tuZNzdvQ9G8c3pXsjCArt+xhX1WEcDRKVVipikapCkddXeq8T7SBuieipXdgH9LTff/7KiN8c1h3ggH3HW7YuY8Nu8rYXLyPinAUgMO75fCt4T1QVVZt28MXRXspLqsiEnV1cNt8zXcSrf5uaoaj3vYsIgS876ZHx0zOK+jdsoInoDUumOoFbIh7vdEb19j4ekTkSlxvgD59+uxXIfaUVzHqZ6+RFgyQHgoQDAjd22fSv2s2aaEA/btm8/n2UnaVVbJx1z6yM0KEAkIoKKQFAuwuryIj5JatjCg5GUH65GYREKG4rIovd5axfU95dcDKDAWIqFKyr4rs9BC/mT6apV/u4tPNu9m2u5ydeyury1ZaESYSUXIyQ3TITKM8HGH77gpE3MYYCggZaQHSgwEy0oLec6C6PBmhYPXw3ooIu/dVEVElPRggNzudzSX7qIpEyUoLkZEWoLisiq456WwuLiccjZKZFiQUDFBWEaasMkJZpXtOC7rPKRgQLj2uH9dOHgS4HeZry7fy2L/X8cmmkuqNu31GiJzMEFtKyvfrO2pNfXKzWHTzSdWvyyrDPP3ulyxcVcSS9bvYVxWpNf85Y3rxm+mjeWt1Efe9vJKVW3fXCwRv3DiBw7vloKoUrt/F3MKNfLK5hLVf7aWssvb6mjIwL5uqiPLlzrIDqqNJnlOGdUeA15Zva3B6KCD8YMIAnl+6mU3F+1rtfUf37tRmA/0BU9WZwEyAgoKC/fonlGgUrp00iK0l5VSEI3Rsl8Y7X+zklU+3Vs/TNSeD3rntGJHfkdLyMJXhKIEAlFZEyO+cRWUkSmU4Qkhh194qVmzZVh2wxw/IZULPPIKBAMEA1T/8ju3SeOzf67h41nsADOqWQ4+OmQzMy0FwLZCcjBARVfZWhCmvipARCtI1J4NYAzOiSkXYtTzdc4SKcJSKqmh16y82LSMUoHNWOhnBAHsrwmzYVUbnrHQ6ZKaxryrCntIw7TNCrN9RxkAvaFVGFAEO65hJVnqIrPQg7dKDlFaECQWE2W+v5921O7kWKK+KMH3mO3y0oZg+uVlccHQf8tpnUFEV4c9vfsERndpx/lF96NExg8y0INnpoeoda0DE23G4VkrQex2ofo61JrVeaz3W8hHchxJrdMa3+oMBQVW57+WVLFy1nWhUCQSEDTvLmPbnt9m6u5yhPTtw1pjDGJXfib2VEdpnhHh37U6e/WAjR/XP5afPf0Lv3CyunTSILtnptM8MsXV3Ob98ZRVbSvYxoGs2V8wuZN7K7WSlBxnWswPTxuXTt0s26aEAWelBwhGlc3Y62RlBgiJURZRQ0BX4e//9Lp8X7WVAXjbXnzyIPrlZ9OzYjlBQqApH2b6nguyMEB3bpdGhXYicjBBVEaVw3U5K9lXRrUMmPTpkkhFyO3FVCARirb6a7T32OcU+01irPVA97Eaoxn/WrkcV/zlXr69OI7/Oy1q9gPrT6i5bq6DV9laE+dWrq+iclc6pI3rQqV0aacEAoaCQHtfoiNVHNa5+8dtEvRI0rm7ZGrOvMsLry7fRoV0aVz25hNe9AH/Jcf0YP6ALvXPbkd8pi4y0AMu37OacPy7m4QWf07dLFjdPOYITB+XRJSedtGCAgNRs6wFvu419LwGv9V7zW6jdGztYWiPQbwLid0H53rhNuPRN/PiFrfB+DeqYlcYNpwyuNS4aVXbsrWTV1j2M7N2R9hmh+t3WZny4oZioKmP7NH544cWPNvNVaSU/OHEAt5w2dL/Kn0xf7ixjR6nbof3iXyv5aEMxD0wbxdljehGMiy7XTBrUJlJjJwzqyvyV2yneV0Vudjr3/2sFeyvC/P2qYzmqX269+dNCwrMfbOSWf3zMkB7t+ftVx9I+M616+qqte/jlK6so2VfFk++uZ97K7dx4ymAu+0Z/sjNa9hP59XmjeHzxOp687JgWLdu/a3aL3icVdWyXxm+nj052MRqUmRbkvKNcGPv5WUfy4Bur+f6xfbl20uH1YsaY3p244eTBbCnZx71nj6j1G9kfsR1boAU7sJZqjUD/AnCNiMzBHXgtUdUtIvIqcF/cAdhvAre0wvslLBAQ8tpnkNc+Y7/XMbp3p2bn+fV5oylct5Mb6+xoUkXnrHQ+LyqlvCrCM4Ub+M64fL4zLr/efG0hyAPV3+f2PeUERXjt021ccly/BoM8uAATc8Mpg2sFeYBOWe7159v38uhbX3DCoK4N/sATMXV0L6aObjBDaVLEReP7ctH4vo1OFxGuO3nQ11iiA9dsoBeRv+Ja5l1FZCPuTJo0AFX9M/AycBqwBigDLvWm7RSRnwPve6u6O3Zg1m8mDM5jwuAGbxqXEjq2S6N4bxXvrd1JWWWEM0b2THaRmtStfSYA23dXsGnXPsJR5eRh3RudPz7QF/St3zOLTX9x2WZKK8L8ZMqQ/QryxrRViZx1c0Ez0xW4upFps4BZ+1c083XpnJXOnoown27eDcCo/E7JLVAzunkt+m27y1n71V7SgtJkmeMDfW52er3pmWnuYPea7aVkhAIMaeTsG2NSVdvoi5uk6pztAuGS9bvokp1O5waCYVvSs1MmIrBx1z7WfrWX3rlZtEsPNjp/h7hA31hLPZa+OaJHe0JB+1kYf7Et2lS3eD/4chcD83KSXJrmZYSC9OyQyYadZWwq3kevTu2anD++Rd+Y2PGHoxvJ8xuTyizQG3K8s0N27q2kZ6fMJJcmMX26ZLF+Zxmbdu0jv3PTgT4j1HhrP+Ybh7tjLCcN6dYq5TOmLWkT59Gb5IpPezSUw26L8jtn8caKbRSXVTXbogeYceqQJs+guuesIzlnbK8GD9Yak+os0BvapdUE+i4pEug7tUujuKwKgMO7NX/w9KoJA5ucHgxIo6dnGpPqLHVjyEqv2d/nZu//NQdfp/gDrEf1s1a4MU2xQG9qtehTJXXTIdPtnLLSg3TJSY2dkzHJYoHekJlesxl0yUmNQB+7ujWjjVyta0xbZr8SUyd1kxqBPpa6aSu3ZTCmLbNfiUnJg7HtvdSNBXpjmme/EkMwINW3Gu6Q2fzFRW1B7Nz/RM6RN+ZQZ6dXGsC16tOCgVr/wtSWpXm3KejZMTUu8DImmSzQG8CdvZIqrXmAwd1zuP30oXZLYGMSYIHeAK5FnyoHYsHdnOzyEwYkuxjGpAQL9AaASUO60cPSIMb4kgV6A8DtZwxLdhGMMQeJnXVjjDE+Z4HeGGN8zgK9Mcb4nAV6Y4zxOQv0xhjjcxbojTHG5xIK9CIyRURWicgaEZnRwPS+IjJPRJaJyEIRyY+b9l8i8on3mN6ahTfGGNO8ZgO9iASBh4FTgWHABSJS96TrB4DZqjoSuBu431v2dGAsMBo4BrhJRDq0WumNMcY0K5EW/dHAGlX9QlUrgTnA1DrzDAPme8ML4qYPAxapalhV9wLLgCkHXmxjjDGJSiTQ9wI2xL3e6I2L9xFwjjd8NtBeRLp446eISJaIdAVOAnrXfQMRuVJECkWksKioqKV1MMYY04TWOhh7EzBBRJYCE4BNQERVXwNeBhYDfwXeBiJ1F1bVmapaoKoFeXl5rVQkY4wxkFig30TtVni+N66aqm5W1XNUdQxwmzeu2Hu+V1VHq+opgACftUbBjTHGJCaRQP8+MEhE+otIOnA+8EL8DCLSVURi67oFmOWND3opHERkJDASeK21Cm+MMaZ5zd69UlXDInIN8CoQBGap6qcicjdQqKovABOB+0VEgUXA1d7iacBbIgKwG/ieqoZbvxrGGGMaI6qa7DLUUlBQoIWFhckuhjHGpBQRWaKqBQ1NsytjjTHG5yzQG2OMz1mgN8YYn7NAb4wxPmeB3hhjfM4CvTHG+JwFemOM8TkL9MYY43MW6I0xxucs0BtjjM9ZoDfGGJ+zQG+MMT5ngd4YY3zOAr0xxvicBXpjjPE5C/TGGONzFuiNMcbnLNAbY4zPWaA3xhifs0BvjDE+Z4HeGGN8zgK9Mcb4XEKBXkSmiMgqEVkjIjMamN5XROaJyDIRWSgi+XHTfikin4rIChF5SESkNStgjDGmac0GehEJAg8DpwLDgAtEZFid2R4AZqvqSOBu4H5v2eOA44GRwJHAUcCEViu9McaYZiXSoj8aWKOqX6hqJTAHmFpnnmHAfG94Qdx0BTKBdCADSAO2HWihjTHGJC6RQN8L2BD3eqM3Lt5HwDne8NlAexHpoqpv4wL/Fu/xqqquqPsGInKliBSKSGFRUVFL62CMMaYJrXUw9iZggogsxaVmNgERETkcGArk43YOk0TkhLoLq+pMVS1Q1YK8vLxWKpIxxhiAUALzbAJ6x73O98ZVU9XNeC16EckBzlXVYhG5AnhHVUu9af8CjgXeaoWyG2OMSUAiLfr3gUEi0l9E0oHzgRfiZxCRriISW9ctwCxv+EtcSz8kImm41n691I0xxpiDp9lAr6ph4BrgVVyQfkZVPxWRu0XkTG+2icAqEfkM6A7c642fC3wOfIzL43+kqi+2bhWMMcY0RVQ12WWopaCgQAsLC5NdDGOMSSkiskRVCxqaZlfGGmOMz1mgN8YYn7NAb4wxPmeB3hhjfM4CvTHG+JwFemOM8TkL9MYY43MW6I0xxucs0BtjjM9ZoDfGGJ+zQG+MMT5ngd4YY3zOAr0xxvicBXpjjPE5C/TGGONzFuiNMcbnLNAbY4zPWaA3xhifs0BvjDE+Z4HeGGN8zgK9Mcb4nAV6Y4zxOQv0xhjjcwkFehGZIiKrRGSNiMxoYHpfEZknIstEZKGI5HvjTxKRD+Me5SJyVivXwRhjTBOaDfQiEgQeBk4FhgEXiMiwOrM9AMxW1ZHA3cD9AKq6QFVHq+poYBJQBrzWesU3xhjTnERa9EcDa1T1C1WtBOYAU+vMMwyY7w0vaGA6wHeAf6lq2f4W1hhjTMslEuh7ARviXm/0xsX7CDjHGz4baC8iXerMcz7w14beQESuFJFCESksKipKoEjGGGMS1VoHY28CJojIUmACsAmIxCaKSE9gBPBqQwur6kxVLVDVgry8vFYqkjHGGIBQAvNsAnrHvc73xlVT1c14LXoRyQHOVdXiuFnOA55T1aoDKq0xxpgWS6RF/z4wSET6i0g6LgXzQvwMItJVRGLrugWYVWcdF9BI2sYYY8zB1WygV9UwcA0u7bICeEZVPxWRu0XkTG+2icAqEfkM6A7cG1teRPrhegRvtm7RjTHGJEJUNdllqKWgoEALCwuTXQxjjEkpIrJEVQsammZXxhpjjM9ZoDfGGJ+zQG+MMT5ngd4YY3zOAr0xxvicBXpjjPE5C/TGGONzFuiNMcbnLNAbY4zPWaA3xhifs0BvjDE+Z4HeGGN8zgK9Mcb4nAV6Y4zxOQv0xhjjcxbojTHG5yzQG2OMz1mgN8YYn7NAb4wxPmeB3hhjfM4CvTHG+JwFemOM8bmEAr2ITBGRVSKyRkRmNDC9r4jME5FlIrJQRPLjpvURkddEZIWILBeRfq1YfmOMMc1oNtCLSBB4GDgVGAZcICLD6sz2ADBbVUcCdwP3x02bDfxKVYcCRwPbW6PgxhhjEpNIi/5oYI2qfqGqlcAcYGqdeYYB873hBbHp3g4hpKqvA6hqqaqWtUrJjTHGJCSRQN8L2BD3eqM3Lt5HwDne8NlAexHpAgwGikXkHyKyVER+5fUQahGRK0WkUEQKi4qKWl4LY4wxjWqtg7E3ARNEZCkwAdgERIAQcII3/ShgAHBJ3YVVdaaqFqhqQV5eXisVyRhjDCQW6DcBveNe53vjqqnqZlU9R1XHALd544pxrf8PvbRPGHgeGNsK5TbGGJOgRAL9+8AgEekvIunA+cAL8TOISFcRia3rFmBW3LKdRCTWTJ8ELD/wYhtjjElUs4Hea4lfA7wKrACeUdVPReRuETnTm20isEpEPgO6A/d6y0ZwaZt5IvIxIMCjrV4LY4wxjRJVTXYZaikoKNDCwsJkF8MYY1KKiCxR1YKGptmVscYY43MW6I0xxucs0BtjjM9ZoDfGGJ+zQG+MMT5ngd4YY3zOAr0xxvicBXpjjPE5C/TGGONzFuiNMcbnLNAbY4zPWaA3xhifs0BvjDE+Z4HeGGN8zgK9Mcb4nAV6Y4zxOQv0xhjjcxbojTHG5yzQm9T1wWwo25nsUphD0a718NlryS5FwizQGycagfVvJ7sUidv6CbxwLfzz6mSXxByK/nQ8PD0t2aVImAV64yx5DB6bAqv+leySJKZyr3ve+1Vyy2EOTZV7kl2CFrFAb5xYwNzwbnLLkaho2D0HQi1bbvdmqGiDP9LSIvjb92DfrmSX5OCp2APhimSXonVFwskuQUISCvQiMkVEVonIGhGZ0cD0viIyT0SWichCEcmPmxYRkQ+9xwutWfg246M58PJ/JrsUB6bDYe65eENyy5Go6kAfTHyZ4g3wm6Hw4vUHpUj77dHJ8MDhsOJFeO+/k12alln+Ajx/Nag2P+/9+TB7Krx2Oyx98uCXraUq98JrP21ZL3Hezw5eeVpRs4FeRILAw8CpwDDgAhEZVme2B4DZqjoSuBu4P27aPlUd7T3ObKVyty3P/QDem5nsUhwYjbrnEp8G+n8/CK94bZTPXjk4ZQLY8TlUlbdsmU2FNcNVe1u3PAdTpAqeuQg+fNLVuynhSvf85duw+Pdt89jKZ6/A4ofg2csTX2bxQynROEqkRX80sEZVv1DVSmAOMLXOPMOA+d7wggam+1ekqmY4Gk1eOQ5U7Ie44V145ZbkliURLU3dvH4HrHzJDUcqD06XO1wJvx/rgl+i6raEq/bB1o/hy3dat2wHw84vaoZXNNNZT4UGxJZl7nndW7B7S+LLfbHwoBSnNSUS6HsB8d/SRm9cvI+Ac7zhs4H2ItLFe50pIoUi8o6InNXQG4jIld48hUVFRYmXPhHbV0Lp9gNbx5InoPCxhqcVf1kz/Ootzbds2qpIZc3wO3+Eos++vvfe+nHtoJGIqn3uec0b8PmCli0fqXR1/P04eGgsrHoFyne7x/x7Wt4ij9nrbburmzjtLhKGTR/UvK57vKC8xKU3Zn0Llv+z/vK71sFbv4Ztn7rXqu40v+VxgVbVTd+2vOb1gdjxOTw4Gp74Nsy5ENa+5Z2l9W83PaOj69E2lX/fubb+uMqyAytXa9v2iXuOhuE3Q+CjvyW23Fdf429lP7XwSFajbgL+ICKXAIuATUDEm9ZXVTeJyABgvoh8rKq1oqGqzgRmAhQUFBzgVhknGoE/HuOGf/oVBNNavo6STfDij91wwaX1p5duqxl+98+w+nX48Qf152sLPl8AXQZCpz71p0Xq/Ej/+SM48jsw/qqDX64/f8M931WS+DKxQA/wl7Pc83XLoHPf5pcNpMHrP615/dfpkNkJ+ox33fduw+DIcxpdvFHx20K4AkIZ9ed58xew6FfwH6+6gLJ9Re3paxdB2Q43/OL1LmBv+QjSs6FzP9fg2LcTFj0Ag6fAlg9rdnJXvwf7it2OYPWrblyXQS73HK2CI06D8T+EvCHw+Xy3XKe+MHASLPsb7FgNuQNdL6nXWGiXC+8/6rbr8hLYtRaC6TU9o5izHnYHkpc9A2Mb6c188Hj9cTvWQM+RLiceqQQJwGevukfpNjjz925n8vHf4fjrYMjpbrlI2NUnrV39dUYjsOplCGZA/xOhaCWUfeXeY88W95l1G1p7GVXXYNv7FXQdXBO4C2fBqOm1591XDBkdao/bvdntDLcvh6Hfbrj+SZZIoN8E9I57ne+Nq6aqm/Fa9CKSA5yrqsXetE3e8xcishAYA3w9zd7K0prh4i9dkGupopU1w6VFkJNXe/qeOl28nZ9DyUZ47DTIaA+n/hL6Hd/y923KuzPdj/Rb90EgwROnolEXEEOZcPs216rMynXBA2pSNzEb33ePpX+BHiOg1zgo+A+XE6/c6x453dy8Oz6H3ZvcD6uuSBieOhdyusOUX7j3VAURN718d828VeWQlplYfcL76o8r/B845e7ml518B/zfb+E7s1y93n8U5t1dk7uvLIWNha6OZTug/wRY/CDs2Qan/9qdGROtgg+fdttHaZELVKMuqHmP9f+G3uMhPcu9jkZd7yN2LGfh/fDFm0Cdds1u76c19Y/w0g3w5n9BTg/vM9/j1jlxhjsIuH6xK/+I8+D/fgN/PBY04oLcyXe555X/C6F0yOwInzzrLjLr1AeK1yf2OQMMmAgTb4UeR7pU5SdzXU85Pds1noac4baR+T93x3oq97pgm90VTvu1G7fipfrr3fYJrHkdFv6ido8ypweUboXHT3OfdbvOricx6Tb3WX8wG7Lz4MdL4cvFbsc2YYbbpl69rfYxj7reuAsKLoNR58PaN91nuG25ez+AwafWBPoN77gd6nHXutcv/BiWzYHsbrXX+clc9wA4689wxJSadE63YZB3RO35w5Wu7h0Oc3ULZbjfRNFKV9++xyXwpbSMaDPdOhEJAZ8Bk3EB/n3gu6r6adw8XYGdqhoVkXuBiKreISKdgTJVrfDmeRuYqqrLG3u/goICLSxs4otqid2b3VkWACfd7losvY+Gw09JrOUH8Olz8PdL3PDpv4FhZ0F2F9fd37XWbcSv3lp/ufQcF9TKd8OFc6HnKPejiAW4kk1u2Q693HMwwc7VrnXw4Cg3fOov4ZgfJLZc8ZfwuxFuuGPvmpzpBXPgiFPhjZ+5YBFv5PnuMyvZ5H4IfY6DqX+AOd91VwYOP8v9YP5ytqvDwMlw9p9dPTM7uatWX7sNPvqrW18oE9r3cOmKs/7sAsGaN2DBvW769551Lcc+x0GHnvXrEAnD+//tWt4vXONSPvFyB8CxV8OaeS4QTL7D7XTbdar5zMZc5OoQv7OJRuDu3Jr19D/Rtawb0v4w2LO59rhYYKoroyPc8LG7NuGjOfDFAleuTn1dMAqEXACOteDj3bbNtdx3fA59joWqMre9dR/R8M79q9WuBdp1MAyb6ra9uvZsg/+90X3GQ86A4WfD5g/cDiuzAxx/vespRSpdT6Jqn2v99j+x5rNqzOo34Onz3I4GIKuL+56D6a53E61yAbbwf9z0tCwX5HZvgkHfgr7Hut/K8LPdTuOLhfDmL932MvVh+McVNT2JUDu3o7/gb+5st5Iv3Q5v23K3Yz3hJtcIWf2a21ba5bpejAi8/Qe3o4jpPsIF3FgP6OgfwHuP1K9fKBPC5a6x88XC+qnCzI7uEZ/Kjcnp7noCae3cNlq0qvZB97QsN61sB3Q/En7476Y/60aIyBJVLWhwWnOB3lvBacDvgCAwS1XvFZG7gUJVfUFEvoM700ZxqZurveB+HPAIEMUdD/idqv5PU+91QIF+xYtw+Mk1Xbqiz+DhoxqoUABGfxeOuw7yBje9ziVP1KRuALoNhx+8CT/v2vD8gZDrkk97wu1MHp1cs/FndoLxP3J7+H9cWZMuyewEp/wMxl3SfB3XvAFPnuvSDznd4IZPG/8Rhitdaw5cV/0vZ9e8X3qO23C7Dnbd9A+fcq2Jsd+H/KPdD+bIc938qq4FPO9ngFCvFQrux7r2TbdOcBtsyUbXujvqMhjzPZf/jobdj6GpvGa7XOg+3AWK3Zthyv2unP/7/2Dje/Xnn/6Ue5/nrmz6s5t0O5zYyGmwz17uAtz6t2Gvd0znuGuhc3/Xsxn7fRcgPnsFxl/tWrMZ7V3QPO4aWPqUS3fVlTvQ9fLApR9Out31GD54AkZOh/Y9XUB9/HQXzGM9yJaksdqKkk3ue9i73X1/X61x9czu6raH9Gz46/lu3jEXwaYlbicy+Q43rSnRiDtm0b4HHDYW7jsMDhvt1hG/o73uo5peakNU3Zk/ZTug9zE1vdKnprkdw4QZLr0GkH+U++5jJsyAk26BBfe5nla8vt9wDZVXb3E7ttHfdbFg81JXxnad3Weza737bHqNddt3LCVWVQ4DJrjeU0Op1QQ0FegTakaq6svAy3XG3RE3PBeY28Byi4ERLSrt/ir6DJ75vvtyOveHjJz6XaAJP3HB6183u3zismdc4B11fv28XUyFl1qY/qTLY654sX6Qb5frWl+BEFzzvttjH3Gqm3blAtd91qhrcSy8z43vNswdJM7p7sr6L+/Uv71Fbq8/dKqXMx1QO9dbstE9H3eta4Hv/KLhlNQrt7jeyNXvupbGV6vd+BtXutZeLE/99h9c9xfcxnjm7+uvSwROuNHtCBY/5HaUt211+eXFD7kgP2o6bP7QtQ6zcuHdR6C8uHav47vewa29O2D5867u5cWQN9T9cFa/CgNOcq3J4vUuCO7dDs9eVlOW7Dz3Q9i0pGZcKMPlb8PlLqgccZqr27p/u/d+znv/QBPHaM71zl9/ZIJ7z7yh8M173LijvPfvfYwLFA31vkZ/1y0XrnTBLZaC2em1yHsfAyfd5na8oVz4xg01y6Znue0kEoaXrnP1TkUdvXM0Yo2n3ke5R0x8D2zqH1q27kCw9nGTLgPdNpCeA5e/Dv+8xqXOmgry4LblhlIj7Tp7z51qxl32uttO+xznAnKsXhntay/bY4TbRtIy4Yzf1p7WZ3wClTv4WutgbPLlDYYz/+B+4LGrO7/0no88152udsxVLgh9/58uyM79D/j379xj4GS3IY36bu2ucXmJC2xHnO5+sOsXu73vsDNdiiKnhwuCvxsJpz/gAnPugJrle45yD3BBYtkzrqs5/mq33mC6y/M/fR68eF39erXrDJf8rztDYelf3MYtAVen//uNq1cg5HJ+HfPdTiejvTurBOC3I2DgRNcaapfrWkSxHsDQb7tAHxNs4OBhvIGTXGAfPMUF18NGuzx3zGGj3QNcC/7Ld2p6BfGyu9QEz5jpf3HfSafetdMqlWUu31u2A4af4z6PYAju6hhX7nQ3/7iLa8bFgjS4zzVcntipmL3GuQOcAybUn9bUOfsiNcG74FL3PW37BD59Hs5/uuFUSl3BkEtT+FWHuifrHYC8Ia5XeMRpbsd/8QFei5npbU/RSM04EZdKAmjfvWZ8/MHYtGy46v8O7L2/Bv4J9ABjLnSPTR/AS9e7sxXAtX7jAxK4Ltv5T7nW9u5N8P4sdxHHtk9dAMwb4g4SvfeI2wgCAddavKbQBY3YlaQx/7m6+fKJ1D+KD64ldMUCl5bpOth19z54wnXvljwB//iB2zmUx3Xnuw1zG9l7j7iURuzgZDAdDhvjhnuNc2mSlV5nLHdA7TRPn/Fw+TyXCiovdss2pf8E12JpKHjXq1M+jPhO8/PFhDJckIfaZUzPan49DZ3dUou3vkTOujrhRtebOO7Hzc/bmJxurkd3xKmNp4oORbFWc2sYdb7bZk/4f62zvhwvkEfD7vhFU+f9Z8YFekmNu8j4K9DH9BoL/U6oCfTpOQ3Pl9nRdbnBHcB54VrXEo61hmPil0+kZbY/Qukw5DQ33PVwGOndGS+9PSy4x6Udrn7f9VbSs9yOp/twl7Pu3A9O/plL+6yZB595Nyb77jNu5wTw8dyankW8/IKabnComUAfCLiDUW1B3pCafHZzgT6240jkKtqO+S7XalqfiOsBDzzpwNc15PSa0y1bw/gfutTkUZfDN65vet741E1zB6nbCH8Geqid52ws0McTcXnDI89xOeTVr7mDNiUbavKtyXDsj1wrY/A3XXoq/uDxEae64wGn/9odhAY4+gp3vvz6xTVBHppuFYsXAJtL3bQlV8x3B+QggXLHAv1+XEdhWtdNq5JdgoalZ8O37k1s3oy4tKEF+iTLjjvfPSOBQB8zcJJ7HjnNHT2/p5s7op4s6dnuSH9DTrjRPeoaeFLLWk2x3HVzLfq2JP4sjWZb9F73en8umDOmrti1EWCpm6TL7lIznNbMqVuNCWXArS2450WqiqU0msvRt1XNlbs6dePfzd18jUJxF/VZoE+yrLi0RaJXjzYkfu/tV6ke6ON/eA2yQG9aUfyJGN9+KHnlaAH/bvnx+WnTtFiOvtmzV9qo5lJOsTSqBfr9UlVVxcaNGykv38+bvfnR9Li7i65Y0fh8B0FmZib5+fmkpSWeivTvlh/L0Q/6ZnLLkQpiATBVW/SJHoy1HP1+2bhxI+3bt6dfv35Iihx89CtVZceOHWzcuJH+/fsnvJx/A30oA3707n5fTnxIqU7dpGggbDZH76Xu7Kyb/VJeXm5Bvo0QEbp06UJLb+fu30AP0G1IskuQGmKBPlVTG80dg2nJefSmQRbk2479+S5S45CxObhiOXq/tnjt9EpziLNAb+Jy9Cnaom+WnXVjDm225Zu41E2KtXgvn1fzl3pNEbsy1iQmHA4TCvkvLPqvRqblqlv0KRYI8wvco1mxs25scz9QP3vxU5Zv3t38jC0w7LAO3Pnt4c3Od9ZZZ7FhwwbKy8u57rrruPLKK3nllVe49dZbiUQidO3alXnz5lFaWsq1115LYWEhIsKdd97JueeeS05ODqWl7l/n5s6dy0svvcTjjz/OJZdcQmZmJkuXLuX444/n/PPP57rrrqO8vJx27drx2GOPccQRRxCJRPjJT37CK6+8QiAQ4IorrmD48OE89NBDPP/88wC8/vrr/PGPf+S5555r1c/oQNmWb+LOSvHp5mBXxvrCrFmzyM3NZd++fRx11FFMnTqVK664gkWLFtG/f3927twJwM9//nM6duzIxx+7+9/v2rWr2XVv3LiRxYsXEwwG2b17N2+99RahUIg33niDW2+9lWeffZaZM2eybt06PvzwQ0KhEDt37qRz58786Ec/oqioiLy8PB577DH+4z/ayI3/4tiWb2oCoF8DoZ1e2WoSaXkfLA899FB1S3nDhg3MnDmTE088sfp88txcd2fZN954gzlz5lQv17lz87dHnjZtGsGgS2GWlJRw8cUXs3r1akSEqqqq6vVeddVV1amd2PtddNFFPPnkk1x66aW8/fbbzJ49u+E3SSKf/rJNi6T6efTNstMrU93ChQt54403ePvtt8nKymLixImMHj2alStXJryO+NMS617lm51dcz+sn/70p5x00kk899xzrFu3jokTJza53ksvvZRvf/vbZGZmMm3atDaZ47ezbswhcHqlBfpUV1JSQufOncnKymLlypW88847lJeXs2jRItauXQtQnbo55ZRTePjhmn/qiqVuunfvzooVK4hGo03m0EtKSujVy/0b1uOPP149/pRTTuGRRx4hHA7Xer/DDjuMww47jHvuuYdLL7209SrdiizQmxq+PVhpF/ukuilTphAOhxk6dCgzZsxg/Pjx5OXlMXPmTM455xxGjRrF9Onu39tuv/12du3axZFHHsmoUaNYsGABAL/4xS8444wzOO644+jZs/H/5b355pu55ZZbGDNmTHVQB7j88svp06cPI0eOZNSoUTz99NPV0y688EJ69+7N0KGN/Pd0komqJrsMtRQUFGhhYWGyi3Foefk/4b2ZcPJdtf+02i9+N8L9peKPP4TcxO8PYpwVK1a02QDWVlxzzTWMGTOGyy67rPmZW0FD34mILFHVBk9Dsxa9AY26Z7+mbkZ/zz0frL+BNIe0cePGsWzZMr73ve8luyiNSijQi8gUEVklImtEZEYD0/uKyDwRWSYiC0Ukv870DiKyUUT+0FoFN60o1qvz68HYCTfDbdvcfwQb08qWLFnCokWLyMhou7f5bjbQi0gQeBg4FRgGXCAiw+rM9gAwW1VHAncD99eZ/nNg0YEX1xwUsRZ9ivxbTouJQFpzf05ijH8l8ss+Glijql+oaiUwB5haZ55hwHxveEH8dBEZB3QHXjvw4pqDw2vR+zXQG3OIS+SX3QvYEPd6ozcu3kfAOd7w2UB7EekiIgHg18BNTb2BiFwpIoUiUtjS+yybVhBL3ditaI3xpdZqwt0ETBCRpcAEYBMQAX4EvKyqG5taWFVnqmqBqhbk5eW1UpFM4qxFb4yfJXLi9Cagd9zrfG9cNVXdjNeiF5Ec4FxVLRaRY4ETRORHQA6QLiKlqlrvgK5JoliO3s43N8aXEgn07wODRKQ/LsCfD3w3fgYR6QrsVNUocAswC0BVL4yb5xKgwIJ8GxS7lMJSN8Yn4u9UaRII9KoaFpFrgFeBIDBLVT8VkbuBQlV9AZgI3C8iiju75uqDWGbT6qojfVJLYVLAv2bA1o9bd509RsCpv2jddbYRbeX+9gklZVX1ZVUdrKoDVfVeb9wdXpBHVeeq6iBvnstVtaKBdTyuqte0bvFNq1DL0Zu2bcaMGbXuX3PXXXdxzz33MHnyZMaOHcuIESP45z//mdC6SktLG11u9uzZ1bc4uOiiiwDYtm0bZ599NqNGjWLUqFEsXryYdevWceSRR1Yv98ADD3DXXXcBMHHiRK6//noKCgp48MEHefHFFznmmGMYM2YMJ598Mtu2basux6WXXsqIESMYOXIkzz77LLNmzeL666+vXu+jjz7KDTe0wtXqqtqmHuPGjVPzNXv2StU7O6gufSrZJTFt0PLly5NdBP3ggw/0xBNPrH49dOhQ/fLLL7WkpERVVYuKinTgwIEajUZVVTU7O7vRdVVVVTW43CeffKKDBg3SoqIiVVXdsWOHqqqed955+tvf/lZVVcPhsBYXF+vatWt1+PDh1ev81a9+pXfeeaeqqk6YMEF/+MMfVk/buXNndbkeffRRvfHGG1VV9eabb9brrruu1nx79uzRAQMGaGVlpaqqHnvssbps2bJ6dWjoO8FlWBqMq8nvU5g2wFI3pm0bM2YM27dvZ/PmzRQVFdG5c2d69OjBDTfcwKJFiwgEAmzatIlt27bRo0ePJtelqtx66631lps/fz7Tpk2ja9euQM395ufPn199j/lgMEjHjh2b/TOT2A3WwP2pyfTp09myZQuVlZXV989v7L75kyZN4qWXXmLo0KFUVVUxYsSIFn5a9VmgN3YevUkJ06ZNY+7cuWzdupXp06fz1FNPUVRUxJIlS0hLS6Nfv3717jPfkP1dLl4oFCIajVa/bur+9tdeey033ngjZ555JgsXLqxO8TTm8ssv57777mPIkCGtdttjS8oa7Dx6kwqmT5/OnDlzmDt3LtOmTaOkpIRu3bqRlpbGggULWL9+fULraWy5SZMm8fe//50dO3YANfebnzx5Mn/6058AiEQilJSU0L17d7Zv386OHTuoqKjgpZdeavL9Yve3f+KJJ6rHN3bf/GOOOYYNGzbw9NNPc8EFFyT68TTJftkGgt7NmPz6V4LGF4YPH86ePXvo1asXPXv25MILL6SwsJARI0Ywe/ZshgwZktB6Gltu+PDh3HbbbUyYMIFRo0Zx4403AvDggw+yYMECRowYwbhx41i+fDlpaWnccccdHH300ZxyyilNvvddd93FtGnTGDduXHVaCBq/bz7Aeeedx/HHH5/Q3yAmwu5Hb6BsJ/z7QZj0Ux//+YjZX3Y/+q/fGWecwQ033MDkyZMbnG73ozctl5ULp/zMgrwxSVZcXMzgwYNp165do0F+f9gv2xjjSx9//HH1ufAxGRkZvPvuu0kqUfM6derEZ5991urrtUBvjGmWqiIpdlbWiBEj+PDDD5NdjFa3P+l2S90YY5qUmZnJjh079ivAmNalquzYsYPMzJb9kY616I0xTcrPz2fjxo3Yf0W0DZmZmeTn5zc/YxwL9MaYJqWlpVVfzWlSk6VujDHG5yzQG2OMz1mgN8YYn2tzV8aKSBGQ2E0rGtYV+KqVipNMfqkHWF3aKqtL27S/demrqg3+6XabC/QHSkQKG7sMOJX4pR5gdWmrrC5t08Goi6VujDHG5yzQG2OMz/kx0M9MdgFaiV/qAVaXtsrq0ja1el18l6M3xhhTmx9b9MYYY+JYoDfGGJ/zTaAXkSkiskpE1ojIjGSXpzkiMktEtovIJ3HjckXkdRFZ7T139saLiDzk1W2ZiIxNXsnrE5HeIrJARJaLyKcicp03PqXqIyKZIvKeiHzk1eNn3vj+IvKuV96/iUi6Nz7De73Gm94vqRVogIgERWSpiLzkvU7JuojIOhH5WEQ+FJFCb1xKbV8xItJJROaKyEoRWSEixx7suvgi0ItIEHgYOBUYBlwgIsOSW6pmPQ5MqTNuBjBPVQcB87zX4Oo1yHtcCfzpaypjosLA/1PVYcB44Grv80+1+lQAk1R1FDAamCIi44H/An6rqocDu4DLvPkvA3Z543/rzdfWXAesiHudynU5SVVHx51jnmrbV8yDwCuqOgQYhft+Dm5dVDXlH8CxwKtxr28Bbkl2uRIodz/gk7jXq4Ce3nBPYJU3/AhwQUPztcUH8E/glFSuD5AFfAAcg7tKMVR3WwNeBY71hkPefJLsssfVId8LGpOAlwBJ4bqsA7rWGZdy2xfQEVhb97M92HXxRYse6AVsiHu90RuXarqr6hZveCvQ3RtOmfp5Xf4xwLukYH28VMeHwHbgdeBzoFhVw94s8WWtroc3vQTo8rUWuGm/A24Got7rLqRuXRR4TUSWiMiV3riU276A/kAR8JiXUvtvEcnmINfFL4Hed9TtvlPq3FcRyQGeBa5X1d3x01KlPqoaUdXRuNbw0cCQ5JZo/4jIGcB2VV2S7LK0km+o6lhcKuNqETkxfmKqbF+43tJY4E+qOgbYS02aBjg4dfFLoN8E9I57ne+NSzXbRKQngPe83Rvf5usnImm4IP+Uqv7DG52y9VHVYmABLr3RSURif9ITX9bqenjTOwI7vt6SNup44EwRWQfMwaVvHiQ164KqbvKetwPP4XbCqbh9bQQ2qmrsH8rn4gL/Qa2LXwL9+8Ag74yCdOB84IUkl2l/vABc7A1fjMt1x8Z/3zsCPx4oievmJZ2ICPA/wApV/U3cpJSqj4jkiUgnb7gd7jjDClzA/443W916xOr3HWC+1xpLOlW9RVXzVbUf7vcwX1UvJAXrIiLZItI+Ngx8E/iEFNu+AFR1K7BBRI7wRk0GlnOw65LsgxOteJDjNOAzXE71tmSXJ4Hy/hXYAlTh9vKX4XKi84DVwBtArjev4M4q+hz4GChIdvnr1OUbuK7mMuBD73FaqtUHGAks9erxCXCHN34A8B6wBvg7kOGNz/Rer/GmD0h2HRqp10TgpVSti1fmj7zHp7Hfd6ptX3H1GQ0UetvZ80Dng10XuwWCMcb4nF9SN8YYYxphgd4YY3zOAr0xxvicBXpjjPE5C/TGGONzFujNIUNEIt7dD2OPVrvLqYj0k7g7kRrTloSan8UY39in7vYGxhxSrEVvDnnevc5/6d3v/D0ROdwb309E5nv3AZ8nIn288d1F5Dlx963/SESO81YVFJFHxd3L/jXv6lpE5Mfi7tW/TETmJKma5hBmgd4cStrVSd1Mj5tWoqojgD/g7voI8HvgCVUdCTwFPOSNfwh4U91968firtYEd8/wh1V1OFAMnOuNnwGM8dZz1cGpmjGNsytjzSFDREpVNaeB8etwfzjyhXdztq2q2kVEvsLd+7vKG79FVbuKSBGQr6oVcevoB7yu7o8jEJGfAGmqeo+IvAKU4i53f15VSw9yVY2pxVr0xjjayHBLVMQNR6g5BnY67n4lY4H34+4eaczXwgK9Mc70uOe3veHFuDs/AlwIvOUNzwN+CNV/VNKxsZWKSADoraoLgJ/gbv9br1dhzMFkLQtzKGnn/XtUzCuqGjvFsrOILMO1yi/wxl2L+yeg/8T9K9Cl3vjrgJkichmu5f5D3J1IGxIEnvR2BgI8pO5e98Z8bSxHbw55Xo6+QFW/SnZZjDkYLHVjjDE+Zy16Y4zxOWvRG2OMz1mgN8YYn7NAb4wxPmeB3hhjfM4CvTHG+Nz/By2pVw/s1yHiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_graphs(history, string, start_at=0):\n",
    "    plt.plot(history[string][start_at:])\n",
    "    plt.plot(history['val_'+string][start_at:])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(string[start_at:])\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "    \n",
    "plot_graphs(history, \"accuracy\", start_at=400)\n",
    "plot_graphs(history, \"loss\", start_at=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Indicator function  -#-%-  Definition  -#-%-  \"The indicator function of a subset \\'\\'A\\'\\' of a set \\'\\'X\\'\\' is a function\\\\n\\\\n:<math>\\\\\\\\mathbf{1}_A \\\\\\\\colon X \\\\\\\\to \\\\\\\\{ 0,1 \\\\\\\\} </math>\\\\n\\\\ndefined as\\\\n\\\\n:<math>\\\\\\\\mathbf{1}_A(x) :=\\\\n\\\\\\\\begin{cases}\\\\n1 &\\\\\\\\text{if } x \\\\\\\\in A, \\\\\\\\\\\\\\\\\\\\n0 &\\\\\\\\text{if } x \\\\\\\\notin A.\\\\n\\\\\\\\end{cases}\\\\n</math>\\\\n\\\\nThe [[Iverson bracket]] allows the equivalent notation, <math>[x\\\\\\\\in A]</math>, to be used instead of <math>\\\\\\\\mathbf{1}_A(x)</math>.\\\\n\\\\nThe function <math>\\\\\\\\mathbf{1}_A</math> is sometimes denoted <math>I_A</math>, <math>\\\\\\\\chi_A</math>, \\'\\'K<sub>A</sub>\\'\\' or even just <math>A</math>. (The [[Greek alphabet|Greek letter]] <math>\\\\\\\\chi</math> appears because it is the initial letter of the Greek word χαρακτήρ, which is the ultimate origin of the word \\'\\'characteristic\\'\\'.)\\\\n\\\\nThe set of all indicator functions on <math>X</math> can be identified with <math>\\\\\\\\mathcal{P}(X)</math>, the [[power set]] of <math>X</math>.  Consequently, both sets are sometimes denoted by <math>2^X</math>. This is a special case (<math>Y =\\\\\\\\{0,1\\\\\\\\}=2</math>) of the notation <math>Y^X</math> for the set of all functions <math>f:X\\\\\\\\to Y </math>.\"\\n'"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki[850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
